{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46d7b6b",
   "metadata": {},
   "source": [
    "# Deep mutual learning setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd177ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import medpy\n",
    "from medpy.io import load\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp, warpImage #, correlation_layer, meanfield\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d, deeds2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb6128",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load data and split into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load('Data/img.pth')\n",
    "segs = torch.load('Data/seg.pth')\n",
    "\n",
    "H,W = imgs[0].shape\n",
    "\n",
    "#define a training split \n",
    "torch.manual_seed(10)\n",
    "# Now, we prepare our train & test dataset.\n",
    "test_set = torch.LongTensor([35, 41, 0, 4, 39])\n",
    "train_set = torch.arange(43)\n",
    "for idx in test_set:\n",
    "    train_set = train_set[train_set != idx]\n",
    "print(\"Train: \", train_set)\n",
    "print(\"Test: \", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cb123",
   "metadata": {},
   "source": [
    "## FlowNet2 as teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc39039",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow2 = load_flownet2().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f3538",
   "metadata": {},
   "source": [
    "## Student creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224eb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "o_m = H//4\n",
    "o_n = W//4\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2).cuda()\n",
    "disp_range = 0.25\n",
    "displacement_width = 15    \n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2).cuda()\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66756bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        \n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02084f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "\n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance#.detach()\n",
    "\n",
    "# initalize the sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8abc0",
   "metadata": {},
   "source": [
    "# 3.1 DML with two instances\n",
    "\n",
    "train for 100 epochs with two optimizers. The loss is calculated between predictions of the two students and teacher output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6b366",
   "metadata": {},
   "source": [
    "## Obelisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one = OBELISK2d(16)\n",
    "init_weights(student_one)\n",
    "student_one.train().cuda()\n",
    "\n",
    "optimizer_student_one = torch.optim.Adam(list(student_one.parameters()), lr=0.00025)\n",
    "\n",
    "\n",
    "student_two = OBELISK2d(16)\n",
    "init_weights(student_two)\n",
    "student_two.train().cuda()\n",
    "\n",
    "optimizer_student_two = torch.optim.Adam(list(student_two.parameters()), lr=0.00025)\n",
    "\n",
    "students = [student_one, student_two]\n",
    "optims = [optimizer_student_one, optimizer_student_two]\n",
    "\n",
    "epochs = 1\n",
    "grad_accum = 5\n",
    "displace_range = 11\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # get training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "\n",
    "    fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "    moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "    \n",
    "    fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "    moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "    \n",
    "    # get the teacher output\n",
    "    flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "    teacher_flow = flow2(flow_in)\n",
    "    teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "    \n",
    "        \n",
    "    feat_one_fixed = student_one(fixed.cuda())\n",
    "    feat_one_moving = student_one(moving.cuda())\n",
    "    \n",
    "    feat_two_fixed = student_two(fixed.cuda())\n",
    "    feat_two_moving = student_two(moving.cuda())\n",
    "    \n",
    "    ssd_distance_one = correlation_layer(displace_range, feat_one_moving, feat_one_fixed)\n",
    "    soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "    ssd_distance_two = correlation_layer(displace_range, feat_two_moving, feat_two_fixed)\n",
    "    soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "    \n",
    "    loss_one = 0\n",
    "    loss_two = 0\n",
    "    \n",
    "    student_diff = torch.sum(torch.pow(disp_xy_one - disp_xy_two, 2)) \n",
    "    \n",
    "    loss_one += student_diff\n",
    "    loss_two += student_diff\n",
    "    \n",
    "    loss_one += torch.sum(torch.pow(teacher_flow - disp_xy_one, 2))\n",
    "    loss_two += torch.sum(torch.pow(teacher_flow - disp_xy_two, 2))\n",
    "    \n",
    "    loss_one.backward(retain_graph=True)\n",
    "    loss_two.backward()\n",
    "    \n",
    "    if (epoch+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations : backpropagate the accumulated gradients\n",
    "        optimizer_student_one.step()\n",
    "        optimizer_student_one.zero_grad()\n",
    "        \n",
    "        optimizer_student_two.step()\n",
    "        optimizer_student_two.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one.eval()\n",
    "student_two.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "p_mov = test_set[rnd_test_idx[1]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "\n",
    "flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "teacher_flow = flow2(flow_in)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat_one = student_one(fixed.cuda())\n",
    "    moving_feat_one = student_one(moving.cuda())\n",
    "\n",
    "    fixed_feat_two = student_two(fixed.cuda())\n",
    "    moving_feat_two = student_two(moving.cuda())\n",
    "\n",
    "ssd_distance_one = correlation_layer(displace_range, moving_feat_one, fixed_feat_one).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_one = F.interpolate(disp_xy_one,scale_factor=4,mode='bicubic')\n",
    "\n",
    "ssd_distance_two = correlation_layer(displace_range, moving_feat_two, fixed_feat_two).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_two = F.interpolate(disp_xy_two,scale_factor=4,mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg_one = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_one.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_student_seg_two = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_two.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+teacher_flow.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "#warped_teacher_seg = warpImage(moving_seg.unsqueeze(0).float().cuda(), teacher_flow.cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg_one.squeeze(),9)\n",
    "print(\"PDD-Student 1: \", d1,d1.mean())\n",
    "d2 = dice_coeff(fixed_seg,warped_student_seg_two.squeeze(),9)\n",
    "print(\"PDD-Student 2: \", d2,d2.mean())\n",
    "d3 = dice_coeff(fixed_seg,warped_teacher_seg.squeeze(),9)\n",
    "print(\"Teacher: \", d3, d3.mean())\n",
    "d4 = dice_coeff(fixed_seg,moving_seg,9)\n",
    "print(\"diff fixed, moving: \", d4, d4.mean())\n",
    "\n",
    "rgb_one = showFlow(dense_flow_fit_one.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_one = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_one.data.squeeze().t().flip(0),False)\n",
    "\n",
    "rgb_two = showFlow(dense_flow_fit_two.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_two = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_two.data.squeeze().t().flip(0),False)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb_one)\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay_one)\n",
    "plt.subplot(223)\n",
    "plt.imshow(rgb_two)\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d65c64",
   "metadata": {},
   "source": [
    "## Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())\n",
    "student_one.train().cuda()\n",
    "\n",
    "optimizer_student_one = torch.optim.Adam(list(student_one.parameters()), lr=0.00025)\n",
    "\n",
    "\n",
    "student_two = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())\n",
    "student_two.train().cuda()\n",
    "\n",
    "optimizer_student_two = torch.optim.Adam(list(student_two.parameters()), lr=0.00025)\n",
    "\n",
    "students = [student_one, student_two]\n",
    "optims = [optimizer_student_one, optimizer_student_two]\n",
    "\n",
    "epochs = 100\n",
    "grad_accum = 5\n",
    "displace_range = 11\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # get training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "\n",
    "    fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "    moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "    \n",
    "    fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "    moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "    \n",
    "    # get the teacher output\n",
    "    flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "    teacher_flow = flow2(flow_in)\n",
    "    teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "    \n",
    "        \n",
    "    feat_one_fixed = student_one(fixed.cuda())\n",
    "    feat_one_moving = student_one(moving.cuda())\n",
    "    \n",
    "    feat_two_fixed = student_two(fixed.cuda())\n",
    "    feat_two_moving = student_two(moving.cuda())\n",
    "    \n",
    "    ssd_distance_one = correlation_layer(displace_range, feat_one_moving, feat_one_fixed)\n",
    "    # compute the MIN-convolution & probabilistic output with the given function\n",
    "    soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "    ssd_distance_two = correlation_layer(displace_range, feat_two_moving, feat_two_fixed)\n",
    "    # compute the MIN-convolution & probabilistic output with the given function\n",
    "    soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "    \n",
    "    loss_one = 0\n",
    "    loss_two = 0\n",
    "    \n",
    "    student_diff = torch.sum(torch.pow(disp_xy_one - disp_xy_two, 2)) \n",
    "    \n",
    "    loss_one += student_diff\n",
    "    loss_two += student_diff\n",
    "    \n",
    "    loss_one += torch.sum(torch.pow(teacher_flow - disp_xy_one, 2))\n",
    "    loss_two += torch.sum(torch.pow(teacher_flow - disp_xy_two, 2))\n",
    "    \n",
    "    loss_one.backward(retain_graph=True)\n",
    "    loss_two.backward()\n",
    "    \n",
    "    if (epoch+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations : backpropagate the accumulated gradients\n",
    "        optimizer_student_one.step()\n",
    "        optimizer_student_one.zero_grad()\n",
    "        \n",
    "        optimizer_student_two.step()\n",
    "        optimizer_student_two.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one.eval()\n",
    "student_two.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "p_mov = test_set[rnd_test_idx[1]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "\n",
    "flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "teacher_flow = flow2(flow_in)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat_one = student_one(fixed.cuda())\n",
    "    moving_feat_one = student_one(moving.cuda())\n",
    "\n",
    "    fixed_feat_two = student_two(fixed.cuda())\n",
    "    moving_feat_two = student_two(moving.cuda())\n",
    "\n",
    "ssd_distance_one = correlation_layer(displace_range, moving_feat_one, fixed_feat_one).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_one = F.interpolate(disp_xy_one,scale_factor=4,mode='bicubic')\n",
    "\n",
    "ssd_distance_two = correlation_layer(displace_range, moving_feat_two, fixed_feat_two).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_two = F.interpolate(disp_xy_two,scale_factor=4,mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg_one = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_one.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_student_seg_two = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_two.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+teacher_flow.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "#warped_teacher_seg = warpImage(moving_seg.unsqueeze(0).float().cuda(), teacher_flow.cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg_one.squeeze(),9)\n",
    "print(\"Student 1: \", d1,d1.mean())\n",
    "d2 = dice_coeff(fixed_seg,warped_student_seg_two.squeeze(),9)\n",
    "print(\"Student 2: \", d2,d2.mean())\n",
    "d3 = dice_coeff(fixed_seg,warped_teacher_seg.squeeze(),9)\n",
    "print(\"Teacher: \", d3, d3.mean())\n",
    "d4 = dice_coeff(fixed_seg,moving_seg,9)\n",
    "print(\"diff fixed, moving: \", d4, d4.mean())\n",
    "\n",
    "\n",
    "rgb_one = showFlow(dense_flow_fit_one.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_one = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_one.data.squeeze().t().flip(0),False)\n",
    "\n",
    "rgb_two = showFlow(dense_flow_fit_two.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_two = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_two.data.squeeze().t().flip(0),False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb_one)\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay_one)\n",
    "plt.subplot(223)\n",
    "plt.imshow(rgb_two)\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe0de1",
   "metadata": {},
   "source": [
    "# 3.2 Combine with labelloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d063dc",
   "metadata": {},
   "source": [
    "## Obelisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one = OBELISK2d(16)\n",
    "init_weights(student_one)\n",
    "student_one.train().cuda()\n",
    "\n",
    "optimizer_student_one = torch.optim.Adam(list(student_one.parameters()), lr=0.00025)\n",
    "\n",
    "\n",
    "student_two = OBELISK2d(16)\n",
    "init_weights(student_two)\n",
    "student_two.train().cuda()\n",
    "\n",
    "optimizer_student_two = torch.optim.Adam(list(student_two.parameters()), lr=0.00025)\n",
    "\n",
    "students = [student_one, student_two]\n",
    "optims = [optimizer_student_one, optimizer_student_two]\n",
    "\n",
    "epochs = 100\n",
    "grad_accum = 5\n",
    "displace_range = 11\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # get training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "\n",
    "    fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "    moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "    \n",
    "    fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "    moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "    \n",
    "     # Label onehot and scale to outputsize \n",
    "    label_moving = F.one_hot(moving_seg,num_classes=9).permute(0,3,1,2).float()\n",
    "    _,C1,Hf,Wf = label_moving.size()\n",
    "    label_moving = F.interpolate(label_moving,size=(H//4,Wf//4),mode='bilinear')\n",
    "    label_fixed = F.one_hot(fixed_seg,num_classes=9).permute(0,3,1,2).float()\n",
    "    label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "    # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "    # according to the corresponding discrete displacement pair\n",
    "    label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,9,displace_range**2,-1)\n",
    "    \n",
    "    \n",
    "    # get the teacher output\n",
    "    flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "    teacher_flow = flow2(flow_in)\n",
    "    teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "    \n",
    "        \n",
    "    feat_one_fixed = student_one(fixed.cuda())\n",
    "    feat_one_moving = student_one(moving.cuda())\n",
    "    \n",
    "    feat_two_fixed = student_two(fixed.cuda())\n",
    "    feat_two_moving = student_two(moving.cuda())\n",
    "    \n",
    "    ssd_distance_one = correlation_layer(displace_range, feat_one_moving, feat_one_fixed)\n",
    "    soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "    label_warped_one = torch.sum(soft_cost_one.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "\n",
    "    \n",
    "    ssd_distance_two = correlation_layer(displace_range, feat_two_moving, feat_two_fixed)\n",
    "    soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "    \n",
    "    label_warped_two = torch.sum(soft_cost_two.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "\n",
    "    \n",
    "    loss_one = 0\n",
    "    loss_two = 0\n",
    "    \n",
    "    #student_diff = torch.sum(torch.pow(disp_xy_one - disp_xy_two, 2)) \n",
    "    student_diff = torch.sum(torch.pow(label_warped_two.reshape(9,-1)-label_warped_one.reshape(9,-1),2),0).mean().cuda()\n",
    "    \n",
    "    loss_one += student_diff\n",
    "    loss_two += student_diff\n",
    "    \n",
    "    loss_one += torch.sum(torch.pow(label_fixed.reshape(9,-1)-label_warped_one.reshape(9,-1),2),0).mean()\n",
    "    loss_two += torch.sum(torch.pow(label_fixed.reshape(9,-1)-label_warped_two.reshape(9,-1),2),0).mean()\n",
    "    \n",
    "    loss_one += torch.sum(torch.pow(teacher_flow - disp_xy_one, 2))\n",
    "    loss_two += torch.sum(torch.pow(teacher_flow - disp_xy_two, 2))\n",
    "    \n",
    "    loss_one.backward(retain_graph=True)\n",
    "    loss_two.backward()\n",
    "    \n",
    "    if (epoch+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations : backpropagate the accumulated gradients\n",
    "        optimizer_student_one.step()\n",
    "        optimizer_student_one.zero_grad()\n",
    "        \n",
    "        optimizer_student_two.step()\n",
    "        optimizer_student_two.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749156",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_one.eval()\n",
    "student_two.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "p_mov = test_set[rnd_test_idx[1]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "moving_seg = segs[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "\n",
    "flow_in = preprocessing_flownet(fixed.reshape(H,W,1),moving.reshape(H,W,1)).cuda()\n",
    "teacher_flow = flow2(flow_in)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat_one = student_one(fixed.cuda())\n",
    "    moving_feat_one = student_one(moving.cuda())\n",
    "\n",
    "    fixed_feat_two = student_two(fixed.cuda())\n",
    "    moving_feat_two = student_two(moving.cuda())\n",
    "\n",
    "ssd_distance_one = correlation_layer(displace_range, moving_feat_one, fixed_feat_one).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy_one = meanfield(ssd_distance_one, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_one = F.interpolate(disp_xy_one,scale_factor=4,mode='bicubic')\n",
    "\n",
    "ssd_distance_two = correlation_layer(displace_range, moving_feat_two, fixed_feat_two).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_two,disp_xy_two = meanfield(ssd_distance_two, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit_two = F.interpolate(disp_xy_two,scale_factor=4,mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg_one = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_one.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_student_seg_two = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit_two.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+teacher_flow.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "#warped_teacher_seg = warpImage(moving_seg.unsqueeze(0).float().cuda(), teacher_flow.cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg_one.squeeze(),9)\n",
    "print(\"PDD-Student 1: \", d1,d1.mean())\n",
    "d2 = dice_coeff(fixed_seg,warped_student_seg_two.squeeze(),9)\n",
    "print(\"PDD-Student 2: \", d2,d2.mean())\n",
    "d3 = dice_coeff(fixed_seg,warped_teacher_seg.squeeze(),9)\n",
    "print(\"Teacher: \", d3, d3.mean())\n",
    "d4 = dice_coeff(fixed_seg,moving_seg,9)\n",
    "print(\"diff fixed, moving: \", d4, d4.mean())\n",
    "\n",
    "rgb_one = showFlow(dense_flow_fit_one.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_one = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_one.data.squeeze().t().flip(0),False)\n",
    "\n",
    "rgb_two = showFlow(dense_flow_fit_two.cpu().transpose(-2,-1).flip(1))\n",
    "overlay_two = overlaySegment(fixed.squeeze().t().flip(0),warped_student_seg_two.data.squeeze().t().flip(0),False)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb_one)\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay_one)\n",
    "plt.subplot(223)\n",
    "plt.imshow(rgb_two)\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843ca92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
