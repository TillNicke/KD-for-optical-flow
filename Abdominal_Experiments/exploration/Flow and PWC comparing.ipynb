{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, subprocess\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "from models.flownet2.models import FlowNet2\n",
    "from models.flownet2.utils import tools\n",
    "from models.flownet2 import models, losses\n",
    "from models.flownet2.utils.flow_utils import *\n",
    "\n",
    "from models.pwc_net.models.PWCNet import PWCDCNet\n",
    "\n",
    "from utils.plotting import overlaySegment, countParameters\n",
    "from utils.layers import warp\n",
    "from utils.encoding import dice_coeff, labelMatrixOneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f93ab",
   "metadata": {},
   "source": [
    "### Loading PWC Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95641f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc = PWCDCNet()\n",
    "state_dict = torch.load(\"models/PWC_Net/PyTorch/pwc_net_chairs.pth.tar\")\n",
    "pwc.load_state_dict(state_dict)\n",
    "print(\"A new player has entered the game\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743c0ed",
   "metadata": {},
   "source": [
    "### Loading FlowNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--start_epoch', type=int, default=1)\n",
    "parser.add_argument('--total_epochs', type=int, default=10000)\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=8, help=\"Batch size\")\n",
    "parser.add_argument('--train_n_batches', type=int, default = -1, help='Number of min-batches per epoch. If < 0, it will be determined by training_dataloader')\n",
    "parser.add_argument('--crop_size', type=int, nargs='+', default = [256, 256], help=\"Spatial dimension to crop training samples for training\")\n",
    "parser.add_argument('--gradient_clip', type=float, default=None)\n",
    "parser.add_argument('--schedule_lr_frequency', type=int, default=0, help='in number of iterations (0 for no schedule)')\n",
    "parser.add_argument('--schedule_lr_fraction', type=float, default=10)\n",
    "parser.add_argument(\"--rgb_max\", type=float, default = 255.)\n",
    "\n",
    "parser.add_argument('--number_workers', '-nw', '--num_workers', type=int, default=8)\n",
    "parser.add_argument('--number_gpus', '-ng', type=int, default=-1, help='number of GPUs to use')\n",
    "parser.add_argument('--no_cuda', action='store_true')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--name', default='run', type=str, help='a name to append to the save directory')\n",
    "parser.add_argument('--save', '-s', default='./work', type=str, help='directory for saving')\n",
    "\n",
    "parser.add_argument('--validation_frequency', type=int, default=5, help='validate every n epochs')\n",
    "parser.add_argument('--validation_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--render_validation', action='store_true', help='run inference (save flows to file) and every validation_frequency epoch')\n",
    "\n",
    "parser.add_argument('--inference', action='store_true')\n",
    "parser.add_argument('--inference_visualize', action='store_true',\n",
    "                    help=\"visualize the optical flow during inference\")\n",
    "parser.add_argument('--inference_size', type=int, nargs='+', default = [-1,-1], help='spatial size divisible by 64. default (-1,-1) - largest possible valid size would be used')\n",
    "parser.add_argument('--inference_batch_size', type=int, default=1)\n",
    "parser.add_argument('--inference_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--save_flow', action='store_true', help='save predicted flows to file')\n",
    "\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--log_frequency', '--summ_iter', type=int, default=1, help=\"Log every n batches\")\n",
    "\n",
    "parser.add_argument('--skip_training', action='store_true')\n",
    "parser.add_argument('--skip_validation', action='store_true')\n",
    "\n",
    "parser.add_argument('--fp16', action='store_true', help='Run model in pseudo-fp16 mode (fp16 storage fp32 math).')\n",
    "parser.add_argument('--fp16_scale', type=float, default=1024., help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "tools.add_arguments_for_module(parser, models, argument_for_class='model', default='FlowNet2')\n",
    "\n",
    "tools.add_arguments_for_module(parser, losses, argument_for_class='loss', default='L1Loss')\n",
    "\n",
    "tools.add_arguments_for_module(parser, torch.optim, argument_for_class='optimizer', default='Adam', skip_params=['params'])\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "flownet = FlowNet2(args)\n",
    "flownet.load_state_dict(torch.load(\"models/flownet2/weights/FlowNet2_checkpoint.pth.tar\")['state_dict'])\n",
    "print(\"A new player has entered the game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PWC has \", countParameters(pwc), \" Parameters\")\n",
    "print(\"FlowNet has \", countParameters(flownet), \" Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71cd98",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7143467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pwc(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Preprocessing function for PWC net.\n",
    "    img1: numpy.ndarray in shape of (H,W,C)\n",
    "    img2: numpy.ndarray in shape of (H,W,C)\n",
    "    \n",
    "    return: torch.tensor in the shape of (1,2*C,H,W)\n",
    "    \"\"\"\n",
    "    img1 = img_1.numpy().copy()\n",
    "    img2 = img_2.numpy().copy()\n",
    "\n",
    "    H,W,C = img_1.shape   \n",
    "    \n",
    "    divisor = 64\n",
    "    H_ = int(ceil(H/divisor) * divisor)\n",
    "    W_ = int(ceil(W/divisor) * divisor)\n",
    "        \n",
    "    img1 = cv2.resize(np.concatenate([img1,img1,img1], 2), (W_,H_))\n",
    "    img2 = cv2.resize(np.concatenate([img2,img2,img2], 2), (W_,H_))\n",
    "        \n",
    "    images = [img1,img2]\n",
    "    \n",
    "    \n",
    "    for _i, _inputs in enumerate(images):\n",
    "        images[_i] = images[_i][:, :, ::-1]\n",
    "\n",
    "        images[_i] = np.transpose(images[_i], (2, 0, 1))\n",
    "\n",
    "        # Running into a \"negative number\" error in th line below, need to change that\n",
    "        # The error suggested to use the copy() function as a workaround. \n",
    "        images[_i] = torch.from_numpy(images[_i].copy())\n",
    "        # Worked ¯\\_(ツ)_/¯\n",
    "\n",
    "        images[_i] = images[_i].expand(1, images[_i].size()[0], images[_i].size()[1], images[_i].size()[2])\t\n",
    "        images[_i] = images[_i].float()\n",
    "    \n",
    "    return torch.cat(images,1)\n",
    "\n",
    "def preprocessing_flownet(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Preprocessing function for PWC net.\n",
    "    img1: numpy.ndarray in shape of (H,W,C)\n",
    "    img2: numpy.ndarray in shape of (H,W,C)\n",
    "    \n",
    "    return: torch.tensor in the shape of (1,B,C,H,W)\n",
    "    \n",
    "    \"\"\"\n",
    "    img1 = img_1.numpy().copy()\n",
    "    img2 = img_2.numpy().copy()\n",
    "    \n",
    "    if img1.max() <= 1.0:\n",
    "        img1 = img1 * 255\n",
    "    if img2.max() <= 1.0:\n",
    "        img2 = img2 * 255\n",
    "        \n",
    "    if img1.shape[2] == 1:\n",
    "        img1 = np.concatenate([img1,img1,img1], 2)\n",
    "    \n",
    "    if img2.shape[2] == 1:\n",
    "        img2 = np.concatenate([img2,img2,img2], 2)\n",
    "    \n",
    "    images = [img1, img2]\n",
    "    images = np.array(images).transpose(3, 0, 1, 2)\n",
    "    return torch.from_numpy(images.astype(np.float32)).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f5370",
   "metadata": {},
   "source": [
    "## Loading Torso Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3263d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.load('Tests/img.pth') \n",
    "seg = torch.load('Tests/seg.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = 9; mov = 10;\n",
    "fixed = img[fix:fix+1,:,:].float() /255\n",
    "moving = img[mov:mov+1,:,:].float() /255\n",
    "\n",
    "fixed_seg = seg[fix:fix+1, :,:].long().contiguous()\n",
    "moving_seg = seg[mov:mov+1,:,:].long().contiguous()\n",
    "\n",
    "C,H,W = fixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = overlaySegment(moving.squeeze(),moving_seg.squeeze(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(231)\n",
    "plt.title(\"Fixed scann\")\n",
    "plt.imshow(fixed.squeeze())\n",
    "plt.subplot(232)\n",
    "plt.title(\"Fixed segmentation\")\n",
    "plt.imshow(fixed_seg.squeeze())\n",
    "plt.subplot(233)\n",
    "plt.title(\"Overlayed for fixed frame\")\n",
    "plt.imshow(overlaySegment(fixed.squeeze(),fixed_seg.squeeze(), False).numpy())\n",
    "plt.subplot(234)\n",
    "plt.title(\"Moving scan\")\n",
    "plt.imshow(moving.squeeze())\n",
    "plt.subplot(235)\n",
    "plt.title(\"Moving segmentation\")\n",
    "plt.imshow(moving_seg.squeeze())\n",
    "plt.subplot(236)\n",
    "plt.title(\"Overlay moving frame\")\n",
    "plt.imshow(overlaySegment(moving.squeeze(),moving_seg.squeeze(),False).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "flownet_input = preprocessing_flownet(fixed.reshape(H,W,C),moving.reshape(H,W,C))\n",
    "pwc_input = preprocessing_pwc(fixed.reshape(H,W,C),moving.reshape(H,W,C))\n",
    "print(\"FlowNet Input: \", flownet_input.shape)\n",
    "print(\"PWCNet Input: \", pwc_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e84113",
   "metadata": {},
   "source": [
    "## FlowNet2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "flownet.cuda()\n",
    "flow_1 = flownet(flownet_input.cuda()).cpu()\n",
    "flownet.cpu()\n",
    "data = flow_1.squeeze().cpu().detach().numpy().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee505c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flow2img(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93715d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_seg_2 = warp(moving_seg.unsqueeze(0).float().cuda(), flow_1.cuda()).cpu().squeeze().to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d97c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(moving_seg,fixed_seg,8)\n",
    "print(d0, d0.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1303eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dice_coeff(warped_seg_2.unsqueeze(0), fixed_seg, 8)\n",
    "print(d1, d1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aea5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(overlaySegment(fixed.squeeze(),warped_seg_2.squeeze() ,False).numpy())\n",
    "plt.subplot(132)\n",
    "plt.imshow(overlaySegment(fixed.squeeze(),fixed_seg.squeeze() ,False).numpy())\n",
    "plt.subplot(133)\n",
    "plt.imshow(overlaySegment(moving.squeeze(),moving_seg.squeeze(),False).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a6bf1",
   "metadata": {},
   "source": [
    "### Verdict FlowNet\n",
    "\n",
    "Seems to work out of the box for medical data. Some finetuning needs to happen to make the flownet a suitable teacher, tho. For the fine tuning the focus should be on small number of epochs and a small amount of data, so that the majority of data can be used for the student teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d561e",
   "metadata": {},
   "source": [
    "## PWC-Net inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc.cuda()\n",
    "pwc.eval()\n",
    "\n",
    "flow_pwc = pwc(pwc_input.cuda())\n",
    "\n",
    "flow_pwc = flow_pwc[0]*20\n",
    "flow_pwc = flow_pwc.cpu().data.numpy()\n",
    "\n",
    "flow_pwc = np.swapaxes(np.swapaxes(flow_pwc, 0, 1), 1, 2) \n",
    "\n",
    "u_ = cv2.resize(flow_pwc[:,:,0],(W,H))\n",
    "v_ = cv2.resize(flow_pwc[:,:,1],(W,H))\n",
    "flow_pwc = np.dstack((u_,v_))\n",
    "flow_pwc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flow2img(flow_pwc))\n",
    "flow_pwc = torch.from_numpy(flow_pwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c18ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_flow = warp(moving_seg.unsqueeze(0).float().cuda(), flow_pwc.reshape(1,2,H,W).cuda()).cpu().squeeze().to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979cedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(moving_seg,fixed_seg,8)\n",
    "print(d0, d0.mean())\n",
    "\n",
    "d1 = dice_coeff(warped_flow.unsqueeze(0), fixed_seg, 8)\n",
    "print(d1, d1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(overlaySegment(fixed.squeeze(),warped_flow.squeeze() ,False).numpy())\n",
    "plt.subplot(132)\n",
    "plt.imshow(overlaySegment(fixed.squeeze(),fixed_seg.squeeze() ,False).numpy())\n",
    "plt.subplot(133)\n",
    "plt.imshow(overlaySegment(moving.squeeze(),moving_seg.squeeze(),False).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4c34d",
   "metadata": {},
   "source": [
    "### Verdict PWCNet\n",
    "\n",
    "needs some more fine tuning than flowNet2. Also the flow estimation is not as smooth as from FlowNet2. Performance could be better, but can be used after fine tuning, I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b98126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
