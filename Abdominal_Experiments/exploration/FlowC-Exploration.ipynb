{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.flownet2.networks.FlowNetC import FlowNetC\n",
    "from models.flownet2.utils import tools\n",
    "from models.flownet2 import models, losses\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "\n",
    "import torch\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--start_epoch', type=int, default=1)\n",
    "parser.add_argument('--total_epochs', type=int, default=10000)\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=8, help=\"Batch size\")\n",
    "parser.add_argument('--train_n_batches', type=int, default=-1,\n",
    "                    help='Number of min-batches per epoch. If < 0, it will be determined by training_dataloader')\n",
    "parser.add_argument('--crop_size', type=int, nargs='+', default=[256, 256],\n",
    "                    help=\"Spatial dimension to crop training samples for training\")\n",
    "parser.add_argument('--gradient_clip', type=float, default=None)\n",
    "parser.add_argument('--schedule_lr_frequency', type=int, default=0, help='in number of iterations (0 for no schedule)')\n",
    "parser.add_argument('--schedule_lr_fraction', type=float, default=10)\n",
    "parser.add_argument(\"--rgb_max\", type=float, default=255.)\n",
    "\n",
    "parser.add_argument('--number_workers', '-nw', '--num_workers', type=int, default=8)\n",
    "parser.add_argument('--number_gpus', '-ng', type=int, default=-1, help='number of GPUs to use')\n",
    "parser.add_argument('--no_cuda', action='store_true')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--name', default='run', type=str, help='a name to append to the save directory')\n",
    "parser.add_argument('--save', '-s', default='./work', type=str, help='directory for saving')\n",
    "\n",
    "parser.add_argument('--validation_frequency', type=int, default=5, help='validate every n epochs')\n",
    "parser.add_argument('--validation_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--render_validation', action='store_true',\n",
    "                    help='run inference (save flows to file) and every validation_frequency epoch')\n",
    "\n",
    "parser.add_argument('--inference', action='store_true')\n",
    "parser.add_argument('--inference_visualize', action='store_true',\n",
    "                    help=\"visualize the optical flow during inference\")\n",
    "parser.add_argument('--inference_size', type=int, nargs='+', default=[-1, -1],\n",
    "                    help='spatial size divisible by 64. default (-1,-1) - largest possible valid size would be used')\n",
    "parser.add_argument('--inference_batch_size', type=int, default=1)\n",
    "parser.add_argument('--inference_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--save_flow', action='store_true', help='save predicted flows to file')\n",
    "\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--log_frequency', '--summ_iter', type=int, default=1, help=\"Log every n batches\")\n",
    "\n",
    "parser.add_argument('--skip_training', action='store_true')\n",
    "parser.add_argument('--skip_validation', action='store_true')\n",
    "\n",
    "parser.add_argument('--fp16', action='store_true', help='Run model in pseudo-fp16 mode (fp16 storage fp32 math).')\n",
    "parser.add_argument('--fp16_scale', type=float, default=1024.,\n",
    "                    help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "tools.add_arguments_for_module(parser, models, argument_for_class='model', default='FlowNet2')\n",
    "\n",
    "tools.add_arguments_for_module(parser, losses, argument_for_class='loss', default='L1Loss')\n",
    "\n",
    "tools.add_arguments_for_module(parser, torch.optim, argument_for_class='optimizer', default='Adam', skip_params=['params'])\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowC = FlowNetC(args).cuda()\n",
    "flowC.eval()\n",
    "print(torch.load(\"models/flownet2/weights/FlowNet2-C_checkpoint.pth.tar\")['state_dict'].keys())\n",
    "#flowC.load_state_dict(torch.load(\"models/flownet2/weights/FlowNet2-C_checkpoint.pth.tar\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba368130",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load('Data/img.pth')\n",
    "segs = torch.load('Data/seg.pth')\n",
    "\n",
    "fix = 6; mov=3\n",
    "fixed = imgs[fix:fix+1,:,:].float() /255\n",
    "moving = imgs[mov:mov+1,:,:].float() /255\n",
    "\n",
    "fixed_seg = segs[fix:fix+1, :,:].float().contiguous()\n",
    "moving_seg = segs[mov:mov+1,:,:].float().contiguous()\n",
    "\n",
    "#fixed = F.interpolate(fixed.unsqueeze(0), size=(100,100)).view(1,100,100)\n",
    "#moving = F.interpolate(moving.unsqueeze(0), size=(100,100)).view(1,100,100)\n",
    "\n",
    "#fixed_seg = F.interpolate(fixed_seg.unsqueeze(0), size=(100,100)).view(1,100,100)\n",
    "#moving_seg = F.interpolate(moving_seg.unsqueeze(0), size=(100,100)).view(1,100,100)\n",
    "\n",
    "C,h,w = fixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_in = preprocessing_flownet(fixed.reshape(h,w,C),moving.reshape(h,w,C))#.cuda()\n",
    "flowC_in = preprocessing_pwc(fixed.reshape(h,w,C),moving.reshape(h,w,C)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8faace",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA],\n",
    "    with_flops=True\n",
    ") as p:\n",
    "    flowC(flowC_in)\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cuda_time_total\", row_limit=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74cee6",
   "metadata": {},
   "source": [
    "If I read this correctly, it takes around 4.89ms for one pas and requires around 4 TFlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4672b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
