{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36100b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import cv2 as cv\n",
    "\n",
    "from flow_net.flownet2_cuda.utils.flow_utils import * \n",
    "\n",
    "from OwnFlowNet2.FlowNet2 import FlowNet2\n",
    "from utils.encoding import *\n",
    "from utils.layers import *\n",
    "from utils.plotting import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b092e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "torch.manual_seed(10)\n",
    "\n",
    "pat_indices = torch.cat((torch.arange(0, 17), torch.arange(18, 43)), 0)\n",
    "\n",
    "rnd_perm_idc = torch.randperm(pat_indices.size(0))\n",
    "pat_indices = pat_indices[rnd_perm_idc]\n",
    "# train_set = pat_indices[:35]\n",
    "# test_set = torch.cat((pat_indices[35:],torch.LongTensor([17])),0)\n",
    "\n",
    "# Now, we prepare our train & test dataset.\n",
    "test_set = torch.LongTensor([35, 41, 0, 4, 33, 38, 39, 17])\n",
    "train_set = torch.arange(43)\n",
    "for idx in test_set:\n",
    "    train_set = train_set[train_set != idx]\n",
    "\n",
    "print('Test_Set:', test_set)\n",
    "print('Train_Set:', train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.load('/home/till/uni/masterArbeit/Tests/img.pth')\n",
    "seg = torch.load('/home/till/uni/masterArbeit/Tests/seg.pth')\n",
    "print(\"images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate net\n",
    "net = FlowNet2()\n",
    "net.load_state_dict(torch.load(\"OwnFlowNet2/weights/FlowNet2_checkpoint.pth.tar\")['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f5d60",
   "metadata": {},
   "source": [
    "## Show Flownet2 for one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450bfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "fix = train_set[rnd_train_idx[0]]\n",
    "mov = train_set[rnd_train_idx[1]]\n",
    "fix = 10; mov = 15\n",
    "#print(fix , mov)\n",
    "\n",
    "img_fixed = img[fix:fix + 1, :, :].unsqueeze(1).float() / 255\n",
    "img_moving = img[mov:mov + 1, :, :].unsqueeze(1).float() / 255\n",
    "H, W = img_fixed.shape[-2:]\n",
    "\n",
    "img_fixed_rgb = cv.cvtColor(np.asarray(img_fixed).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "img_moving_rgb = cv.cvtColor(np.asarray(img_moving).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "\n",
    "images = [img_fixed_rgb, img_moving_rgb]\n",
    "images = np.array(images).transpose(3, 0, 1, 2)\n",
    "im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output = net(im)\n",
    "data = net_output.squeeze().data.cpu().numpy().transpose(1, 2, 0)\n",
    "flow = net_output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3daf938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract two segmentations and compute Dice\n",
    "moving_seg = (seg[mov:mov+1,:,:]).long().contiguous()\n",
    "fixed_seg = (seg[fix:fix+1,:,:]).long().contiguous()\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,8)\n",
    "print(d0,d0.mean())\n",
    "\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "warped_seg = F.grid_sample(moving_seg.float().unsqueeze(1),identity+net_output.permute(0,2,3,1),mode='nearest',align_corners=False)\n",
    "d1 = dice_coeff(fixed_seg,warped_seg.squeeze(),8)\n",
    "print(d1,d1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fb0a8",
   "metadata": {},
   "source": [
    "## Visualize results.\n",
    "It can be seen that the dice score from warped moving seg and fixed seg is similar to the dice score from moving seg and fixed seg, leading to the assumption, that little flow was used to warp the labels.\n",
    "\n",
    "Having a look at the arped labels, they are not very accurate, for the given example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(231)\n",
    "plt.imshow(img_fixed.squeeze())\n",
    "plt.subplot(233)\n",
    "plt.imshow(flow2img(data))\n",
    "plt.subplot(232)\n",
    "plt.imshow(img_moving.squeeze())\n",
    "plt.subplot(234)\n",
    "plt.imshow(seg[mov:mov+1,:,:].squeeze())\n",
    "plt.subplot(235)\n",
    "plt.imshow(seg[fix:fix+1,:,:].squeeze())\n",
    "plt.subplot(236)\n",
    "plt.imshow(overlaySegment(img_fixed.squeeze(),warped_seg.data.squeeze(),True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07023386",
   "metadata": {},
   "source": [
    "Later I discovered,that the flow looks that bad, because the input images were normalides, and FlowNet2 expects a range from 0 to 255 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8def87",
   "metadata": {},
   "source": [
    "## Training loop for Flownet2, to train on the small image dataset\n",
    "Just for experimenting how the net learns. \n",
    "First Experiment: using the whole dataset once, and showing all images from the training set once, in order to overcome the nan loss problem\n",
    "- Adam optim gives nan loss after first step, regardless of the learning rate\n",
    "- loss funciton of squarred error between warped labels and fixed labels does not lead to improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optim (run with SGD)\n",
    "#del net\n",
    "net = FlowNet2()\n",
    "net.load_state_dict(torch.load(\"OwnFlowNet2/weights/FlowNet2_checkpoint.pth.tar\")['state_dict'])\n",
    "\n",
    "optimizer = torch.optim.Adam(list(net.parameters()), lr = 0.0025)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "grad_accum = 20\n",
    "\n",
    "net.train()\n",
    "net.cuda()\n",
    "\n",
    "# Identity matrix used for computation. \n",
    "H, W = img[0:1,:,:].shape[-2:]\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "\n",
    "epochs = np.arange(0,200)\n",
    "loss_vals = []\n",
    "# Iterate over all examples once\n",
    "for epoch in epochs:\n",
    "    \n",
    "    # randomize selection\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "        \n",
    "    # load images and transform them to RGB images\n",
    "    img_fixed = img[p_fix:p_fix + 1, :, :].unsqueeze(1).float() / 255\n",
    "    img_moving = img[p_mov:p_mov + 1, :, :].unsqueeze(1).float() / 255\n",
    "    \n",
    "    H, W = img_fixed.shape[-2:]\n",
    "    \n",
    "    img_fixed_rgb = cv.cvtColor(np.asarray(img_fixed).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "    img_moving_rgb = cv.cvtColor(np.asarray(img_moving).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "\n",
    "    # used from the code of the official flownet2 repo\n",
    "    # https://github.com/NVIDIA/flownet2-pytorch\n",
    "    images = np.asarray([img_fixed_rgb, img_moving_rgb]).transpose(3,0,1,2)\n",
    "    im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0)\n",
    "    \n",
    "    # estimatin flow with Flownet2\n",
    "    flow_estimation = net(im.cuda()).cpu()\n",
    "    \n",
    "    # loading the two segmentations and comapre them\n",
    "    fixed_seg = seg[p_fix:p_fix + 1, :, :].long().contiguous()\n",
    "    moving_seg = seg[p_mov:p_mov + 1, :, :].long().contiguous()\n",
    "    #d0 = dice_coeff(fixed_seg,moving_seg,8)\n",
    "    \n",
    "    # warping the fixed seg with the output of the flow net\n",
    "    warped_seg = F.grid_sample(moving_seg.float().unsqueeze(1),identity+flow_estimation.permute(0,2,3,1),mode='nearest',align_corners=False).reshape(1,H,W)\n",
    "    #plt.imshow(warped_seg.detach().squeeze().numpy())\n",
    "    \n",
    "    label_fixed = F.one_hot(fixed_seg, num_classes=9).permute(0, 3, 1, 2).float()\n",
    "    _, C1, Hf, Wf = label_fixed.size()\n",
    "    label_fixed = F.interpolate(label_fixed, size=(Hf, Wf), mode='bilinear').squeeze()\n",
    "        \n",
    "    label_warped = F.one_hot(warped_seg.to(torch.int64), num_classes=9).permute(0, 3, 1, 2).float()\n",
    "    _, C1, Hf, Wf = label_warped.size()\n",
    "    label_warped = F.interpolate(label_warped, size=(Hf, Wf), mode='bilinear').squeeze()\n",
    "    \n",
    "    # let us try to use the difference in dice as loss\n",
    "    loss = torch.sum(torch.pow(label_fixed.reshape(9, -1) - label_warped.reshape(9, -1), 2), 0).mean()\n",
    "    loss.requires_grad=True\n",
    "    #print(loss)\n",
    "    loss.cuda().backward()\n",
    "    loss_vals.append(loss.cpu().item())\n",
    "    \n",
    "    if (epoch + 1) % grad_accum == 0:\n",
    "        # every grad_accum iterations : backpropagate the accumulated gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #print(loss.item())\n",
    "\n",
    "plt.plot(epochs, loss_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ed50e",
   "metadata": {},
   "source": [
    "## verdict\n",
    "Leave Flownet2 as is, because sometimes it seems to be good. So definetly can be used for ensemble. If time: Train more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe6ad6",
   "metadata": {},
   "source": [
    "## Some more Exploration (06.05.21)\n",
    "Finally was able to compile the cuda correlation layer, as well as other layers the lfownet uses. \n",
    "Therefore I want to explore the flownet cuda implementation, from https://github.com/NVIDIA/flownet2-pytorch together with the pretrained weights\n",
    "Maybe, just maybe, I can finally fine tune it on the medical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c421794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, subprocess\n",
    "import torch\n",
    "#sys.path.insert(1, './flow_net')\n",
    "from models.flownet2.models import FlowNet2\n",
    "from models.flownet2.utils import tools\n",
    "from models.flownet2 import models, losses\n",
    "from models.flownet2.utils.flow_utils import *\n",
    "\n",
    "from utils.encoding import dice_coeff\n",
    "from utils.plotting import overlaySegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--start_epoch', type=int, default=1)\n",
    "parser.add_argument('--total_epochs', type=int, default=10000)\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=8, help=\"Batch size\")\n",
    "parser.add_argument('--train_n_batches', type=int, default = -1, help='Number of min-batches per epoch. If < 0, it will be determined by training_dataloader')\n",
    "parser.add_argument('--crop_size', type=int, nargs='+', default = [256, 256], help=\"Spatial dimension to crop training samples for training\")\n",
    "parser.add_argument('--gradient_clip', type=float, default=None)\n",
    "parser.add_argument('--schedule_lr_frequency', type=int, default=0, help='in number of iterations (0 for no schedule)')\n",
    "parser.add_argument('--schedule_lr_fraction', type=float, default=10)\n",
    "parser.add_argument(\"--rgb_max\", type=float, default = 255.)\n",
    "\n",
    "parser.add_argument('--number_workers', '-nw', '--num_workers', type=int, default=8)\n",
    "parser.add_argument('--number_gpus', '-ng', type=int, default=-1, help='number of GPUs to use')\n",
    "parser.add_argument('--no_cuda', action='store_true')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--name', default='run', type=str, help='a name to append to the save directory')\n",
    "parser.add_argument('--save', '-s', default='./work', type=str, help='directory for saving')\n",
    "\n",
    "parser.add_argument('--validation_frequency', type=int, default=5, help='validate every n epochs')\n",
    "parser.add_argument('--validation_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--render_validation', action='store_true', help='run inference (save flows to file) and every validation_frequency epoch')\n",
    "\n",
    "parser.add_argument('--inference', action='store_true')\n",
    "parser.add_argument('--inference_visualize', action='store_true',\n",
    "                    help=\"visualize the optical flow during inference\")\n",
    "parser.add_argument('--inference_size', type=int, nargs='+', default = [-1,-1], help='spatial size divisible by 64. default (-1,-1) - largest possible valid size would be used')\n",
    "parser.add_argument('--inference_batch_size', type=int, default=1)\n",
    "parser.add_argument('--inference_n_batches', type=int, default=-1)\n",
    "parser.add_argument('--save_flow', action='store_true', help='save predicted flows to file')\n",
    "\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--log_frequency', '--summ_iter', type=int, default=1, help=\"Log every n batches\")\n",
    "\n",
    "parser.add_argument('--skip_training', action='store_true')\n",
    "parser.add_argument('--skip_validation', action='store_true')\n",
    "\n",
    "parser.add_argument('--fp16', action='store_true', help='Run model in pseudo-fp16 mode (fp16 storage fp32 math).')\n",
    "parser.add_argument('--fp16_scale', type=float, default=1024., help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "tools.add_arguments_for_module(parser, models, argument_for_class='model', default='FlowNet2')\n",
    "\n",
    "tools.add_arguments_for_module(parser, losses, argument_for_class='loss', default='L1Loss')\n",
    "\n",
    "tools.add_arguments_for_module(parser, torch.optim, argument_for_class='optimizer', default='Adam', skip_params=['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "net = FlowNet2(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"models/flownet2/weights/FlowNet2_checkpoint.pth.tar\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63283f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = 10; mov = 15\n",
    "#print(fix , mov)\n",
    "img = torch.load('Data/img.pth')\n",
    "seg = torch.load('Data/seg.pth')\n",
    "img_fixed = img[fix:fix + 1, :, :].float()\n",
    "img_moving = img[mov:mov + 1, :, :].float()\n",
    "C,H,W = img_fixed.shape\n",
    "\n",
    "fixed_seg = seg[fix:fix + 1, :, :]\n",
    "moving_seg = seg[mov:mov + 1, :, :]\n",
    "\n",
    "#img_fixed_rgb = cv.cvtColor(np.asarray(img_fixed).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "#img_moving_rgb = cv.cvtColor(np.asarray(img_moving).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "img_fixed = img_fixed.reshape(H,W,C).numpy()\n",
    "img_moving= img_moving.reshape(H,W,C).numpy()\n",
    "\n",
    "img_fixed = np.concatenate([img_fixed, img_fixed, img_fixed], 2)\n",
    "img_moving = np.concatenate([img_moving, img_moving, img_moving], 2)\n",
    "\n",
    "images = [img_fixed, img_moving]\n",
    "images = np.array(images).transpose(3, 0, 1, 2)\n",
    "im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151685f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cuda()\n",
    "flow = net(im.cuda()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = flow.data.cpu().numpy().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9437018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowVis = flow2img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([1,1,1,1])\n",
    "plt.imshow(flowVis)\n",
    "\n",
    "plt.imsave(\"FlowNet2-FlowVis.png\", flowVis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layers import warp\n",
    "warpedSeg = warp(moving_seg.unsqueeze(0).float().cuda(), flow).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(fixed_seg,warpedSeg,8)\n",
    "print(d0,d0.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = img[fix:fix + 1, :, :].float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(overlaySegment(test.squeeze(), warpedSeg.view(320,256))\n",
    "plt.subplot(132)\n",
    "plt.imshow(overlaySegment(test.squeeze(), fixed_seg.squeeze()))\n",
    "plt.subplot(133)\n",
    "plt.imshow(overlaySegment(test.squeeze(), moving_seg.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2be37",
   "metadata": {},
   "source": [
    "# Verdict\n",
    "Looks good. Can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aea8d",
   "metadata": {},
   "source": [
    "# Revisit flownet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07ba4e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.flownet2_noKernel.flownet_pytorch_main.flownet2_mph import *\n",
    "import torch\n",
    "from utils.plotting import flow2img\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "crnt_dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "flow= FlowNet2()\n",
    "state_dict = torch.load(\"models/flownet2/weights/FlowNet2_checkpoint.pth.tar\")\n",
    "flow.load_state_dict(state_dict['state_dict'])\n",
    "flow.to(crnt_dev)\n",
    "flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = 10; mov = 15\n",
    "#print(fix , mov)\n",
    "img = torch.load('Data/img.pth')\n",
    "seg = torch.load('Data/seg.pth')\n",
    "img_fixed = img[fix:fix + 1, :, :].float()\n",
    "img_moving = img[mov:mov + 1, :, :].float()\n",
    "C,H,W = img_fixed.shape\n",
    "\n",
    "fixed_seg = seg[fix:fix + 1, :, :]\n",
    "moving_seg = seg[mov:mov + 1, :, :]\n",
    "\n",
    "#img_fixed_rgb = cv.cvtColor(np.asarray(img_fixed).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "#img_moving_rgb = cv.cvtColor(np.asarray(img_moving).squeeze(), cv.COLOR_GRAY2RGB)# .reshape(3, 320, 256)\n",
    "img_fixed = img_fixed.reshape(H,W,C).numpy()\n",
    "img_moving= img_moving.reshape(H,W,C).numpy()\n",
    "\n",
    "img_fixed = np.concatenate([img_fixed, img_fixed, img_fixed], 2)\n",
    "img_moving = np.concatenate([img_moving, img_moving, img_moving], 2)\n",
    "\n",
    "images = [img_fixed, img_moving]\n",
    "images = np.array(images).transpose(3, 0, 1, 2)\n",
    "im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_out = flow(im.to(crnt_dev)).squeeze()\n",
    "data = flow_out.data.cpu().numpy().transpose(1,2,0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51506526",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowVis = flow2img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78529245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([1,1,1,1])\n",
    "plt.imshow(flowVis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf2207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
