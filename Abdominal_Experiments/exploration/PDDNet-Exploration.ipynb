{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6217adc1",
   "metadata": {},
   "source": [
    "## Playground for PDDNet\n",
    "The class below was taken from the git repo https://github.com/multimodallearning/pdd_net\n",
    "\n",
    "This is the playgrund for PDD exploration and creating a model, that will be used as a student with FLowNet2 and PWC-Net as teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils.plotting import countParameters, flow2img, overlaySegment\n",
    "from utils.layers import warp\n",
    "from utils.encoding import labelMatrixOneHot\n",
    "\n",
    "import cv2\n",
    "from utils.layers import warp \n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ad4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.load(\"Data/img.pth\")\n",
    "seg= torch.load(\"Data/seg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11893cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = img[0].shape\n",
    "print(h,w)\n",
    "o_m = h//3\n",
    "o_n = w//3\n",
    "#o_o = D//3\n",
    "#print('numel_o',o_m*o_n)\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2)#.cuda()\n",
    "print(\"ogrid_xy: \", ogrid_xy.shape)\n",
    "\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 11#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2)#.cuda()\n",
    "print(\"shift: \", shift_xy.shape)\n",
    "\n",
    "#_,_,H,W,D = img00.size()\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)#.cuda()\n",
    "print(\"grid: \", grid_xy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695326d",
   "metadata": {},
   "source": [
    "Basically I reduced every dimension as far as I could by 1. This does $\\textbf{not}$ mean that this isthe right way to do it. I need to look into the logic behind it\n",
    "\n",
    "## Obelisk and Deeds 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels*2)*0.05)\n",
    "        self.layer0 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels*4, channels*4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels*4)\n",
    "        self.layer2 = nn.Conv2d(channels*4, channels*2, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels*2)\n",
    "        self.layer3 = nn.Conv2d(channels*2, channels*1, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img,3,padding=1,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in,ogrid_xy + self.offsets[0,:].view(1,-1,1,2)).view(1,-1,o_m,o_n)\n",
    "        sampled -= F.grid_sample(img_in,ogrid_xy + self.offsets[1,:].view(1,-1,1,2)).view(1,-1,o_m,o_n)\n",
    "    \n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class deeds2d(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(deeds2d, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.Tensor([1,.1,1,1,.1,1])).cuda()\n",
    "\n",
    "        self.pad1 = nn.ReplicationPad2d(3).cuda()\n",
    "        self.avg1 = nn.AvgPool2d(3,stride=1).cuda()\n",
    "        self.max1 = nn.MaxPool2d(3,stride=1).cuda()\n",
    "        self.pad2 = nn.ReplicationPad2d(2).cuda()##\n",
    "        \n",
    "    def forward(self, feat00,feat50):\n",
    "        \n",
    "        #deeds correlation layer (slightly unrolled)\n",
    "        deeds_cost = torch.zeros(1,grid_size**2,displacement_width, displacement_width).cuda()\n",
    "\n",
    "        #print(deeds_cost.shape)\n",
    "        xy8 = grid_size**2\n",
    "        #i=1\n",
    "        #print(grid_xy[:,i*grid_size:(i+1)*grid_size,:,:].shape)\n",
    "        for i in range(grid_size): \n",
    "            moving_unfold = F.grid_sample(feat50, grid_xy[:,i*grid_size:(i+1)*grid_size,:,:] + shift_xy,padding_mode='border')\n",
    "            fixed_grid = F.grid_sample(feat00, grid_xy[:,i*grid_size:(i+1)*grid_size,:,:]) # grid_xy[:,i*xy8:(i+1)*xy8,:,:]\n",
    "            deeds_cost[:,i*grid_size:(i+1)*grid_size,:,:] = self.alpha[1]+self.alpha[0]*torch.sum(torch.pow(fixed_grid-moving_unfold,2),1).view(1,-1,displacement_width,displacement_width)\n",
    "        \n",
    "\n",
    "        # remove mean (not really necessary)\n",
    "        #deeds_cost = deeds_cost.view(-1,displacement_width**3) - deeds_cost.view(-1,displacement_width**3).mean(1,keepdim=True)[0]\n",
    "        #deeds_cost = deeds_cost.view(1,-1,displacement_width,displacement_width)\n",
    "        #print(deeds_cost.shape)\n",
    "\n",
    "        # approximate min convolution / displacement compatibility\n",
    "        cost = self.avg1(self.avg1(-self.max1(-self.pad1(deeds_cost))))            \n",
    "        # grid-based mean field inference (one iteration)\n",
    "        cost_permute = cost.permute(2,3,0,1).view(1,displacement_width**2,grid_size, grid_size)\n",
    "        cost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0,2,3,1).view(1,-1,displacement_width,displacement_width)\n",
    "\n",
    "        # second path\n",
    "        cost = self.alpha[4]+self.alpha[2]*deeds_cost+self.alpha[3]*cost_avg\n",
    "        cost = self.avg1(self.avg1(-self.max1(-self.pad1(cost))))\n",
    "        \n",
    "        # grid-based mean field inference (one iteration)\n",
    "        cost_permute = cost.permute(2,3,0,1).view(1,displacement_width**2,grid_size,grid_size)\n",
    "        cost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0,2,3,1).view(grid_size**2,displacement_width**2)\n",
    "        #cost = alpha[4]+alpha[2]*deeds_cost+alpha[3]*cost.view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "        #cost = avg1(avg1(-max1(-pad1(cost))))\n",
    "        \n",
    "        #probabilistic and continuous output\n",
    "        cost_soft = F.softmax(-self.alpha[5]*cost_avg,1)\n",
    "        #pred_xyz = torch.sum(F.softmax(-5self.alpha[2]*cost_avg,1).unsqueeze(2)*shift_xyz.view(1,-1,3),1)\n",
    "        pred_xy = torch.sum(cost_soft.unsqueeze(2)*shift_xy.view(1,-1,2),1)\n",
    "\n",
    "\n",
    "        return cost_soft,pred_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OBELISK2d(chan=10).cuda()\n",
    "#reg = deeds2d().cuda()\n",
    "\n",
    "macs_obel, params_obel = get_model_complexity_info(net, input_res=(1,320,256), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs_obel))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params_obel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2287fda",
   "metadata": {},
   "source": [
    "MAC stands for *Multiply–accumulate operation* and can be roughly interpreted as GMACs = 0.5 * GFLOPs $\\Leftrightarrow$ 2 * GMACs = GFLOPs \n",
    "\n",
    "From \"AutoDVT\" by Tanno et al, we knwo, that the PowerVR GT7600 Plus (a mobile GPU) provides up to 384 GFLOPs. devided by 25 images/s (leave room for other processes on the GPU), this yields $\\frac{384 GFLOPs}{25 IMGs} \\approx 15GFLOP$ for one image pass.\n",
    "\n",
    "With the api, used in the cell above, we can estimate the GFLOPs of the used model. By multiplying the estimated GMac by 2 to convert it into FLOP we get $0.16 * 2 * 2 = 0.32$ GFLOP for one images for the feature extraction. We only need to estmate this for one image, as the feature extracted from the previous frame can be held in memory and does not need to be computed twice. Passing through two images, will only need to be done in the beginning, when there is no feature representation existend in memory.\n",
    "\n",
    "Now the reg model, is a bit more difficult. The api used above, only allows for one input resolution, and therefore can't be used with the reg model, as it expects two inputs. However, we can roughly estimate the FLOP used by the reg model, when we think about what is happening. We first estimate the correlation between the two extracted features for every grid point. Both features have a size of BxOutputChannelx106x85, with B = 1. With a grid size of 32x32, we loop over the correlation estimation 32 times. The correlation estimation can be expressed as $\\alpha_1 + \\alpha_0 * \\sum_{c=0}^{Channel}(fixed-moving)^2$ where fixed has a shape of Channel x Grid x 1 and moving Channel x Grid x Offset². We need to do gridsize x displacement² subtractions and multiplications. This is done over the Channels in every grid. We end up with grid x Channel x (grid x displacement²)² Flop. For 14 channels and a grid of 32 x 32 with an displacement of 11, we end up with $32 * 14 * (32 * 121)^2 = 6.716.588.032$ operations. This would be 6.7 GFlop for one pass. *And I am honestly not sure about this...*\n",
    "\n",
    "Alternative: $32 * 14 * (32 * 121) = 1734656$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7421b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img[0].cuda()\n",
    "segmen = seg[0].float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OBELISK2d(chan=10).cuda()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA],\n",
    "    with_flops=True,\n",
    "    profile_memory=True,\n",
    "    \n",
    ") as p:\n",
    "    output = net(image.reshape(1,h,w).unsqueeze(0).to(torch.float32))\n",
    "    output = net(image.reshape(1,h,w).unsqueeze(0).to(torch.float32))\n",
    "    cost_out, pred = reg(output.cuda(), output.cuda())\n",
    "print(p.key_averages().table(\n",
    "    sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50079dab",
   "metadata": {},
   "source": [
    "If I read this correctly, the network takes around 3.1 ms on a GTX 1060 and requires $857.424 + 904.664 + 1273041.118 = 1.274.803.206$ MFlops\n",
    "\n",
    "If the numver of channels is reduced to 10, the runtime is abviously lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = []\n",
    "for i in range(1):\n",
    "    begin = time.time()\n",
    "    output = net(image.reshape(1,h,w).unsqueeze(0).to(torch.float32))\n",
    "    cost_out, pred = reg(output, output)\n",
    "    end = time.time()\n",
    "    time_array.append(end-begin)\n",
    "#print(f\"Total runtime of the Obelisk Net is {(end - begin)} for one image \\nThat means {25*(end - begin)} for 25 images\")\n",
    "plt.scatter(np.arange(1000), time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean inference time: \", np.asarray(time_array).mean())\n",
    "print(\"Variance: \", np.asarray(time_array).var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a6a46",
   "metadata": {},
   "source": [
    "Mean infeence time is around 0.034 seconds. Some variance, as seen in the plot. Might be due to background programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_out, pred = reg(output, output)\n",
    "print(pred.view(grid_size,grid_size,2).shape)\n",
    "print(cost_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(pred.reshape(grid_size,grid_size,2).detach().numpy(),(h,w))\n",
    "flow_img = flow2img(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624993ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flow_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d30719",
   "metadata": {},
   "source": [
    "Obviously this is nonesense, but the flow is there.. not sure about the upscaling though.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c47b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT SURE about this cell. If it does any good\n",
    "\n",
    "segmen_grid = F.grid_sample(segmen.unsqueeze(0).unsqueeze(0),grid_xy+shift_xy,padding_mode='border')\n",
    "print(segmen_grid.shape)\n",
    "print(cost_out.view(1,-1,grid_size**2,displacement_width**2).shape)\n",
    "warped = segmen_grid * cost_out.view(1,-1,grid_size**2,displacement_width**2)\n",
    "#warped_label = (F.grid_sample(segmen.unsqueeze(0).unsqueeze(0),grid_xy+shift_xy,padding_mode='border') *cost_out.view(1,-1,grid_size**2,displacement_width**2)).sum(3,keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec42ee",
   "metadata": {},
   "source": [
    "## Train the network for 2D Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(2,3).unsqueeze(0) + torch.randn(B, 2, 3) * strength)#.to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,H,W)))\n",
    "\n",
    "    img_out = F.grid_sample(img_in.float(), meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float().unsqueeze(1), meshgrid, mode='nearest').long().squeeze(1)\n",
    "\n",
    "    return img_out, seg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e450a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    # tensor dimensionalities in comments are for an arbitrary choice of\n",
    "    # displace_range = 11 & feat sizes of [1,24,80,78];\n",
    "    # they clearly depend on the actual choice and only serve as numerical examples here.\n",
    "\n",
    "    disp_hw = (displace_range -1 )//2\n",
    "    # feat_mov: [1,24,80,78] -> 24 feature channels + spatial HW dims\n",
    "    # feat_mov_unfold: [24,121,6240] -> mind chans, 11*11 = 121 displ steps, 6240 = 80*78 spatial positions\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1 ,0) ,(displace_range ,displace_range) ,padding=disp_hw)\n",
    "\n",
    "    B ,C ,H ,W = feat_fixed.size()\n",
    "\n",
    "    # feat_fixed: [24,1,6240] -> compute scalarproduct along feature dimension per broadcast + sum along 0\n",
    "    # and reshape to [1,121,80,78]\n",
    "    ssd_distance = ((feat_fixed.view(C ,1 ,-1) - feat_moving_unfold )**2).sum(0).view(1 ,displace_range**2 ,H ,W)\n",
    "    # reshape the 4D tensor back to spatial dimensions\n",
    "    return ssd_distance\n",
    "\n",
    "def meanfield(ssd_distance ,img_fixed ,displace_range ,H ,W):\n",
    "\n",
    "    # crnt_dev = ssd_distance.device\n",
    "\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "    # probabilistic output: compute the contributions weights of every discrete displacement pair for all positions\n",
    "    # along 121 possible xy displacements -> normalize to [0,1] with softmax:\n",
    "    # in order to have the lowest SSD value as the \"max\" 1 value, multiply it with -10 beforehand\n",
    "\n",
    "    # therefore apply the softmax along the displacement dimension\n",
    "    # reshaping the cost tensor as follows: [1,121,H,W] -> [121, H*W] -> [H*W,121] : perform softmax along dim 1\n",
    "    soft_cost = F.softmax(-10 *cost.view(displace_range**2 ,-1).t() ,1)\n",
    "    # calculate displacement field (could be shorted when stacking x,y - but less intuitive)\n",
    "    # idea: 1) construct a meshgrid of all discrete displacement pairs per position\n",
    "    #       2) use broadcasting to get the weighted contributions of all displacement values (separated for x&y)\n",
    "    #          (soft_cost) [H*W,121] x [1,121] (xs,ys) : sum along dim 1 -> [H*W] -> reshape [H,W]\n",
    "\n",
    "    disp_hw = (displace_range -1 )//2\n",
    "    xs ,ys = torch.meshgrid(torch.arange(-disp_hw ,disp_hw +1).float(),\n",
    "                           torch.arange(-disp_hw ,disp_hw +1).float())\n",
    "\n",
    "    disp_x = (soft_cost *xs.reshape(1 ,-1)).sum(1).data.reshape(H ,W)#.cpu()\n",
    "    disp_y = (soft_cost *ys.reshape(1 ,-1)).sum(1).data.reshape(H ,W)#.cpu()\n",
    "        \n",
    "    # resample field to high resolution and add identity transform\n",
    "\n",
    "    # this factor is used to resize the displacements to a valid range since the CNN architecture\n",
    "    # downsamples the input images by a factor of 4\n",
    "    scale_factor = 4\n",
    "\n",
    "    # since so far only the position-wise displacements are computed, we also need to prepare the identity field\n",
    "    # in order to warp the moving images correctly\n",
    "    x ,y = torch.meshgrid(torch.arange(0 ,img_fixed.size(2)).float(),\n",
    "                         torch.arange(0 ,img_fixed.size(3)).float())\n",
    "\n",
    "    # before adding the displacements and the identity transform, the displacements need to be scaled according to\n",
    "    # the original image dimensionality (so far computed on the 4x downsampled feature maps)\n",
    "    disp_xy_up = scale_factor *F.interpolate(torch.stack((disp_x ,disp_y) ,0).cpu().unsqueeze(0),\n",
    "                                            size=(img_fixed.size(2) ,img_fixed.size(3)),\n",
    "                                            mode='bicubic')\n",
    "    xi = x + disp_xy_up[0 ,0 ,: ,:]\n",
    "    yi = y + disp_xy_up[0 ,1 ,: ,:]\n",
    "    \n",
    "    return soft_cost, disp_xy_up  # ,xi,yi\n",
    "\n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5 ,stride=1)\n",
    "    max1 = nn.MaxPool2d(3 ,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    # 1) switch dimensions in order to get per HW position the displacement with \"highest correlation\" by\n",
    "    # means of lowest SSD\n",
    "    # therefore, swap the dimensions of the tensor in order to make the pooling operations work along the\n",
    "    # displacement search region: [1,121,80,78] -> [1,80,78,121] -> [1,80*78,11,11] = [1,6240,11,11]\n",
    "    # using appropriate padding, -max(-x) u get the min value and 2x avg-pooling afterwards for quadratic smoothing\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0 ,2 ,3 ,1).reshape(1 ,-1 ,displace_range ,displace_range)))))\n",
    "\n",
    "    # 2)reconstruct the spatial arangement [1,121,80,78] and perform the spatial mean-field inference under valid padding\n",
    "    ssd_minconv = ssd_minconv.permute(0 ,2 ,3 ,1).view(1 ,-1 ,H ,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "\n",
    "    return min_conv_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "pat_indices = torch.cat((torch.arange(0, 17), torch.arange(18, 43)), 0)\n",
    "\n",
    "rnd_perm_idc = torch.randperm(pat_indices.size(0))\n",
    "pat_indices = pat_indices[rnd_perm_idc]\n",
    "# train_set = pat_indices[:35]\n",
    "# test_set = torch.cat((pat_indices[35:],torch.LongTensor([17])),0)\n",
    "\n",
    "# Now, we prepare our train & test dataset.\n",
    "test_set = torch.LongTensor([35, 41, 0, 4, 33, 38, 39, 17])\n",
    "train_set = torch.arange(43)\n",
    "for idx in test_set:\n",
    "    train_set = train_set[train_set != idx]\n",
    "\n",
    "print('Test_Set:', test_set)\n",
    "print('Train_Set:', train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57975ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OBELISK2d()\n",
    "net.apply(init_weights)\n",
    "net.train()\n",
    "net.cuda()\n",
    "#reg = deeds2d()\n",
    "\n",
    "H, W = img[0].size()\n",
    "\n",
    "class_weight = torch.sqrt(1.0/(torch.bincount(seg.view(-1)).float()))\n",
    "class_weight = class_weight/class_weight.mean()\n",
    "class_weight[0] = 0.15\n",
    "#class_weight[4] /= 10\n",
    "print('inv sqrt class_weight',class_weight)\n",
    "criterion = nn.CrossEntropyLoss(class_weight)\n",
    "\n",
    "epochs = 1\n",
    "optimizer = torch.optim.Adam(list(net.parameters()),lr=0.05)\n",
    "lambda_weight = 1.5#2#2.5#1.5\n",
    "displace_range = 15\n",
    "disp_hw = 5\n",
    "run_labelloss = torch.zeros(epochs)#/0\n",
    "run_diffloss = torch.zeros(epochs)\n",
    "losses = []\n",
    "\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    # tensor dimensionalities in comments are for an arbitrary choice of\n",
    "    # displace_range = 11 & feat sizes of [1,24,80,78];\n",
    "    # they clearly depend on the actual choice and only serve as numerical examples here.\n",
    "\n",
    "    disp_hw = (displace_range -1 )//2\n",
    "    # feat_mov: [1,24,80,78] -> 24 feature channels + spatial HW dims\n",
    "    # feat_mov_unfold: [24,121,6240] -> mind chans, 11*11 = 121 displ steps, 6240 = 80*78 spatial positions\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1 ,0) ,(displace_range ,displace_range) ,padding=disp_hw)\n",
    "\n",
    "    B ,C ,H ,W = feat_fixed.size()\n",
    "\n",
    "    # feat_fixed: [24,1,6240] -> compute scalarproduct along feature dimension per broadcast + sum along 0\n",
    "    # and reshape to [1,121,80,78]\n",
    "    ssd_distance = ((feat_fixed.view(C ,1 ,-1) - feat_moving_unfold )**2).sum(0).view(1 ,displace_range**2 ,H ,W)\n",
    "    # reshape the 4D tensor back to spatial dimensions\n",
    "    return ssd_distance\n",
    "\n",
    "def meanfield(ssd_distance ,img_fixed ,displace_range ,H ,W):\n",
    "\n",
    "    # crnt_dev = ssd_distance.device\n",
    "\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "    # probabilistic output: compute the contributions weights of every discrete displacement pair for all positions\n",
    "    # along 121 possible xy displacements -> normalize to [0,1] with softmax:\n",
    "    # in order to have the lowest SSD value as the \"max\" 1 value, multiply it with -10 beforehand\n",
    "\n",
    "    # therefore apply the softmax along the displacement dimension\n",
    "    # reshaping the cost tensor as follows: [1,121,H,W] -> [121, H*W] -> [H*W,121] : perform softmax along dim 1\n",
    "    soft_cost = F.softmax(-10 *cost.view(displace_range**2 ,-1).t() ,1)\n",
    "    # calculate displacement field (could be shorted when stacking x,y - but less intuitive)\n",
    "    # idea: 1) construct a meshgrid of all discrete displacement pairs per position\n",
    "    #       2) use broadcasting to get the weighted contributions of all displacement values (separated for x&y)\n",
    "    #          (soft_cost) [H*W,121] x [1,121] (xs,ys) : sum along dim 1 -> [H*W] -> reshape [H,W]\n",
    "\n",
    "    disp_hw = (displace_range -1 )//2\n",
    "    xs ,ys = torch.meshgrid(torch.arange(-disp_hw ,disp_hw +1).float(),\n",
    "                           torch.arange(-disp_hw ,disp_hw +1).float())\n",
    "\n",
    "    disp_x = (soft_cost *xs.reshape(1 ,-1)).sum(1).data.reshape(H ,W)#.cpu()\n",
    "    disp_y = (soft_cost *ys.reshape(1 ,-1)).sum(1).data.reshape(H ,W)#.cpu()\n",
    "        \n",
    "    # resample field to high resolution and add identity transform\n",
    "\n",
    "    # this factor is used to resize the displacements to a valid range since the CNN architecture\n",
    "    # downsamples the input images by a factor of 4\n",
    "    scale_factor = 4\n",
    "\n",
    "    # since so far only the position-wise displacements are computed, we also need to prepare the identity field\n",
    "    # in order to warp the moving images correctly\n",
    "    x ,y = torch.meshgrid(torch.arange(0 ,img_fixed.size(2)).float(),\n",
    "                         torch.arange(0 ,img_fixed.size(3)).float())\n",
    "\n",
    "    # before adding the displacements and the identity transform, the displacements need to be scaled according to\n",
    "    # the original image dimensionality (so far computed on the 4x downsampled feature maps)\n",
    "    disp_xy_up = scale_factor *F.interpolate(torch.stack((disp_x ,disp_y) ,0).cpu().unsqueeze(0),\n",
    "                                            size=(img_fixed.size(2) ,img_fixed.size(3)),\n",
    "                                            mode='bicubic')\n",
    "    xi = x + disp_xy_up[0 ,0 ,: ,:]\n",
    "    yi = y + disp_xy_up[0 ,1 ,: ,:]\n",
    "    \n",
    "    return soft_cost, disp_xy_up  # ,xi,yi\n",
    "\n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5 ,stride=1)\n",
    "    max1 = nn.MaxPool2d(3 ,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    # 1) switch dimensions in order to get per HW position the displacement with \"highest correlation\" by\n",
    "    # means of lowest SSD\n",
    "    # therefore, swap the dimensions of the tensor in order to make the pooling operations work along the\n",
    "    # displacement search region: [1,121,80,78] -> [1,80,78,121] -> [1,80*78,11,11] = [1,6240,11,11]\n",
    "    # using appropriate padding, -max(-x) u get the min value and 2x avg-pooling afterwards for quadratic smoothing\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0 ,2 ,3 ,1).reshape(1 ,-1 ,displace_range ,displace_range)))))\n",
    "\n",
    "    # 2)reconstruct the spatial arangement [1,121,80,78] and perform the spatial mean-field inference under valid padding\n",
    "    ssd_minconv = ssd_minconv.permute(0 ,2 ,3 ,1).view(1 ,-1 ,H ,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "\n",
    "    return min_conv_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e1ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    idx = train_set[torch.randperm(6)].view(2,3)[:,0]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    img00 = img[idx[0]:idx[0]+1,:,:].unsqueeze(0).float() / 255\n",
    "    img50 = img[idx[1]:idx[1]+1,:,:].unsqueeze(0).float() / 255\n",
    "    \n",
    "    label_moving = seg[idx[1]:idx[1]+1,:,:].contiguous().long()\n",
    "    label_fixed = seg[idx[0]:idx[0]+1,:,:].contiguous().long()\n",
    "    \n",
    "    moving_seg = labelMatrixOneHot(label_moving,9)\n",
    "    fixed_seg = labelMatrixOneHot(label_fixed,9)\n",
    "\n",
    "    #img00, seg50 = augmentAffine(img00_in,seg[idx[0]:idx[0]+1,:,:],0.0375)\n",
    "    img00.requires_grad = True\n",
    "    img50.requires_grad = True\n",
    "\n",
    "    \n",
    "    # get features (regular grid)\n",
    "    feat00 = net(img00.cuda())#checkpoint(net,img00)#net(img00)# #00 is fixed\n",
    "    feat50 = net(img50.cuda())#checkpoint(net,img50)#net(img50)# #50 is moving\n",
    "    # run differentiable deeds (regular grid)\n",
    "    # compute the cost tensor using the correlation layer\n",
    "    ssd_distance = correlation_layer(displace_range, feat50.cpu(), feat00.cpu())\n",
    "    \n",
    "    # compute the MIN-convolution & probabilistic output with the given function\n",
    "    soft_cost,pred_xy = meanfield(ssd_distance, img00, displace_range, h//3,w//3)\n",
    "    # loss computation:\n",
    "\n",
    "    #cost_soft,pred_xy =  reg(feat00,feat50)#checkpoint(reg,feat00,feat50)#reg(feat00,feat50)#\n",
    "    #pred_xy = pred_xy.view(1,grid_size,grid_size,2)\n",
    "    # evaluate diffusion regularisation loss\n",
    "    #print(pred_xy.shape)\n",
    "\n",
    "    \n",
    "    moving_seg = F.interpolate(moving_seg,size=(h//3,w//3),mode='bilinear')\n",
    "    print(moving_seg.shape)\n",
    "    label_moving_unfold = F.unfold(moving_seg,(displace_range,displace_range),padding=disp_hw).view(1,9,displace_range**2,-1)\n",
    "    print(label_moving_unfold.shape)\n",
    "    print(soft_cost.shape)\n",
    "    label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "    # compute the loss as sum of squared differences between the fixed label representation and the \"warped\" labels\n",
    "    label_distance1 = torch.sum(torch.pow(label_fixed.reshape(9,-1)-label_warped.reshape(9,-1),2),0)\n",
    "    loss = label_distance1.mean()\n",
    "    # perform the backpropagation and weight updates\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    optimizer.step()\n",
    "    if(i%50==49):\n",
    "\n",
    "        loss_avg = F.avg_pool1d(run_labelloss.view(1,1,-1),5,stride=1).squeeze().numpy()[:i]\n",
    "        print('run_labelloss',loss_avg[-1])\n",
    "        loss_avg = F.avg_pool1d(run_diffloss.view(1,1,-1),5,stride=1).squeeze().numpy()[:i]\n",
    "        print('run_diffloss',loss_avg[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9afb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test = test_set[torch.randperm(6)].view(2,3)[:,0]\n",
    "test_img_1 = img[random_test[0]:random_test[0]+1,:,:].unsqueeze(0).float() / 255\n",
    "test_img_2 = img[random_test[1]:random_test[1]+1,:,:].unsqueeze(0).float() / 255\n",
    "\n",
    "label_1= seg[random_test[0]:random_test[0]+1,:,:]\n",
    "label_2= seg[random_test[1]:random_test[1]+1,:,:]\n",
    "label_2.view(h,w,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613359dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = net(test_img_1)\n",
    "moving = net(test_img_2)\n",
    "\n",
    "test_cost_xy, test_pred_xy = reg(fixed,moving) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_xy = test_pred_xy.view(1,grid_size,grid_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resized = cv2.resize(test_pred_xy.reshape(grid_size,grid_size,2).detach().numpy(),(w,h))\n",
    "resized = F.interpolate(test_pred_xy,scale_factor=4,mode='bicubic')\n",
    "flow_img = flow2img(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52073d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94203b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flow_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = torch.from_numpy(resized).reshape(2,h,w)\n",
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = label_2.view(1,1,h,w).float()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_seg_2 = warp(target.cuda(), resized.cuda()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(overlaySegment(test_img_1.squeeze(),warped_seg_2.squeeze() ,False).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0abba99",
   "metadata": {},
   "source": [
    "# Exploration 20.05\n",
    "try to figure out how to upscale the pred_xy from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet\n",
    "from utils.plotting import flow2img\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d, deeds2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OBELISK2d()\n",
    "reg = deeds2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.load(\"Data/img.pth\")/255\n",
    "segs = torch.load(\"Data/seg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = 10; mov = 11\n",
    "fixed = img[fix:fix+1,:,:] / 255\n",
    "moving = img[mov:mov+1,:,:] / 255\n",
    "\n",
    "segs_fixed = segs[fix:fix+1,:,:]\n",
    "segs_moving = segs[mov:mov+1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e72c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = F.interpolate(fixed.unsqueeze(0), size=(120,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a04dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat00 = net(F.interpolate(fixed.unsqueeze(0), size=(120,120)))\n",
    "feat50 = net(F.interpolate(moving.unsqueeze(0), size=(120,120)))\n",
    "\n",
    "cost_soft, pred_xy = reg(feat00,feat50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)\n",
    "\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 11#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (F.grid_sample(seg_moving,grid_xy+shift_xy,padding_mode='border')\\\n",
    "                          *cost_soft.view(1,-1,32**2,11**2,1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c43f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_moving.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = grid_xy+shift_xy\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = F.grid_sample(seg_moving,grid_xy+shift_xy,padding_mode='border')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9091cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = (test1* cost_soft).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.view(8,120,-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b9b09",
   "metadata": {},
   "source": [
    "## Exploration 27.05\n",
    "explore 3D version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77247f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd73ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load(\"Data/img.pth\")\n",
    "H,W,D = imgs.shape\n",
    "\n",
    "o_m = H//3\n",
    "o_n = W//3\n",
    "o_o = D//3\n",
    "#print('numel_o',o_m*o_n*o_o)\n",
    "ogrid_xyz = F.affine_grid(torch.eye(3,4).unsqueeze(0),(1,1,o_m,o_n,o_o)).view(1,1,-1,1,3)\n",
    "\n",
    "\n",
    "class OBELISK(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(OBELISK, self).__init__()\n",
    "        channels = 24#16\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels*2,3)*0.05)\n",
    "        self.layer0 = nn.Conv3d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm3d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv3d(channels*8, channels*4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm3d(channels*4)\n",
    "        self.layer2 = nn.Conv3d(channels*4, channels*4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm3d(channels*4)\n",
    "        self.layer3 = nn.Conv3d(channels*4, channels*1, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool3d(input_img,3,padding=1,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in,ogrid_xyz + self.offsets[0,:,:].view(1,-1,1,1,3)).view(1,-1,o_m,o_n,o_o)\n",
    "        sampled -= F.grid_sample(img_in,ogrid_xyz + self.offsets[1,:,:].view(1,-1,1,1,3)).view(1,-1,o_m,o_n,o_o)\n",
    "    \n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "disp_range = 0.4#0.25\n",
    "displacement_width = 15#11#17\n",
    "shift_xyz = F.affine_grid(disp_range*torch.eye(3,4).unsqueeze(0),(1,1,displacement_width,displacement_width,displacement_width)).view(1,1,-1,1,3)#.cuda()\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xyz = F.affine_grid(torch.eye(3,4).unsqueeze(0),(1,1,grid_size,grid_size,grid_size)).view(1,-1,1,1,3)#.cuda()\n",
    "\n",
    "class deeds(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(deeds, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.Tensor([1,.1,1,1,.1,1]))#.cuda()\n",
    "\n",
    "        self.pad1 = nn.ReplicationPad3d(3)#.cuda()\n",
    "        self.avg1 = nn.AvgPool3d(3,stride=1)#.cuda()\n",
    "        self.max1 = nn.MaxPool3d(3,stride=1)#.cuda()\n",
    "        self.pad2 = nn.ReplicationPad3d(2)#.cuda()##\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, feat00,feat50):\n",
    "        \n",
    "        #deeds correlation layer (slightly unrolled)\n",
    "        deeds_cost = torch.zeros(1,grid_size**3,displacement_width,displacement_width,displacement_width)#.cuda()\n",
    "        xyz8 = grid_size**2\n",
    "        for i in range(grid_size): \n",
    "            moving_unfold = F.grid_sample(feat50,grid_xyz[:,i*xyz8:(i+1)*xyz8,:,:,:] + shift_xyz,padding_mode='border')\n",
    "            fixed_grid = F.grid_sample(feat00,grid_xyz[:,i*xyz8:(i+1)*xyz8,:,:,:])\n",
    "            deeds_cost[:,i*xyz8:(i+1)*xyz8,:,:,:] = self.alpha[1]+self.alpha[0]*torch.sum(torch.pow(fixed_grid-moving_unfold,2),1).view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "\n",
    "        # remove mean (not really necessary)\n",
    "        #deeds_cost = deeds_cost.view(-1,displacement_width**3) - deeds_cost.view(-1,displacement_width**3).mean(1,keepdim=True)[0]\n",
    "        deeds_cost = deeds_cost.view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "    \n",
    "        # approximate min convolution / displacement compatibility\n",
    "        cost = self.avg1(self.avg1(-self.max1(-self.pad1(deeds_cost))))\n",
    "   \n",
    "        # grid-based mean field inference (one iteration)\n",
    "        cost_permute = cost.permute(2,3,4,0,1).view(1,displacement_width**3,grid_size,grid_size,grid_size)\n",
    "        cost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0,2,3,4,1).view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "        \n",
    "        # second path\n",
    "        cost = self.alpha[4]+self.alpha[2]*deeds_cost+self.alpha[3]*cost_avg\n",
    "        cost = self.avg1(self.avg1(-self.max1(-self.pad1(cost))))\n",
    "        # grid-based mean field inference (one iteration)\n",
    "        cost_permute = cost.permute(2,3,4,0,1).view(1,displacement_width**3,grid_size,grid_size,grid_size)\n",
    "        cost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0,2,3,4,1).view(grid_size**3,displacement_width**3)\n",
    "        #cost = alpha[4]+alpha[2]*deeds_cost+alpha[3]*cost.view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "        #cost = avg1(avg1(-max1(-pad1(cost))))\n",
    "        \n",
    "        #probabilistic and continuous output\n",
    "        cost_soft = F.softmax(-self.alpha[5]*cost_avg,1)\n",
    "#        pred_xyz = torch.sum(F.softmax(-5self.alpha[2]*cost_avg,1).unsqueeze(2)*shift_xyz.view(1,-1,3),1)\n",
    "        pred_xyz = torch.sum(cost_soft.unsqueeze(2)*shift_xyz.view(1,-1,3),1)\n",
    "\n",
    "\n",
    "\n",
    "        return cost_soft,pred_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55632c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OBELISK()\n",
    "reg = deeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_weights = torch.load(\"models/pdd_net/dense_reg_feat_v01.pth\")\n",
    "net_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(net_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41c856",
   "metadata": {},
   "source": [
    "# Explore 01.06.21\n",
    "reworked Obelisk layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.load_models import init_weights\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils.plotting import countParameters, flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "import cv2\n",
    "from utils.layers import warp \n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load(\"Data/img.pth\")\n",
    "segs = torch.load(\"Data/seg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,h = (320,256)\n",
    "o_m = h//3\n",
    "o_n = w//3\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2)#.cuda()\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 15#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2)#.cuda()\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "\tdef __init__(self, chan=16):\n",
    "\t\tsuper(OBELISK2d, self).__init__()\n",
    "\t\tchannels = chan\n",
    "\t\tself.offsets = nn.Parameter(torch.randn(2, channels * 2, 2) * 0.05)\n",
    "\t\tself.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "\t\tself.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "\t\tself.layer1 = nn.Conv2d(channels * 8, channels * 4, 1, bias=False, groups=1)\n",
    "\t\tself.batch1 = nn.BatchNorm2d(channels * 4)\n",
    "\t\tself.layer2 = nn.Conv2d(channels * 4, channels * 4, 3, bias=False, padding=1)\n",
    "\t\tself.batch2 = nn.BatchNorm2d(channels * 4)\n",
    "\t\tself.layer3 = nn.Conv2d(channels * 4, channels * 1, 1)\n",
    "\n",
    "\tdef forward(self, input_img):\n",
    "\t\timg_in = F.avg_pool2d(input_img, 3, padding=1, stride=2)\n",
    "\t\timg_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "\t\tsampled = F.grid_sample(img_in, ogrid_xy + self.offsets[0, :, :].view(1, -1, 1, 2)).view(1, -1, o_m, o_n)\n",
    "\t\tsampled -= F.grid_sample(img_in, ogrid_xy + self.offsets[1, :, :].view(1, -1, 1, 2)).view(1, -1, o_m, o_n)\n",
    "\n",
    "\t\tx = F.relu(self.batch1(self.layer1(sampled)))\n",
    "\t\tx = F.relu(self.batch2(self.layer2(x)))\n",
    "\t\tfeatures = self.layer3(x)\n",
    "\t\treturn features\n",
    "\n",
    "\n",
    "class deeds2d(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\tsuper(deeds2d, self).__init__()\n",
    "\t\tself.alpha = nn.Parameter(torch.Tensor([1 ,.1 ,1 ,1 ,.1 ,1])  )  # .cuda()\n",
    "\n",
    "\t\tself.pad1 = nn.ReplicationPad2d(3  )  # .cuda()\n",
    "\t\tself.avg1 = nn.AvgPool2d(3 ,stride=1  )  # .cuda()\n",
    "\t\tself.max1 = nn.MaxPool2d(3 ,stride=1  )  # .cuda()\n",
    "\t\tself.pad2 = nn.ReplicationPad2d(2  )  # .cuda()##\n",
    "\n",
    "\tdef forward(self, feat00 ,feat50):\n",
    "\n",
    "\t\t# deeds correlation layer (slightly unrolled)\n",
    "\t\tdeeds_cost = torch.zeros(1 ,grid_size**2 ,displacement_width, displacement_width)#.cuda()\n",
    "\n",
    "\t\t# print(deeds_cost.shape)\n",
    "\t\txy8 = grid_size**2\n",
    "\t\t# i=1\n",
    "\t\t# print(grid_xy[:,i*grid_size:(i+1)*grid_size,:,:].shape)\n",
    "\t\tfor i in range(grid_size):\n",
    "\t\t\tmoving_unfold = F.grid_sample(feat50, grid_xy[: , i *grid_size:( i +1 ) *grid_size ,: ,:] + shift_xy\n",
    "\t\t\t\t\t\t\t\t\t\t  ,padding_mode='border')\n",
    "\t\t\tfixed_grid = F.grid_sample(feat00, grid_xy[: , i *grid_size:( i +1 ) *grid_size ,: ,:]) # grid_xy[:,i*xy8:(i+1)*xy8,:,:]\n",
    "\t\t\tdeeds_cost[: , i *grid_size:( i +1 ) *grid_size ,: ,:] = self.alpha[1 ] +self.alpha[0 ] *torch.sum \\\n",
    "\t\t\t\t(torch.pow(fixed_grid -moving_unfold ,2) ,1).view(1 ,-1 ,displacement_width ,displacement_width)\n",
    "\n",
    "\n",
    "\t\t# remove mean (not really necessary)\n",
    "\t\t# deeds_cost = deeds_cost.view(-1,displacement_width**3) - deeds_cost.view(-1,displacement_width**3).mean(1,keepdim=True)[0]\n",
    "\t\t# deeds_cost = deeds_cost.view(1,-1,displacement_width,displacement_width)\n",
    "\t\t# print(deeds_cost.shape)\n",
    "\n",
    "\t\t# approximate min convolution / displacement compatibility\n",
    "\t\tcost = self.avg1(self.avg1(-self.max1(-self.pad1(deeds_cost))))\n",
    "\t\t# grid-based mean field inference (one iteration)\n",
    "\t\tcost_permute = cost.permute(2 ,3 ,0 ,1).view(1 ,displacement_width**2 ,grid_size, grid_size)\n",
    "\t\tcost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0 ,2 ,3 ,1).view(1 ,-1 ,displacement_width\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ,displacement_width)\n",
    "\n",
    "\t\t# second path\n",
    "\t\tcost = self.alpha[4 ] +self.alpha[2 ] *deeds_cost +self.alpha[3 ] *cost_avg\n",
    "\t\tcost = self.avg1(self.avg1(-self.max1(-self.pad1(cost))))\n",
    "\n",
    "\t\t# grid-based mean field inference (one iteration)\n",
    "\t\tcost_permute = cost.permute(2 ,3 ,0 ,1).view(1 ,displacement_width**2 ,grid_size ,grid_size)\n",
    "\t\tcost_avg = self.avg1(self.avg1(self.pad2(cost_permute))).permute(0 ,2 ,3 ,1).view(grid_size**2 ,displacement_width**2)\n",
    "\t\t# cost = alpha[4]+alpha[2]*deeds_cost+alpha[3]*cost.view(1,-1,displacement_width,displacement_width,displacement_width)\n",
    "\t\t# cost = avg1(avg1(-max1(-pad1(cost))))\n",
    "\n",
    "\t\t# probabilistic and continuous output\n",
    "\t\tcost_soft = F.softmax(-self.alpha[5 ] *cost_avg ,1)\n",
    "\t\t# pred_xyz = torch.sum(F.softmax(-5self.alpha[2]*cost_avg,1).unsqueeze(2)*shift_xyz.view(1,-1,3),1)\n",
    "\t\tpred_xy = torch.sum(cost_soft.unsqueeze(2 ) *shift_xy.view(1 ,-1 ,2) ,1)\n",
    "\n",
    "\n",
    "\t\treturn cost_soft ,pred_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0009710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs = 100\n",
    "lr = 0.005\n",
    "delta = 0.5\n",
    "\n",
    "net = OBELISK2d()\n",
    "init_weights(net)\n",
    "net.train()\n",
    "\n",
    "reg = deeds2d()\n",
    "\n",
    "init_weights(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=list(net.parameters()) + list(reg.parameters()), lr=lr)\n",
    "\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 15#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2) \n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)\n",
    "\n",
    "total_loss = []\n",
    "\n",
    "kl_loss = torch.nn.KLDivLoss()\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "pat_indices = torch.cat((torch.arange(0, 17), torch.arange(18, 43)), 0)\n",
    "\n",
    "rnd_perm_idc = torch.randperm(pat_indices.size(0))\n",
    "pat_indices = pat_indices[rnd_perm_idc]\n",
    "\n",
    "# Now, we prepare our train & test dataset.\n",
    "test_set = torch.LongTensor([35, 41, 0, 4, 39, 17])\n",
    "train_set = torch.arange(43)\n",
    "for idx in test_set:\n",
    "    train_set = train_set[train_set != idx]\n",
    "\n",
    "diffs = []\n",
    "for epoch in range(epochs):\n",
    "     # get training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "\n",
    "    fixed = imgs[p_fix:p_fix+1,:,:] / 255\n",
    "    moving = imgs[p_mov:p_mov+1,:,:] / 255\n",
    "    #Flow output from teacher1\n",
    "    \n",
    "    fixed_seg = segs[p_fix:p_fix+1,:,:]\n",
    "    moving_seg = segs[p_mov:p_mov+1,:,:]\n",
    "        \n",
    "    #flow_in = preprocessing_flownet(fixed.reshape(h,w,C),moving.reshape(h,w,C)).cuda()\n",
    "    #teacher_flow = flow2(flow_in).cpu()\n",
    "    #warped_moving_teacher = warpImage(labelMatrixOneHot(moving_seg,9), teacher_flow)\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fixed = Variable(fixed, requires_grad=True)           #fixed\n",
    "    moving = Variable(moving, requires_grad=True)         #moving\n",
    "    \n",
    "    feat00 = net(fixed.unsqueeze(0))\n",
    "    feat50 = net(moving.unsqueeze(0))\n",
    "\n",
    "    # Need some help here. I am not using the soft_cost at all. Not sure how tho\n",
    "    soft_cost, pred_xy = reg(feat00,feat50)\n",
    "    \n",
    "    pred_xy = pred_xy.view(1,grid_size,grid_size,2)\n",
    "    \n",
    "    diffloss = 1.5*((pred_xy[0,:,1:,:]-pred_xy[0,:,:-1,:])**2).mean()+\\\n",
    "            1.5*((pred_xy[0,1:,:,:]-pred_xy[0,:-1,:,:])**2).mean()+\\\n",
    "            1.5*((pred_xy[0,:,:,1:]-pred_xy[0,:,:,:-1])**2).mean()\n",
    "    \n",
    "    warped_seg = (F.grid_sample(labelMatrixOneHot(moving_seg,9).float(),grid_xy+shift_xy,padding_mode='border')\\\n",
    "                          *soft_cost.view(1,-1,grid_size**2,displacement_width**2)).sum(1,keepdim=True)\n",
    "    #warped_teacher = F.grid_sample(warped_moving_teacher, grid_xy).detach()\n",
    "\n",
    "    #teacher_loss = torch.pow((warped_seg - warped_teacher),2).mean()\n",
    "    \n",
    "    nonlocal_label = (F.grid_sample(labelMatrixOneHot(moving_seg,9).float(),grid_xy+shift_xy,padding_mode='border')\\\n",
    "                          *soft_cost.view(1,-1,grid_size**2,displacement_width**2)).sum(1,keepdim=True)\n",
    "    fixed_label = F.grid_sample(labelMatrixOneHot(fixed_seg,9).float(),grid_xy,padding_mode='border')#.long().squeeze(1)\n",
    "\n",
    "    orig_labelloss = ((nonlocal_label-fixed_label)**2).mean()\n",
    "    \n",
    "    \n",
    "    loss = (diffloss + orig_labelloss) #+ (1-delta)*teacher_loss + delta*orig_labelloss)\n",
    "    loss.backward()\n",
    "    diffs.append(loss.item())\n",
    "    \n",
    "    #diff.backward()\n",
    "    #diffs.append(diff.item())\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "reg.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "p_mov = test_set[rnd_test_idx[1]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,:,:] / 255\n",
    "moving = imgs[p_mov:p_mov+1,:,:] / 255\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,:,:]\n",
    "moving_seg = segs[p_mov:p_mov+1,:,:]\n",
    "\n",
    "feat1 = net(fixed.unsqueeze(0))\n",
    "feat2 = net(moving.unsqueeze(0))\n",
    "\n",
    "soft_cost, pred_xy = reg(feat1,feat2)\n",
    "\n",
    "dense_flow_fit = F.interpolate(pred_xy.transpose(0,1).view(1,2,grid_size,grid_size),size=(h,w),mode='bicubic')\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,h,w),align_corners=False)\n",
    "warped_seg = F.grid_sample(moving_seg.float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False)\n",
    "d1 = dice_coeff(fixed_seg,warped_seg.squeeze(),9)\n",
    "print(d1,d1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "reg.eval()\n",
    "\n",
    "feat00 = net(fixed.unsqueeze(0))\n",
    "feat50 = net(moving.unsqueeze(0))\n",
    "\n",
    "cost_soft, pred_xy = reg(feat00,feat50)\n",
    "dense_flow_fit = F.interpolate(pred_xy.transpose(0,1).view(1,2,grid_size,grid_size),size=(h,w),mode='bicubic')\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,h,w),align_corners=False)\n",
    "warped_seg = F.grid_sample(moving_seg.float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False)\n",
    "d1 = dice_coeff(fixed_seg,warped_seg.squeeze(),9)\n",
    "print(d1,d1.mean())\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(\"PDD-Output\")\n",
    "plt.imshow(showFlow(pred_xy.transpose(0,1).reshape(2,grid_size,grid_size).detach()))\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Fixed')\n",
    "plt.imshow(fixed.detach().squeeze())\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Moving')\n",
    "plt.imshow(moving.detach().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b705d",
   "metadata": {},
   "source": [
    "# Student creation with Sequential and Obelisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# This cell imports all requires packages and provides namespace aliases\n",
    "import torch.optim as optim\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.encoding import dice_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048618f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan=16, input_size=(100,100)):\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.H = input_size[0]\n",
    "        self.W = input_size[1]\n",
    "        self.o_m = input_size[0]//4\n",
    "        self.o_n = input_size[1]//4\n",
    "        self.ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,self.o_m,self.o_n)).view(1,1,-1,2)\n",
    "        \n",
    "        self.offsets = nn.Parameter(torch.randn(2, channels * 2, 2) * 0.05)\n",
    "        \n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels * 8, channels * 4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer2 = nn.Conv2d(channels * 4, channels * 4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer3 = nn.Conv2d(channels * 4, channels * 1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img, 3, padding=1, stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        \n",
    "        sampled = F.grid_sample(img_in, self.ogrid_xy + self.offsets[0, :, :].view(1, -1, 1, 2)).view(1, -1, self.o_m, self.o_n)\n",
    "        sampled -= F.grid_sample(img_in, self.ogrid_xy + self.offsets[1, :, :].view(1, -1, 1, 2)).view(1, -1, self.o_m, self.o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class PDD(nn.Module):\n",
    "    def __init__(self, input_size=(100,100), feature_channels=24, sequential = False, displace_range=11, disp_hw=5, ):\n",
    "        super(PDD, self).__init__()\n",
    "        self.displace_range = displace_range\n",
    "        self.H = input_size[0]\n",
    "        self.W = input_size[1]\n",
    "        \n",
    "        if sequential:\n",
    "            self.feature_extractor = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                                                         torch.nn.BatchNorm2d(32),\n",
    "                                                         torch.nn.PReLU(),\n",
    "                                                         torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                                                         torch.nn.BatchNorm2d(32),\n",
    "                                                         torch.nn.PReLU(),\n",
    "                                                         torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                                                         torch.nn.BatchNorm2d(64),\n",
    "                                                         torch.nn.PReLU(),\n",
    "                                                         torch.nn.Conv2d(64,feature_channels,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                                                         torch.nn.Sigmoid())\n",
    "        else:\n",
    "            self.feature_extractor = OBELISK2d(feature_channels, input_size)\n",
    "        \n",
    "    \n",
    "    def correlation_layer(self,feat_moving, feat_fixed):\n",
    "    \n",
    "        disp_hw = (self.displace_range-1)//2\n",
    "        feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(self.displace_range,self.displace_range),padding=disp_hw)\n",
    "        B,C,H,W = feat_fixed.size()\n",
    "\n",
    "        ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,self.displace_range**2,H,W)\n",
    "\n",
    "        return ssd_distance\n",
    "    \n",
    "    def min_convolution(self,ssd_distance, H, W):\n",
    "        # Prepare operators for smooth dense displacement space\n",
    "        pad1 = nn.ReplicationPad2d(5)\n",
    "        avg1 = nn.AvgPool2d(5,stride=1)\n",
    "        max1 = nn.MaxPool2d(3,stride=1)\n",
    "        pad2 = nn.ReplicationPad2d(6)\n",
    "        # approximate min convolution / displacement compatibility\n",
    "\n",
    "        ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,self.displace_range,self.displace_range)))))\n",
    "\n",
    "        ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1, H, W)\n",
    "        min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "\n",
    "        return min_conv_cost\n",
    "\n",
    "    def meanfield(self,ssd_distance,img_fixed, H, W):\n",
    "\n",
    "        crnt_dev = ssd_distance.device\n",
    "        \n",
    "        cost = self.min_convolution(ssd_distance, H, W)\n",
    "\n",
    "        soft_cost = F.softmax(-10*cost.view(self.displace_range**2,-1).t(),1)\n",
    "\n",
    "        disp_hw = (self.displace_range-1)//2\n",
    "        disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,self.displace_range,self.displace_range),align_corners=True)\n",
    "        disp_mesh_grid /= torch.Tensor([(self.W-1)*.5,(self.H-1)*.5])\n",
    "\n",
    "        disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "\n",
    "        return soft_cost,disp_xy\n",
    "    \n",
    "    \n",
    "    def forward(self,fixed_img,moving_img):\n",
    "        \n",
    "        feat_fixed = self.feature_extractor(fixed_img)\n",
    "        feat_moving = self.feature_extractor(moving_img)\n",
    "        \n",
    "        assert feat_fixed.shape == feat_moving.shape\n",
    "        \n",
    "        ssd_distance = self.correlation_layer(feat_moving, feat_fixed)\n",
    "        soft_cost, pred_xy = self.meanfield(ssd_distance, fixed_img, self.H//4, self.W//4)\n",
    "        \n",
    "        return soft_cost, pred_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load('Data/img.pth')\n",
    "segs = torch.load('Data/seg.pth')\n",
    "H,W = imgs[0].shape\n",
    "\n",
    "p_fix = 0\n",
    "p_mov= 10\n",
    "\n",
    "test_fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1).float() / 255\n",
    "test_moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1).float() / 255\n",
    "\n",
    "\n",
    "pdd = PDD(input_size=(H,W), feature_channels=24, displace_range=11)\n",
    "soft_cost, disp_xy = pdd(test_fixed,test_moving)\n",
    "\n",
    "print(soft_cost.shape)\n",
    "print(disp_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a75ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = 12; mov = 3\n",
    "#extract two segmentations and compute Dice\n",
    "moving_seg = (seg[mov:mov+1,:,:]).long().contiguous()\n",
    "fixed_seg = (seg[fix:fix+1,:,:]).long().contiguous()\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,8)\n",
    "print(d0,d0.mean())\n",
    "#load images and apply feature extraction network\n",
    "moving_img = (img[mov:mov+1,:,:]).float().unsqueeze(1)\n",
    "fixed_img = (img[fix:fix+1,:,:]).float().unsqueeze(1)\n",
    "H,W = fixed_img.shape[-2:]\n",
    "#display overlay of moving segmentation onto fixed image\n",
    "rgb = overlaySegment(fixed_img.squeeze().t().flip(0)/255,moving_seg.squeeze().t().flip(0),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.arange(43)\n",
    "train_set = train_set[torch.arange(43).remainder(3)>0] \n",
    "\n",
    "displace_range = 15\n",
    "#train the network with your own discrete registration network and label loss\n",
    "pdd = PDD(input_size=(H,W), feature_channels=24, displace_range=displace_range)\n",
    "crnt_dev = torch.ones(1).device\n",
    "#if(torch.cuda.is_available()):\n",
    "    #crnt_dev = torch.ones(1).cuda().device\n",
    "#net = torch.load('mindnet_cnn.pth')\n",
    "pdd.to(crnt_dev)\n",
    "pdd.feature_extractor.train()\n",
    "\n",
    "optimizer = optim.Adam(list(pdd.feature_extractor.parameters()),lr=0.00025)\n",
    "disp_hw = 5\n",
    "nr_train_pairs = 1000\n",
    "grad_accum = 4\n",
    "\n",
    "losses = []\n",
    "for pdx in range(nr_train_pairs):\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "    p_fix = train_set[rnd_train_idx[0]]\n",
    "    p_mov = train_set[rnd_train_idx[1]]\n",
    "\n",
    "    img_fixed = img[p_fix:p_fix+1,:,:].unsqueeze(1).float().to(crnt_dev)/255#.to(crnt_dev)\n",
    "    img_moving = img[p_mov:p_mov+1,:,:].unsqueeze(1).float().to(crnt_dev)/255#.to(crnt_dev)\n",
    "    #feat_fixed = seq(img_fixed)\n",
    "    #feat_moving = seq(img_moving)\n",
    "\n",
    "    seg_fixed = seg[p_fix:p_fix+1,:,:].long().contiguous()\n",
    "    seg_moving = seg[p_mov:p_mov+1,:,:].long().contiguous()\n",
    "    \n",
    "    label_moving = F.one_hot(seg_moving,num_classes=9).permute(0,3,1,2).float()\n",
    "    _,C1,Hf,Wf = label_moving.size()\n",
    "    label_moving = F.interpolate(label_moving,size=(H//4,Wf//4),mode='bilinear')\n",
    "    label_fixed = F.one_hot(seg_fixed,num_classes=9).permute(0,3,1,2).float()\n",
    "    label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "    # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "    # according to the corresponding discrete displacement pair\n",
    "    label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,9,displace_range**2,-1)\n",
    "    \n",
    "\n",
    "    #forward path: pass both images through the network so that the weights appear in the computation graph\n",
    "    # and will be updated\n",
    "    # feat_fixed = seq(img_fixed)\n",
    "    # feat_moving = seq(img_moving)\n",
    "    # compute the cost tensor using the correlation layer\n",
    "    # ssd_distance = correlation_layer(displace_range, feat_moving, feat_fixed)\n",
    "    \n",
    "    # compute the MIN-convolution & probabilistic output with the given function\n",
    "    # soft_cost,disp_xy = meanfield(ssd_distance, img_fixed, displace_range, H//4, W//4)\n",
    "    soft_cost, disp_xy = pdd(img_fixed, img_moving)\n",
    "    # loss computation:\n",
    "    # compute the weighted sum of the shifted moving label versions \n",
    "    print(label_moving_unfold.squeeze(0).shape)\n",
    "    print(soft_cost.t().unsqueeze(0).shape)\n",
    "    print(displace_range, displace_range**2)\n",
    "    label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "    # compute the loss as sum of squared differences between the fixed label representation and the \"warped\" labels\n",
    "    label_distance1 = torch.sum(torch.pow(label_fixed.reshape(9,-1)-label_warped.reshape(9,-1),2),0)\n",
    "    loss = label_distance1.mean()\n",
    "    # perform the backpropagation and weight updates\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (pdx+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations : backpropagate the accumulated gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if(pdx%(nr_train_pairs/4)==((nr_train_pairs/4)-1)):\n",
    "        print(pdx,loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import showFlow\n",
    "#run the trained network once more on two unseen images\n",
    "#process features\n",
    "with torch.no_grad():\n",
    "    #fixed_feat = net(fixed_img.cuda()).cpu()\n",
    "    #moving_feat = net(moving_img.cuda()).cpu()\n",
    "    soft_cost, disp_xy = pdd(fixed_img.to(crnt_dev),moving_img.to(crnt_dev))\n",
    "\n",
    "#compute 4D tensor\n",
    "#ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "#soft_cost,disp_xy = meanfield(ssd_distance, fixed_img, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,scale_factor=4,mode='bicubic')\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "warped_seg = F.grid_sample(moving_seg.float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False)\n",
    "d1 = dice_coeff(fixed_seg,warped_seg.squeeze(),8)\n",
    "d2 = dice_coeff(fixed_seg,moving_seg,8)\n",
    "print(d1,d1.mean()) #would be around 57-59% after training the CNNs\n",
    "print(d2,d2.mean())\n",
    "\n",
    "\n",
    "#visualise\n",
    "rgb = showFlow(dense_flow_fit.transpose(-2,-1).flip(1))\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "rgb = overlaySegment(fixed_img.squeeze().t().flip(0)/255,warped_seg.data.squeeze().t().flip(0),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca2a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
