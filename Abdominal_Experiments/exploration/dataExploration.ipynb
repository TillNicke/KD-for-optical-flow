{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e68b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import medpy\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7126524",
   "metadata": {},
   "source": [
    "## Exploring Data from  usliverseq/volunteer01\n",
    "The other volunteers seem to have emtpy video files. Volunteer 01 however is provided in bmp files. These can be used. It contains about 14500 frames, which need to be cropped and could be used for flow stimation. However, it is not clear how well they are really suited for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Data/usliverseq/volunteer02.avi\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "frames = np.zeros(200)\n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    ret,frame = cap.read() \n",
    "    print(frame)\n",
    "    frames[i] = frame\n",
    "    i += 1\n",
    "    if i == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd15f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.ArtistAnimation(fig, frames, interval=100, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997861",
   "metadata": {},
   "source": [
    "This dataset is from USTRackedDataset. It contains ultrasound data, which is saved as mha file. The images can be processed into numpy arrays and seem to be clear, as they recorded invitro with a machine.\n",
    "They are worth exploring for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from medpy.io import load\n",
    "import SimpleITK as sitk\n",
    "img = sitk.ReadImage('Data/UsTrackedDataset/DATA/babyPhantom/completeSequence/Scan02_vel_0_00125_F15N/BP_Scan02_vel_0_00125_F_15N.mha', sitk.sitkFloat32)\n",
    "#mage_data, image_header = load('Data/UsTrackedDataset/DATA/babyPhantom/completeSequence/Scan02_vel_0_00125_F15N/BP_Scan02_vel_0_00125_F_15N.mha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed60128",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = sitk.GetArrayFromImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45bd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(600):\n",
    "    frames.append([ax.imshow(array[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=100)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049522b4",
   "metadata": {},
   "source": [
    "## Playing around with the abdominal phantom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medpy.io import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c95cd",
   "metadata": {},
   "source": [
    "### Unprocessed Ultrasound data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mage_data, image_header = load('Data/abdominal_phantom/Scan01_A_0_05_P_0_0375/AP_Scan01_A_0_05_P_0_0375.mha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "frames = []\n",
    "for img in mage_data.T[9000:10000]:\n",
    "    frames.append([ax.imshow(img, cmap='gray')])\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=100)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641618",
   "metadata": {},
   "source": [
    "Could work. Needs to be explored in an experiment\n",
    "\n",
    "| Frames | notes |\n",
    "| --- | --- |\n",
    "| 0-1000 | leave out |\n",
    "| 1000-2000 | leave out |\n",
    "| 2000-3000 | leave out |\n",
    "| 3000-4000 | leave out |\n",
    "| 4000-5000 | leave out |\n",
    "| 5000-6000 | leave out |\n",
    "| 6000-7000 | Can be used |\n",
    "| 7000-8000 | leave out |\n",
    "| 8000-9000 | leabe out |\n",
    "| 9000-10000 | Can be used |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f0f1e",
   "metadata": {},
   "source": [
    "### Processed Us data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1241f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mage_data, image_header = load('Data/abdominal_phantom/Scan01_A_0_05_P_0_0375/AP_Reconstructed_Scan01_A_0_05_P_0_0375.mha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bf836",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "frames = []\n",
    "for img in mage_data:\n",
    "    frames.append([ax.imshow(img.T, cmap='gray')])\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=100)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c516f",
   "metadata": {},
   "source": [
    "Not sure if that is a good idea, or rather take the frames from the unprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f1008",
   "metadata": {},
   "source": [
    "# Exploring COCOAI US Data images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import medpy\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.image as mpllimg\n",
    "from utils.load_models import load_flownet2, load_pwcnet\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.plotting import flow2img\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/cocoai-thinksono-data/compressions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = pd.read_csv(\"Data/cocoai-thinksono-data/quality.csv\")\n",
    "quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f8147",
   "metadata": {},
   "source": [
    "There are some things, that need to be fulfilled in orer for the sequence to be used for training. The table below sums up all the different Columns and what needs to be fulfilled in order to use the ID for the training\n",
    "\n",
    "| Quality Description | useable Index | Description |\n",
    "|---|---|---|\n",
    "|Vessel boundary| 1,2,4| Vessel partially cut of or fully visible during frames|\n",
    "|LM configuration| 1,2,3,4| Landmarks don't need to be present for flow estimation|\n",
    "|Vein contrast| 1,2 | some loss of contrast, but still visible|\n",
    "|Artery contrast| 1,2 | some loss of contrast, but stil visible|\n",
    "|shaprnes of Vein boundaries| 1,2,3 | up until poorly visible boundaries, We try to learn the overall flow|\n",
    "|shaprnes of artery boundaries| 1,2,3 | up until poorly visible boundaries, We try to learn the overall flow|\n",
    "|Overall gain| 1 | only good images are taken. Not too bright or too dark|\n",
    "|artefacts| 1,2 | some aftefacts are present in the some frames|\n",
    "| Movement Sequence| 1,2,3, | Exclude \"lot's\" of movement for now. The other indicate some slow or fast compression of the vein|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "useable = quality[\n",
    "    (quality[\"Vessel in Frame\"] <3) &\n",
    "    (quality['LM configuration'] < 5) &\n",
    "    (quality['Vein contrast/cropping error'] < 3) &\n",
    "    (quality['Artery contrast'] < 3) &\n",
    "    (quality['Vein boundary'] < 4) &\n",
    "    (quality['Artery boundary'] < 4) &\n",
    "    (quality['Gain'] == 1) &\n",
    "    (quality['Artefacts'] < 3) &\n",
    "    (quality['Movement'] < 4)\n",
    "]\n",
    "useable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4dbc8d",
   "metadata": {},
   "source": [
    "Using the indices in the table mentioned above, 2048 examples can be used. From these 2048, I will first use max 500, to train and if everything works out, use the rest for validation. If I am ambitious, I can train on 2000 random examples and use 48 for validation. However, these have to be determined beforehand "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c197",
   "metadata": {},
   "source": [
    "Below, the dataframe for the \"compression\" folder is shown. Here an overview over devices, anatomy, etc. is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de070cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pd.read_csv(\"Data/cocoai-thinksono-data/sequences.csv\")\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first from useable\n",
    "path_to_file = path + useable['ID'][0].astype(int).astype(str) + \"/frames\"\n",
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#frames = []\n",
    "#for file in os.listdir(path_to_file):\n",
    "#    img = mpllimg.imread(os.path.join(path_to_file,file))\n",
    "#    frames.append([ax.imshow(img)])\n",
    "#ani = animation.ArtistAnimation(fig, frames, interval=500, blit=True,\n",
    "#                                repeat_delay=100)\n",
    "#HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79486d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow2 = load_flownet2().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "ridx = np.random.choice(os.listdir(path_to_file), 6)\n",
    "ridx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [torch.from_numpy(mpllimg.imread(os.path.join(path_to_file,file))).unsqueeze(2) for file in ridx]\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ee6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_one = torch.cat((imgs[0].unsqueeze(0),imgs[0].unsqueeze(0),imgs[0].unsqueeze(0),imgs[1].unsqueeze(0),imgs[1].unsqueeze(0),imgs[1].unsqueeze(0)),0).unsqueeze(0).cuda()\n",
    "#input_one.shape\n",
    "#input_two = preprocessing_flownet(imgs[2].unsqueeze(0), imgs[3].unsqueeze(0)).cuda()\n",
    "#input_three = preprocessing_flownet(imgs[4].unsqueeze(0), imgs[5].unsqueeze(0)).cuda()\n",
    "\n",
    "# Somehow Flownet only accapts mutliple of 64 as input size (64, 128, 256, etc.) if the input is squared. Therefore i'll scale the image to 128 for training and eval\n",
    "h = 150\n",
    "size = (64,64)\n",
    "W = 64\n",
    "H = 64\n",
    "img_fixed = F.interpolate(imgs[0].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1).numpy() \n",
    "img_moving = F.interpolate(imgs[1].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1).numpy()\n",
    "\n",
    "print(img_fixed.shape)\n",
    "img_fixed = np.concatenate([img_fixed, img_fixed, img_fixed], 2)\n",
    "img_moving = np.concatenate([img_moving, img_moving, img_moving], 2)\n",
    "\n",
    "images = [img_fixed, img_moving]\n",
    "images = np.array(images).transpose(3, 0, 1, 2)\n",
    "im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0).cuda()\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_one = flow2(im).squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = flow_one.data.cpu().numpy().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bbb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(132)\n",
    "plt.imshow(flow2img(data))\n",
    "plt.subplot(131)\n",
    "plt.imshow(F.interpolate(imgs[0].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1))\n",
    "plt.subplot(133)\n",
    "plt.imshow(F.interpolate(imgs[1].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267709f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc = load_pwcnet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce27999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = im.reshape(1,6,size[0], size[1])\n",
    "print(imgs[0].shape)\n",
    "images = preprocessing_pwc(imgs[0], imgs[1])\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc.eval()\n",
    "flo = pwc(images.cuda())\n",
    "print(\"Output: \", flo.shape)\n",
    "flo = flo[0] * 20.0\n",
    "flo = flo.cpu().data.numpy()\n",
    "print(\"First entry to numpy: \", flo.shape)\n",
    "# scale the flow back to the input size \n",
    "flo = np.swapaxes(np.swapaxes(flo, 0, 1), 1, 2) # \n",
    "u_ = cv2.resize(flo[:,:,0],(W,H))\n",
    "v_ = cv2.resize(flo[:,:,1],(W,H))\n",
    "u_ *= W/ float(64)\n",
    "v_ *= H/ float(64)\n",
    "flo = np.dstack((u_,v_))\n",
    "print(\"Stacked: \", flo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4821b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(132)\n",
    "plt.imshow(flow2img(flo))\n",
    "plt.subplot(131)\n",
    "plt.imshow(F.interpolate(imgs[0].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1))\n",
    "plt.subplot(133)\n",
    "plt.imshow(F.interpolate(imgs[1].reshape(1,1,150,150), size = size).reshape(size[0],size[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad69d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
