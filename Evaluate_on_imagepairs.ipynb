{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e349c1",
   "metadata": {},
   "source": [
    "# Evaluation on Image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from utils.plotting import showFlow, overlaySegment\n",
    "from utils.encoding import dice_coeff\n",
    "from utils.layers import warpImage, warp\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5609a",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da253298",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (150,150)\n",
    "o_m = H//4 +1\n",
    "o_n = W//4 +1\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2).cuda()\n",
    "disp_range = 0.25#0.25\n",
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2).cuda()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(4)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(pad2(ssd_minconv)))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cv2.optflow.DualTVL1OpticalFlow_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "seq_classic = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(64),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                                  torch.nn.Sigmoid())\n",
    "seq_classic.load_state_dict(torch.load(\"models/Experiment_1/seq_16_mix_12_01_22-16-51.pth\"))\n",
    "seq_classic.eval().cuda()\n",
    "\n",
    "seq_kc = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(64),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                                  torch.nn.Sigmoid())\n",
    "seq_kc.load_state_dict(torch.load(\"models/Experiment_2/seq_16_kd_13_01_22-08-03.pth\"))\n",
    "seq_kc.eval().cuda()\n",
    "\n",
    "seq_dml = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(32),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                                  torch.nn.BatchNorm2d(64),\n",
    "                                  torch.nn.PReLU(),\n",
    "                                  torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                                  torch.nn.Sigmoid())\n",
    "seq_dml.load_state_dict(torch.load(\"models/Experiment_3/13_01_22-12-44/student_16_2.pth\"))\n",
    "seq_dml.eval().cuda()\n",
    "\"\"\"\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_1/obel16_mix_WL_06_01_22-11-38.pth\"\n",
    "model_classic = OBELISK2d(16)\n",
    "model_classic.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_classic.eval().cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_2/obel_16_KD_WL_20_01_22-12-54.pth\"\n",
    "model_kc = OBELISK2d(16)\n",
    "model_kc.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_kc.eval().cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_3/27_10_21-10-59/student_16_1.pth\"\n",
    "model_dml = OBELISK2d(16)\n",
    "model_dml.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_dml.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_ultraschall/nicke_ma/data'\n",
    "\n",
    "frames1 = torch.load(os.path.join(path,'test_frames.pth'))\n",
    "segs1 = torch.load(os.path.join(path,'test_segs.pth')) *2\n",
    "\n",
    "frames1 = frames1[:-20]\n",
    "segs1 = segs1[:-20]\n",
    "\n",
    "frames2 = torch.load(os.path.join(path,'test_frames_disp_6.pth'))\n",
    "segs2 = torch.load(os.path.join(path,'test_segs_disp_6.pth')) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = torch.cat((frames1,frames2))\n",
    "segs = torch.cat((segs1,segs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_baseline(torch_flow, moving_seg):\n",
    "    \"\"\"\n",
    "    function to warp segs with predicted flow\n",
    "\n",
    "    torch_flow: troch.tensor predicted flow [1 x 2 x 150 x 150]\n",
    "    seg: torch.tensor [150 x 150]\n",
    "\n",
    "    return warped_segmentation torch.tensor [150 x 150]\n",
    "    \"\"\"\n",
    "\n",
    "    B, C, H, W = torch_flow.size()\n",
    "    # mesh grid\n",
    "    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "    vgrid = grid + torch_flow\n",
    "\n",
    "    # scale grid to [-1,1]\n",
    "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "    warped_seg_grid = nn.functional.grid_sample(moving_seg.float().unsqueeze(0).unsqueeze(0), vgrid)\n",
    "    return warped_seg_grid\n",
    "    \n",
    "def eval_baseline(fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    evaluate dual-tvl1 on image pair\n",
    "\n",
    "    return  torch.tensor d0 dice of vein and artery\n",
    "            run_time: float\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare for baseline\n",
    "    in1 = fixed.view(H,W,1).numpy().astype(np.float32)\n",
    "    in2 = moving.view(H,W,1).numpy().astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    # run baseline\n",
    "    baseline_flow = baseline.calc(in1,in2,None)\n",
    "    end = time.time()\n",
    "    run_time = round(end-start, 4)\n",
    "    \n",
    "    # warp \n",
    "    warped_moving_seg = warp_baseline(torch.from_numpy(baseline_flow.T).unsqueeze(0), moving_seg)\n",
    "    d0 = dice_coeff(fixed_seg, warped_moving_seg, 3)\n",
    "    return d0, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pdd(model, fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    function to warp segmentations with PDD-Net\n",
    "\n",
    "    model: instance of torch.nn.Module\n",
    "    fixed: torch.tensor of size (HxW)\n",
    "    moving: torch.tensor of size (HxW)\n",
    "    fixed_seg: torch.tensor of siez (HxW)\n",
    "    moving_seg: torch.tensor of siez (HxW)\n",
    "\n",
    "    return: troch.tensor dice score and float runtime\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    feat1 = model(fixed)\n",
    "    feat2 = model(moving)\n",
    "    ssd_distance = correlation_layer(displace_range, feat2, feat1)\n",
    "    soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "    # scaling\n",
    "    flow=F.interpolate(disp_xy,size=(150,150))\n",
    "    end = time.time()\n",
    "    run_time = round(end-start, 4)\n",
    "\n",
    "\n",
    "    identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "    warped_student_seg = F.grid_sample(moving_seg.float(),identity+flow.cpu().permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "    d0 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "    return d0, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2621af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_score = torch.zeros(len(frames), 2)\n",
    "baseline_time = torch.zeros(len(frames))\n",
    "\n",
    "c_score = torch.zeros(len(frames), 2)\n",
    "c_time = torch.zeros(len(frames))\n",
    "\n",
    "kd_score = torch.zeros(len(frames), 2)\n",
    "kd_time = torch.zeros(len(frames))\n",
    "\n",
    "dml_score = torch.zeros(len(frames), 2)\n",
    "dml_time = torch.zeros(len(frames))\n",
    "\n",
    "seq_c_score = torch.zeros(len(frames), 2)\n",
    "seq_c_time = torch.zeros(len(frames))\n",
    "\n",
    "seq_kd_score = torch.zeros(len(frames), 2)\n",
    "seq_kd_time = torch.zeros(len(frames))\n",
    "\n",
    "seq_dml_score = torch.zeros(len(frames), 2)\n",
    "seq_dml_time = torch.zeros(len(frames))\n",
    "\n",
    "\n",
    "un_score = torch.zeros(len(frames), 2)\n",
    "\n",
    "counter = 0\n",
    "for idx, imgs in enumerate(frames):\n",
    "    fixed = imgs[0].cuda()\n",
    "    moving = imgs[1].cuda()\n",
    "    \n",
    "    fixed_seg = segs[idx][0]\n",
    "    moving_seg = segs[idx][1]\n",
    "       \n",
    "    \n",
    "    un_score[idx] = dice_coeff(fixed_seg,moving_seg,3)\n",
    "    #baseline_score[idx], baseline_time[idx] = eval_baseline(fixed.cpu(), moving.cpu(), fixed_seg, moving_seg)\n",
    "    \n",
    "    # Obelisk\n",
    "    c_score[idx], c_time[idx] = eval_pdd(model_classic,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    kd_score[idx], kd_time[idx] = eval_pdd(model_kc,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    dml_score[idx], dml_time[idx] = eval_pdd(model_dml,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    \n",
    "    # Sequential\n",
    "    #seq_c_score[idx], seq_c_time[idx] = eval_pdd(seq_classic,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    #seq_kd_score[idx], seq_kd_time[idx] = eval_pdd(seq_kc,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    #seq_dml_score[idx], seq_dml_time[idx] = eval_pdd(seq_dml,fixed.unsqueeze(0).unsqueeze(0).float(), moving.unsqueeze(0).unsqueeze(0).float(), fixed_seg, moving_seg.unsqueeze(0).unsqueeze(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ede1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_pairs = dml_score.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b17fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unwarped:\")\n",
    "print(\"Vene: \",un_score[:,0].mean(),un_score[:,0].var())\n",
    "print(\"Artery: \", un_score[:,1].mean(),un_score[:,1].var())\n",
    "print(\"Overall: \", un_score.mean(),un_score.var())\n",
    "print()\n",
    "print(\"Classic\")\n",
    "print(\"Vene: \",c_score[:,0].mean(),c_score[:,0].var())\n",
    "print(\"Artery: \", c_score[:,1].mean(),c_score[:,1].var())\n",
    "print(\"Overall: \", c_score.mean(),c_score.var())\n",
    "print()\n",
    "print(\"KD:\")\n",
    "print(\"Vene: \",kd_score[:,0].mean(),kd_score[:,0].var())\n",
    "print(\"Artery: \", kd_score[:,1].mean(),kd_score[:,1].var())\n",
    "print(\"Overall: \", kd_score.mean(),kd_score.var())\n",
    "print()\n",
    "print(\"DML\")\n",
    "print(\"Vene: \",dml_score[:,0].mean(),dml_score[:,0].var())\n",
    "print(\"Artery: \", dml_score[:,1].mean(),dml_score[:,1].var())\n",
    "print(\"Overall: \", dml_score.mean(),dml_score.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sequential!!\")\n",
    "print()\n",
    "print(\"Classic\")\n",
    "print(\"Vene: \",seq_c_score[:,0].mean(),seq_c_score[:,0].var())\n",
    "print(\"Artery: \", seq_c_score[:,1].mean(),seq_c_score[:,1].var())\n",
    "print(\"Overall: \", seq_c_score.mean(),seq_c_score.var())\n",
    "print()\n",
    "print(\"KD:\")\n",
    "print(\"Vene: \",seq_kd_score[:,0].mean(),seq_kd_score[:,0].var())\n",
    "print(\"Artery: \", seq_kd_score[:,1].mean(),seq_kd_score[:,1].var())\n",
    "print(\"Overall: \", seq_kd_score.mean(),seq_kd_score.var())\n",
    "print()\n",
    "print(\"DML\")\n",
    "print(\"Vene: \",seq_dml_score[:,0].mean(),seq_dml_score[:,0].var())\n",
    "print(\"Artery: \", seq_dml_score[:,1].mean(),seq_dml_score[:,1].var())\n",
    "print(\"Overall: \", seq_dml_score.mean(),seq_dml_score.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d895f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(frame_pairs), c_time, alpha=0.5)\n",
    "plt.scatter(np.arange(frame_pairs), seq_c_time, alpha=0.5)\n",
    "plt.plot(np.arange(frame_pairs), np.full(frame_pairs, c_time.mean().item()))\n",
    "plt.plot(np.arange(frame_pairs), np.full(frame_pairs, seq_c_time.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ee972",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(18,9), sharey=True)\n",
    "\n",
    "# LABELLOSS\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), c_score[:,0], ax=ax[0][1], alpha=0.5, label=f'Vein; mean: {round(c_score[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), c_score[:,1],ax=ax[0][1], alpha=0.5, label=f'Artery; mean: {round(c_score[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,0].mean()), ax=ax[0][1])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,1].mean()), ax=ax[0][1])\n",
    "ax[0][1].legend(loc='center left')\n",
    "ax[0][1].set_title('label loss trained')\n",
    "\n",
    "# BASELINE\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), baseline_score[:,0], ax=ax[0][0], alpha=0.5, label=f'Vein; mean: {round(baseline_score[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), baseline_score[:,1],ax=ax[0][0], alpha=0.5, label=f'Artery; mean: {round(baseline_score[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,baseline_score[:,0].mean()), ax=ax[0][0])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,baseline_score[:,1].mean()), ax=ax[0][0])\n",
    "ax[0][0].set_title('Dual TV-L1')\n",
    "ax[0][0].legend(loc='center left')\n",
    "\n",
    "# KD\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), kd_score[:,0], ax=ax[1][0], alpha=0.5, label=f'Vein; mean: {round(kd_score[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), kd_score[:,1],ax=ax[1][0], alpha=0.5, label=f'Artery; mean: {round(kd_score[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,kd_score[:,0].mean()), ax=ax[1][0])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,kd_score[:,1].mean()), ax=ax[1][0])\n",
    "ax[1][0].legend(loc='center left')\n",
    "ax[1][0].set_title('Knowledge Distillation')\n",
    "\n",
    "# DML\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), dml_score[:,0], ax=ax[1][1], alpha=0.5, label=f'Vein; mean: {round(dml_score[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), dml_score[:,1],ax=ax[1][1], alpha=0.5, label=f'Artery; mean: {round(dml_score[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,dml_score[:,0].mean()), ax=ax[1][1])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,dml_score[:,1].mean()), ax=ax[1][1])\n",
    "ax[1][1].legend(loc='center left')\n",
    "ax[1][1].set_title('Deep mutual learning')\n",
    "\n",
    "\n",
    "ax[0][0].set_ylabel('Dice Score')\n",
    "ax[1][0].set_ylabel('Dice Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c01241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dual-TVL1: mean: {round(baseline_score.mean().item() * 100,2)}, vein_var: {round(baseline_score[:,0].var().item(), 4)*100}, artery Var {round(baseline_score[:,1].var().item(),4)*100}, overall_var {round(baseline_score.var().item(),4)*100}\")\n",
    "print(f\"label_loss: mean: {round(c_score.mean().item(), 4)*100}, vein_var: {round(c_score[:,0].var().item(), 4)*100}, artery Var {round(c_score[:,1].var().item(), 4)*100}, overall_var {round(c_score.var().item(),4)*100}\")\n",
    "print(f\"KC: mean: {round(kd_score.mean().item(), 4)*100}, vein_var: {round(kd_score[:,0].var().item(), 4)*100}, artery Var {round(kd_score[:,1].var().item(), 4)*100}, overall_var {round(kd_score.var().item(),4)*100}\")\n",
    "print(f\"DML: mean: {round(dml_score.mean().item(), 4)*100}, vein_var: {round(dml_score[:,0].var().item(), 4)*100}, artery Var {round(dml_score[:,1].var().item(),4)*100}, overall_var {round(dml_score.var().item(),4)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55af94",
   "metadata": {},
   "source": [
    "# Soft target evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948335fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_model = OBELISK2d(16)\n",
    "soft_model.load_state_dict(torch.load(\"models/Experiment_2/obel16_ensemble_soft_27_10_21-13-26.pth\"))\n",
    "soft_model=soft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc = load_pwcnet().cuda()\n",
    "flownet = load_flownet2().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65069649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdd_warp(model, fixed, moving, fixed_seg, moving_seg):\n",
    "    model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    feat1 = model(fixed.unsqueeze(0).unsqueeze(0).cuda())\n",
    "    feat2 = model(moving.unsqueeze(0).unsqueeze(0).cuda())\n",
    "    ssd_distance = correlation_layer(displace_range, feat2, feat1)\n",
    "    soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "    # scaling\n",
    "    flow=F.interpolate(disp_xy,size=(150,150))\n",
    "    end = time.time()\n",
    "    run_time = round(end-start, 4)\n",
    "    \n",
    "    identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "    warped_student_seg = F.grid_sample(moving_seg.unsqueeze(0).unsqueeze(0).float().cuda(),identity+flow.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "    return warped_student_seg, run_time\n",
    "\n",
    "\n",
    "def flownet_warp(fixed, moving, fixed_seg, moving_seg):\n",
    "    scale=4\n",
    "    B,C,H,W = fixed.shape\n",
    "    teacher_fixed = F.interpolate(fixed, size=(4*64,4*64), mode='bicubic')\n",
    "    teacher_moving = F.interpolate(moving, size=(4*64,4*64), mode='bicubic')\n",
    "    # Generate the teacher flow estimation\n",
    "    flow_in = preprocessing_flownet(teacher_fixed.reshape(scale*64,scale*64,1),teacher_moving.reshape(scale*64,scale*64,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    flownet_flow = flownet(flow_in.cuda()).cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    run_time = end-start\n",
    "    \n",
    "    flownet_flow = F.interpolate(flownet_flow, size=(H,W), mode='bicubic')\n",
    "\n",
    "    # warp segmentation with flownet flow\n",
    "    warped_flownet_seg = warp(moving_seg.float().unsqueeze(0).unsqueeze(0), flownet_flow)\n",
    "    \n",
    "    return warped_flownet_seg, run_time\n",
    "\n",
    "def pwc_warp(fixed, moving, fixed_seg, moving_seg):\n",
    "    scale=4\n",
    "    B,C,H,W = fixed.shape\n",
    "    \n",
    "    teacher_fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "    teacher_moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "\n",
    "    # Generate the teacher flow estimation\n",
    "    pwc_flow_in = preprocessing_pwc(teacher_fixed.detach().clone().reshape(scale*64,scale*64,1),teacher_moving.detach().clone().reshape(scale*64,scale*64,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    pwc_flow = pwc(pwc_flow_in.cuda()).cpu()\n",
    "    pwc_flow = pwc_flow[0] * 20.0\n",
    "    \n",
    "    end = time.time()\n",
    "    run_time = end-start\n",
    "    \n",
    "    pwc_flow = F.interpolate(pwc_flow.unsqueeze(0), size=(H,W)).cpu()\n",
    "\n",
    "    # warp the segmentations with pwc flow\n",
    "    warped_pwc_seg = warp(moving_seg.float().unsqueeze(0).unsqueeze(0), pwc_flow).cpu()\n",
    "    \n",
    "    return warped_pwc_seg, run_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_segs = torch.zeros(segs.shape)\n",
    "pwc_segs = torch.zeros(segs.shape)\n",
    "flow_segs = torch.zeros(segs.shape)\n",
    "\n",
    "\n",
    "base_time = torch.zeros(frames.shape[0])\n",
    "pwc_time = torch.zeros(frames.shape[0])\n",
    "flow_time = torch.zeros(frames.shape[0])\n",
    "\n",
    "\n",
    "base_dice = torch.zeros(frames.shape[0],2)\n",
    "pwc_dice = torch.zeros(frames.shape[0],2)\n",
    "flow_dice = torch.zeros(frames.shape[0],2)\n",
    "\n",
    "for i, imgs in enumerate(frames):\n",
    "    fixed = imgs[0]\n",
    "    moving = imgs[1]\n",
    "    \n",
    "    fixed_seg = segs[i][0]\n",
    "    moving_seg = segs[i][1]\n",
    "    \n",
    "    #print(dice_coeff(fixed_seg,moving_seg, 3))\n",
    "    \n",
    "    # BASELINE (PDD on Soft targets)\n",
    "    \n",
    "    base_seg, run_time_base = pdd_warp(soft_model, fixed.float(), moving.float(), fixed_seg, moving_seg)\n",
    "    base_segs[i] = base_seg.detach().cpu()\n",
    "    base_time[i] = run_time_base\n",
    "    base_dice[i] = dice_coeff(fixed_seg.cpu(), base_seg.cpu(), 3)\n",
    "    \n",
    "    \n",
    "    # FLOWNET2\n",
    "    warped_seg, run_time = flownet_warp(fixed.unsqueeze(0).unsqueeze(0),\n",
    "                                         moving.unsqueeze(0).unsqueeze(0),\n",
    "                                         fixed_seg,\n",
    "                                         moving_seg)\n",
    "    \n",
    "    flow_segs[i] = warped_seg.detach().cpu()\n",
    "    flow_time[i] = run_time\n",
    "    flow_dice[i] = dice_coeff(fixed_seg.cpu(), warped_seg, 3).cpu()\n",
    "    \n",
    "    # PWC\n",
    "    warped_seg, run_time = pwc_warp(fixed.unsqueeze(0).unsqueeze(0),\n",
    "                                     moving.unsqueeze(0).unsqueeze(0),\n",
    "                                     fixed_seg,\n",
    "                                     moving_seg)\n",
    "    pwc_segs[i] = warped_seg.detach().cpu()\n",
    "    pwc_time[i] = run_time\n",
    "    pwc_dice[i] = dice_coeff(fixed_seg.cpu(), warped_seg, 3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_pairs=base_dice.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02af962",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(18,9), sharey=True)\n",
    "\n",
    "# Soft Target\n",
    "sns.scatterplot(np.arange(base_dice.shape[0]), base_dice[:,0], ax=ax[0][1], alpha=0.5, label=f'Vein; mean: {round(base_dice[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(base_dice.shape[0]), base_dice[:,1],ax=ax[0][1], alpha=0.5, label=f'Artery; mean: {round(base_dice[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,0].mean()), ax=ax[0][1])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,1].mean()), ax=ax[0][1])\n",
    "ax[0][1].legend(loc='center left')\n",
    "ax[0][1].set_title('Soft Target trained')\n",
    "\n",
    "# LABELLOSS\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), c_score[:,0], ax=ax[0][0], alpha=0.5, label=f'Vein; mean: {round(c_score[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(c_score.shape[0]), c_score[:,1],ax=ax[0][0], alpha=0.5, label=f'Artery; mean: {round(c_score[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,0].mean()), ax=ax[0][0])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,c_score[:,1].mean()), ax=ax[0][0])\n",
    "ax[0][0].legend(loc='center left')\n",
    "ax[0][0].set_title('label loss trained')\n",
    "\n",
    "# Flownet\n",
    "sns.scatterplot(np.arange(flow_dice.shape[0]), flow_dice[:,0], ax=ax[1][0], alpha=0.5, label=f'Vein; mean: {round(flow_dice[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(flow_dice.shape[0]), flow_dice[:,1],ax=ax[1][0], alpha=0.5, label=f'Artery; mean: {round(flow_dice[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,flow_dice[:,0].mean()), ax=ax[1][0])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,flow_dice[:,1].mean()), ax=ax[1][0])\n",
    "ax[1][0].legend(loc='center left')\n",
    "ax[1][0].set_title('Flownet2')\n",
    "\n",
    "# PWC\n",
    "sns.scatterplot(np.arange(pwc_dice.shape[0]), pwc_dice[:,0], ax=ax[1][1], alpha=0.5, label=f'Vein; mean: {round(pwc_dice[:,0].mean().item(), 4)}')\n",
    "sns.scatterplot(np.arange(pwc_dice.shape[0]), pwc_dice[:,1],ax=ax[1][1], alpha=0.5, label=f'Artery; mean: {round(pwc_dice[:,1].mean().item(), 4)}')\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,pwc_dice[:,0].mean()), ax=ax[1][1])\n",
    "sns.lineplot(np.arange(frame_pairs), np.full(frame_pairs,pwc_dice[:,1].mean()), ax=ax[1][1])\n",
    "ax[1][1].legend(loc='center left')\n",
    "ax[1][1].set_title('PWC')\n",
    "\n",
    "ax[0][0].set_ylabel('Dice Score')\n",
    "ax[1][0].set_ylabel('Dice Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6315d",
   "metadata": {},
   "source": [
    "soft target trained model performs worse than teachers :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c41b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PDD-soft: mean: {round(base_dice.mean().item(),4)*100}, vein_var: {round(base_dice[:,0].var().item(), 4)*100}, artery Var {round(base_dice[:,1].var().item(),4)*100}, overall_var {round(base_dice.var().item(),4)*100}\")\n",
    "print(f\"PDD: mean: {round(c_score.mean().item(), 4)*100}, vein_var: {round(c_score[:,0].var().item(), 4)*100}, artery Var {round(c_score[:,1].var().item(), 4)*100}, overall_var {round(c_score.var().item(),4)*100}\")\n",
    "print(f\"Flownet2: mean: {round(flow_dice.mean().item(), 4)*100}, vein_var: {round(flow_dice[:,0].var().item(), 4)*100}, artery Var {round(flow_dice[:,1].var().item(), 4)*100}, overall_var {round(flow_dice.var().item(),4)*100}\")\n",
    "print(f\"PWC: mean: {round(pwc_dice.mean().item(), 4)*100}, vein_var: {round(pwc_dice[:,0].var().item(), 4)*100}, artery Var {round(pwc_dice[:,1].var().item(),4)*100}, overall_var {round(pwc_dice.var().item(),4)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c72dce",
   "metadata": {},
   "source": [
    "# Runtime\n",
    "between Flownet2, PWC-Net and PDD-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288728b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = {'fontsize': 22,'family': 'Latin Modern Roman'}\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(np.arange(frame_pairs), base_time * 1000, label=f'PDD-Net: {round(base_time.mean().item()*1000, 3)}', alpha=0.5)\n",
    "plt.scatter(np.arange(frame_pairs), pwc_time * 1000, label=f'PWC-Net: {round(pwc_time.mean().item()*1000, 3)}', alpha=0.5)\n",
    "plt.scatter(np.arange(frame_pairs), flow_time * 1000, label=f'Flownet2: {round(flow_time.mean().item()*1000, 3)}', alpha=0.5)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel(\"Time [ms]\", fontdict=fonts)\n",
    "plt.xlabel(\"# of Imagepair\", fontdict=fonts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_vein = flow_dice[:,0]- pwc_dice[:,0]\n",
    "diff_art = flow_dice[:,1]- pwc_dice[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef446ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(diff_vein.shape[0]), diff_vein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(diff_art.shape[0]), diff_art)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
