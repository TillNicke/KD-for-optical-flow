{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2b24f9",
   "metadata": {},
   "source": [
    "# First experiment\n",
    "Try to let the PDD-Net learn the flow by using the warped labels as targets. In the following we also test, which of the two feature extractors works better for this specific problem. On the one hand, we have the obelisk feature extractor and on the other hand, we have a simple sequential CNN. both have their pros and cons, regarding runtime and used resources.\n",
    "\n",
    "Current state of the notebook is after results completion. There have been slight changes to the intial setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9901e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp, warpImage #, correlation_layer, meanfield\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d, deeds2d\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mode='label loss'\n",
    "\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sheet(epochs, optim, loss,grad_accum,lr, mode, eval_mean_dice, unwarped_dice, notes):\n",
    "    \"\"\"\n",
    "    function to update a csv table to keep track of results\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df = df.append({\n",
    "        'epochs':epochs,\n",
    "        'optim':optim,\n",
    "        'batch size': grad_accum,\n",
    "        'learning rate': lr,\n",
    "        'loss':loss,\n",
    "        'mode': mode,\n",
    "        'Eval mean dice': eval_mean_dice,\n",
    "        'unwarped dice': unwarped_dice,\n",
    "        'time': datetime.now().strftime(\"%d.%m.%y %H:%M\"),\n",
    "        'notes': notes\n",
    "    }, ignore_index=True)\n",
    "    df.to_csv('experiments.csv')\n",
    "    print('updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48476250",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs1 = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames.pth')\n",
    "#segs1 = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs.pth')\n",
    "\n",
    "imgs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames_disp_6.pth')\n",
    "segs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs_disp_6.pth')\n",
    "\n",
    "imgs3 = torch.load('/share/data_ultraschall/nicke_ma/data/frames_oneFixed_multipleMoving_dist2.pth')\n",
    "segs3 = torch.load('/share/data_ultraschall/nicke_ma/data/segs_oneFixed_multipleMoving_dist2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d09931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both data sets\n",
    "imgs = torch.cat((imgs2,imgs3))\n",
    "segs = torch.cat((segs2,segs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a training split \n",
    "torch.manual_seed(42)\n",
    "# Now, we prepare our train & test dataset.\n",
    "train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "test_set = torch.arange(len(imgs))\n",
    "for idx in train_set:\n",
    "    test_set = test_set[test_set != idx]\n",
    "\n",
    "#for i, seg in enumerate(segs):\n",
    "#    f_seg = seg[0]\n",
    "#    m_seg = seg[1]\n",
    "#    if len(torch.where(torch.histc(f_seg) != 0)[0]) == 3 and f_seg.max() < 1:\n",
    "#        print('here')\n",
    "#        segs[i][0] = segs[i][0]*2 \n",
    "#    if len(torch.where(torch.histc(m_seg) != 0)[0]) == 3 and m_seg.max() < 1:\n",
    "#        segs[i][1] = segs[i][1]*2 \n",
    "\n",
    "print(f\"{train_set.shape[0]} train examples\")\n",
    "print(f\"{test_set.shape[0]} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2dec12",
   "metadata": {},
   "source": [
    "# Networks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan=16, size=(150,150)):\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2, channels * 2, 2) * 0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels * 8, channels * 4, 1, bias=False,\n",
    "                                groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer2 = nn.Conv2d(channels * 4, channels * 4, 3, bias=False,\n",
    "                                padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer3 = nn.Conv2d(channels * 4, channels * 1, 1)\n",
    "\n",
    "        H = size[0]\n",
    "        W = size[1]\n",
    "        self.o_m = H // 4 +1\n",
    "        self.o_n = W // 4 +1\n",
    "\n",
    "        self.displace_range = 11\n",
    "        self.disp_hw = 5\n",
    "        self.ogrid_xy = F.affine_grid(torch.eye(2, 3).unsqueeze(0),\n",
    "                                 (1, 1, self.o_m, self.o_n)).view(1, 1, -1, 2).cuda()\n",
    "        self.disp_range = 0.25\n",
    "        self.displacement_width = 11\n",
    "        shift_xy = F.affine_grid(self.disp_range * torch.eye(2, 3).unsqueeze(0), (1, 1, self.displacement_width, self.displacement_width)).view(1, 1, -1, 2).cuda()\n",
    "        grid_size = 32  # 25#30\n",
    "        self.grid_xy = F.affine_grid(torch.eye(2, 3).unsqueeze(0),\n",
    "                                (1, 1, grid_size, grid_size)).view(1, -1, 1,\n",
    "                                                                   2).cuda()\n",
    "\n",
    "    def forward(self, fixed_img, moving_img):\n",
    "        img_in_f = F.avg_pool2d(fixed_img, 3, padding=1, stride=2)\n",
    "        img_in_f = F.relu(self.batch0(self.layer0(img_in_f)))\n",
    "        sampled_f = F.grid_sample(img_in_f,self.ogrid_xy + self.offsets[0, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "        sampled_f -= F.grid_sample(img_in_f,self.ogrid_xy + self.offsets[1, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "\n",
    "        x_1 = F.relu(self.batch1(self.layer1(sampled_f)))\n",
    "        x_1 = F.relu(self.batch2(self.layer2(x_1)))\n",
    "        features_fixed = self.layer3(x_1)\n",
    "        \n",
    "        img_in_m = F.avg_pool2d(moving_img, 3, padding=1, stride=2)\n",
    "        img_in_m = F.relu(self.batch0(self.layer0(img_in_m)))\n",
    "        sampled_m = F.grid_sample(img_in_m,self.ogrid_xy + self.offsets[0, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "        sampled_m -= F.grid_sample(img_in_m,self.ogrid_xy + self.offsets[1, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "\n",
    "        x_2 = F.relu(self.batch1(self.layer1(sampled_m)))\n",
    "        x_2 = F.relu(self.batch2(self.layer2(x_2)))\n",
    "        features_moving = self.layer3(x_2)\n",
    "\n",
    "        ssd_distance = self.correlation_layer(features_moving, features_fixed)\n",
    "        soft_cost,disp_xy = self.meanfield(ssd_distance, fixed_img, self.displace_range, self.o_m, self.o_n)\n",
    "        \n",
    "        return soft_cost, disp_xy\n",
    "\n",
    "\n",
    "    def min_convolution(self, ssd_distance, displace_range, H, W):\n",
    "        # Prepare operators for smooth dense displacement space\n",
    "        pad1 = nn.ReplicationPad2d(5)\n",
    "        avg1 = nn.AvgPool2d(5, stride=1)\n",
    "        max1 = nn.MaxPool2d(3, stride=1)\n",
    "        pad2 = nn.ReplicationPad2d(4)\n",
    "        # approximate min convolution / displacement compatibility\n",
    "\n",
    "        ssd_minconv = avg1(avg1(-max1(-pad1(\n",
    "            ssd_distance.permute(0, 2, 3, 1).reshape(1, -1, self.displace_range,\n",
    "                                                    self.displace_range)))))\n",
    "\n",
    "        ssd_minconv = ssd_minconv.permute(0, 2, 3, 1).view(1, -1, H, W)\n",
    "        min_conv_cost = avg1(avg1(pad2(ssd_minconv)))\n",
    "\n",
    "        return min_conv_cost\n",
    "\n",
    "\n",
    "    def meanfield(self, ssd_distance, img_fixed, displace_range, H, W):\n",
    "        crnt_dev = ssd_distance.device\n",
    "\n",
    "        cost = self.min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "        soft_cost = F.softmax(-10 * cost.view(displace_range ** 2, -1).t(), 1)\n",
    "\n",
    "        disp_hw = (displace_range - 1) // 2\n",
    "        disp_mesh_grid = disp_hw * F.affine_grid(torch.eye(2, 3).unsqueeze(0), (\n",
    "        1, 1, displace_range, displace_range), align_corners=True)\n",
    "        disp_mesh_grid /= torch.Tensor([(W - 1) * .5, (H - 1) * .5])\n",
    "\n",
    "        disp_xy = torch.sum(\n",
    "            soft_cost.view(1, H, W, -1, 1) * disp_mesh_grid.view(1, 1, 1, -1,\n",
    "                                                                2).to(crnt_dev),\n",
    "            3).permute(0, 3, 1, 2)\n",
    "\n",
    "        return soft_cost, disp_xy\n",
    "\n",
    "\n",
    "    def correlation_layer(self, feat_moving, feat_fixed):\n",
    "        disp_hw = (self.displacement_width - 1) // 2\n",
    "        feat_moving_unfold = F.unfold(feat_moving.transpose(1, 0),\n",
    "                                    (self.displace_range, self.displace_range),\n",
    "                                    padding=self.disp_hw)\n",
    "        B, C, H, W = feat_fixed.size()\n",
    "\n",
    "        ssd_distance = ((feat_moving_unfold - feat_fixed.view(C, 1, -1)) ** 2).sum(0).view(1, displace_range ** 2, H, W)\n",
    "\n",
    "        return ssd_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0b299",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "#label_weights = torch.tensor([0.1,0.6, 0.3])# weights for background = 0.1, Vein = 0.6 and Artery = 0.3\n",
    "epochs = 100\n",
    "lr = 0.00025\n",
    "grad_accum = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df36067",
   "metadata": {},
   "outputs": [],
   "source": [
    "obel = OBELISK2d(16)\n",
    "init_weights(obel)\n",
    "obel.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(obel.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca298a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "unwapred = []\n",
    "for epoch in trange(epochs):\n",
    "\n",
    "    train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "    test_set = torch.arange(len(imgs))\n",
    "    for idx in train_set:\n",
    "        test_set = test_set[test_set != idx]\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i,rnd_idx in enumerate(rnd_train_idx):\n",
    "        loss_tmp = []\n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous() * 2\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous() * 2\n",
    "\n",
    "        # sort out unwanted data\n",
    "        if moving_seg.max() <= 0.1 and fixed_seg.max() <= 0.1:\n",
    "            continue\n",
    "            \n",
    "        # Downsize the label\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving = F.one_hot(moving_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving,size=(Hf//4 +1,Wf//4 +1),mode='bicubic')\n",
    "        \n",
    "        label_fixed = F.one_hot(fixed_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4 +1,Wf//4 +1),mode='bicubic')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        # passed through obelisk layer\n",
    "        soft_cost,disp_xy = obel(fixed.cuda(), moving.cuda())      # fixed\n",
    "        #feat50 = obel(moving.cuda())     # moving\n",
    "\n",
    "         # compute the cost tensor using the correlation layer\n",
    "        #ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        #soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "\n",
    "        label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "        \n",
    "        #print(((torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2)).T.mul(label_weights)).T.shape)\n",
    "        label_distance1 = torch.sum((torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2)),0) #* label_weights\n",
    "        #label_distance1 = label_distance1 * label_weights \n",
    "\n",
    "        diffloss = 1.5*((disp_xy[0,:,1:,:]-disp_xy[0,:,:-1,:])**2).mean()+\\\n",
    "            1.5*((disp_xy[0,1:,:,:]-disp_xy[0,:-1,:,:])**2).mean()+\\\n",
    "            1.5*((disp_xy[0,:,:,1:]-disp_xy[0,:,:,:-1])**2).mean()\n",
    "\n",
    "\n",
    "        loss = label_distance1.mean() + diffloss\n",
    "        # perform the backpropagation and weight updates\n",
    "        loss.backward()\n",
    "        loss_tmp.append(loss.item())\n",
    "\n",
    "        #if (i+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    losses.append(np.mean(loss_tmp))\n",
    "    print(np.mean(loss_tmp))\n",
    "    \"\"\"\n",
    "    overall_dice = []\n",
    "    unwarped_dice = []\n",
    "    for i,idx in enumerate(test_set):\n",
    "        \n",
    "        fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[idx:idx+1,0,:].contiguous() * 2\n",
    "        moving_seg = segs[idx:idx+1,1,:].contiguous() * 2\n",
    "        \n",
    "        if moving_seg.max() <= 0.1 and fixed_seg.max() <= 0.1:\n",
    "            continue\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            soft_cost_one,disp_xy = obel(fixed.cuda(), moving.cuda())\n",
    "            #moving_feat = obel(moving.cuda())\n",
    "\n",
    "        #ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "        #regularise using meanfield inference with approx. min-convolutions\n",
    "        #soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "        #upsample field to original resolution\n",
    "        dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "\n",
    "        #apply and evaluate transformation\n",
    "        identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "        warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "        d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "        d2 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "        \n",
    "        #print(f\"{d1} VS unwarped {d2}; Means: {d1.mean()} VS unwarped {d2.mean()}\")\n",
    "        overall_dice.append(d1.mean())\n",
    "        unwarped_dice.append(d2.mean())\n",
    "    \n",
    "    accs.append(torch.from_numpy(np.array(overall_dice)).mean())\n",
    "    unwapred.append(torch.from_numpy(np.array(unwarped_dice)).mean())\n",
    "    print(torch.from_numpy(np.array(overall_dice)).mean(), torch.from_numpy(np.array(unwarped_dice)).mean())\n",
    "    #diffs.append(loss.item())\n",
    "    #if (epoch+1)%grad_accum == 0:\n",
    "    #    # every grad_accum iterations :Make an optimizer step\n",
    "    #    optimizer.step()\n",
    "    #    optimizer.zero_grad()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc640dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "torch.save(obel.state_dict(), f\"models/Experiment_1/obel_16_mix_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Plots\n",
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.plot(np.arange(len(unwapred)), unwapred, label='Unwarped')\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/100-Epochs-refactored.png\")\n",
    "\n",
    "# I am still investigating why the loss looks that way. Have not found a clue as of now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'label loss + diffloss',grad_accum,lr, mode, eval_mean_dice, unwarped_dice, 'eighted Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed85542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.plot(np.arange(len(unwapred)), unwapred, label='Unwarped')\n",
    "plt.legend()\n",
    "#plt.savefig(f'plots/obel_16_1500E_{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276034b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_mean_dice, unwarped_dice = evaluate_model(obel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6caf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_mean_dice)\n",
    "unwarped_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'label loss + diffloss',grad_accum,lr, mode, eval_mean_dice, unwarped_dice, 'eighted Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obel.state_dict(), f\"models/Experiment_1/seq_16_mix_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the flow actually looks ok and does what it is supposed to\n",
    "# The Loss is usually not enough.\n",
    "obel.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "print(rnd_test_idx[0])\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "\n",
    "# quick fix here...\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat = obel(fixed.cuda())\n",
    "    moving_feat = obel(moving.cuda())\n",
    "\n",
    "ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.flip(1).permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,3)\n",
    "\n",
    "print(f\"{d1} VS unwarped {d0}\")\n",
    "\n",
    "rgb = showFlow(dense_flow_fit.flip(1).cpu().transpose(-2,-1).flip(1))\n",
    "overlay = overlaySegment(fixed.squeeze(),warped_student_seg.data.squeeze(),False)\n",
    "\n",
    "overlay_fixed = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),False)\n",
    "overlay_moving = overlaySegment(moving.squeeze(),moving_seg.data.squeeze(),False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"Flow field\")\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Warped\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(overlay_fixed)\n",
    "plt.title('Fixed')\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_moving)\n",
    "plt.title(\"Moving\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fdea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obel.state_dict(), f\"models/Experiment_1/obel16_oneFixed_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2304d1",
   "metadata": {},
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dcdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=0,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,24,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())\n",
    "seq.train().cuda()\n",
    "optimizer = torch.optim.Adam(list(seq.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec342ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i,rnd_idx in enumerate(rnd_train_idx):\n",
    "        loss_tmp = []\n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "\n",
    "        # Downsize the label\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving = F.one_hot(moving_seg.long(),num_classes=2).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        \n",
    "        label_fixed = F.one_hot(fixed_seg.long(),num_classes=2).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,2,displace_range**2,-1)\n",
    "\n",
    "        # passed through obelisk layer\n",
    "        feat00 = seq(fixed.cuda())      # fixed\n",
    "        feat50 = seq(moving.cuda())     # moving\n",
    "\n",
    "         # compute the cost tensor using the correlation layer\n",
    "        ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "        label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "\n",
    "        label_distance1 = torch.sum(torch.pow(label_fixed.reshape(2,-1)-label_warped.reshape(2,-1),2),0)\n",
    "        loss = label_distance1.mean()\n",
    "        # perform the backpropagation and weight updates\n",
    "        loss.backward()\n",
    "        loss_tmp.append(loss.item())\n",
    "    losses.append(np.mean(loss_tmp))\n",
    "    acc, _ = evaluate_model(obel)\n",
    "    accs.append(acc)\n",
    "    #diffs.append(loss.item())\n",
    "    if (epoch+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations :Make an optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mean_dice, unwarped_dice = evaluate_model(seq)\n",
    "print(eval_mean_dice)\n",
    "unwarped_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'Sequentail label loss',grad_accum,lr, mode, eval_mean_dice, unwarped_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "\n",
    "# quick fix here...\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 2:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 2:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat = obel(fixed.cuda())\n",
    "    moving_feat = obel(moving.cuda())\n",
    "\n",
    "ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),2)\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,2)\n",
    "\n",
    "rgb = showFlow(dense_flow_fit.cpu().transpose(-2,-1).flip(1))\n",
    "overlay = overlaySegment(fixed.squeeze(),warped_student_seg.data.squeeze(),False)\n",
    "\n",
    "overlay_fixed = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),False)\n",
    "overlay_moving = overlaySegment(moving.squeeze(),moving_seg.data.squeeze(),False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb)\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(overlay_fixed)\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq.state_dict(), \"models/Experiment_1/seq_24_500.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bd112",
   "metadata": {},
   "source": [
    "# Comparisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Obelisk:\")\n",
    "evaluate_model(obel)\n",
    "print(\"Sequential:\")\n",
    "evaluate_model(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17336bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obel.state_dict(), \"models/obel_solo_24.pth\")\n",
    "torch.save(seq.state_dict(), \"models/seq_solo_24.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb6d94",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc072bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "overall_dice = []\n",
    "\n",
    "for i,idx in enumerate(test_set):\n",
    "        \n",
    "        fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[idx:idx+1,0,:].contiguous()\n",
    "        moving_seg = segs[idx:idx+1,1,:].contiguous()\n",
    "        \n",
    "        # Some images have no segmentation to them, \n",
    "        # even if it was present in the directory\n",
    "        # We leave these ones out, as they cannot be avaluated\n",
    "        \n",
    "        # quick fix here...\n",
    "        if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 2:\n",
    "            fixed_seg = fixed_seg*2\n",
    "        if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 2:\n",
    "            moving_seg = moving_seg*2\n",
    "        if fixed_seg.max() < 0.1 and moving_seg.max() < 0.1:\n",
    "            pass\n",
    "        else:\n",
    "            in1 = fixed.view(H,W,1).numpy().astype(np.float32)\n",
    "            in2 = moving.view(H,W,1).numpy().astype(np.float32)\n",
    "            flow = baseline.calc(in1,in2,None)\n",
    "            \n",
    "            torch_flow = torch.from_numpy(flow).unsqueeze(0)\n",
    "            B, C, H, W = torch_flow.permute(0,3,1,2).size()\n",
    "            # mesh grid\n",
    "            xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "            yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "            xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "            yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "            grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "            vgrid = grid + torch_flow.permute(0,3,1,2)\n",
    "\n",
    "            # scale grid to [-1,1]\n",
    "            vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "            vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "            vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "            warped_seg = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "            #warped_seg = warpImage(moving_seg.view(1,1,H,W).float(), torch.from_numpy(flow).view(1,2,H,W))\n",
    "            #warped_seg = warp(moving_seg.float().unsqueeze(0).cuda(), torch.from_numpy(flow).view(2,H,W).unsqueeze(0).cuda()).cpu().to(torch.int64)\n",
    "            \n",
    "            overall_dice.append(dice_coeff(fixed_seg,warped_seg.squeeze(),2).mean())\n",
    "overall_dice = torch.from_numpy(np.array(overall_dice))\n",
    "print(f\"Baseline reached a mean dice score of {round(overall_dice.mean().item(),5)} with Var of {round(overall_dice.var().item(),5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
