{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fe0a71",
   "metadata": {},
   "source": [
    "# Second Experiment\n",
    "In this experiment, we want to investigate, the PDD network can improove the performance, when using a knowledge compression method. The teacher will be the FlowNet2 with pretrained weights, which can be found [here](https://drive.google.com/drive/folders/0B5EC7HMbyk3CbjFPb0RuODI3NmM?resourcekey=0-SuPU4qVZzuB4s83ngjrAEg). \n",
    "\n",
    "As there are multiple different way in applying the loss function, when using knowledge compression, there will be three different loss functions compared here. \n",
    "1) In a standard knowledge compression, the KL divergence is used to approximate the teacher output. We will compare the different warped labels of student and teacher. However, as this is usually done in classificatin, we do not expect it to perform really well in this setting.\n",
    "\n",
    "2) A different approach is to use the MSE between the estimated flow field of the teacher and the estimated flowfield of the student. This does, however, not incorporate the warped label and dice score. The MSE is expressed as: $$L =\\sum_{i,j}(prediction[i,j] - target[i,j])^2 $$ where $[i,j]$ denotes the flowfield at position i,j. \n",
    "\n",
    "3) What is missing in 2) will be used in the last setting. We incorporate the loss of the warped label. This can happen either by simply adding the loss to the loss between the flow fields OR by using a scale factor (probably dice score of the teacher) which indicates how well the teacher performs on this specific example. The loss can then be expressed as follows: $$L_i = \\delta * L_{flowfield} + (1-\\delta) * L_{Label}$$ With $\\delta$ being the dice score between warped teacher segment and fixed segmentation. $L_i$ is the loss ofr the $i_{th}$ example, $L_{flowfield}$ is the crossentropy loss between student and teacher flow estimation and $L_{Label}$ is the warping loss between student warped segmentation and the segmentation to approximate.\n",
    "\n",
    "\n",
    "Evantually only the results using PWC-Net and Flownet2 as teachers were used in the thesis. The text above was the plan, when the thesis started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d\n",
    "\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f029a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'Knowledge Compression'\n",
    "def update_sheet(epochs, optim, loss_str ,grad_accum,lr, mode, eval_mean_dice, unwarped_dice, notes=\"\"):\n",
    "    \"\"\"\n",
    "    function to update a csv table to keep track of results\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "    df = df.append({\n",
    "        'epochs':epochs,\n",
    "        'optim':optim,\n",
    "        'batch size': grad_accum,\n",
    "        'learning rate': lr,\n",
    "        'loss':loss,\n",
    "        'mode': mode,\n",
    "        'Eval mean dice': eval_mean_dice,\n",
    "        'unwarped dice': unwarped_dice,\n",
    "        'time': datetime.datetime.now().strftime(\"%d.%m.%y %H:%M\"),\n",
    "        'notes': notes\n",
    "    }, ignore_index=True)\n",
    "    df.to_csv('experiments.csv')\n",
    "    print('updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da91ece",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames_disp_6.pth')\n",
    "segs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs_disp_6.pth')\n",
    "\n",
    "imgs3 = torch.load('/share/data_ultraschall/nicke_ma/data/frames_oneFixed_multipleMoving.pth')\n",
    "segs3 = torch.load('/share/data_ultraschall/nicke_ma/data/segs_oneFixed_multipleMoving.pth')\n",
    "\n",
    "imgs = torch.cat((imgs2,imgs3))\n",
    "segs = torch.cat((segs2,segs3))\n",
    "\n",
    "#define a training split \n",
    "torch.manual_seed(42)\n",
    "# Now, we prepare our train & test dataset.\n",
    "train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "test_set = torch.arange(len(imgs))\n",
    "for idx in train_set:\n",
    "    test_set = test_set[test_set != idx]\n",
    "\n",
    "\n",
    "print(f\"{train_set.shape[0]} train examples\")\n",
    "print(f\"{test_set.shape[0]} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de5baf",
   "metadata": {},
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan=16, size=(150,150)):\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2, channels * 2, 2) * 0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels * 8, channels * 4, 1, bias=False,\n",
    "                                groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer2 = nn.Conv2d(channels * 4, channels * 4, 3, bias=False,\n",
    "                                padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels * 4)\n",
    "        self.layer3 = nn.Conv2d(channels * 4, channels * 1, 1)\n",
    "\n",
    "        H = size[0]\n",
    "        W = size[1]\n",
    "        self.o_m = H // 4 +1\n",
    "        self.o_n = W // 4 +1\n",
    "\n",
    "        self.displace_range = 11\n",
    "        self.disp_hw = 5\n",
    "        self.ogrid_xy = F.affine_grid(torch.eye(2, 3).unsqueeze(0),\n",
    "                                 (1, 1, self.o_m, self.o_n)).view(1, 1, -1, 2).cuda()\n",
    "        self.disp_range = 0.25\n",
    "        self.displacement_width = 11\n",
    "        shift_xy = F.affine_grid(self.disp_range * torch.eye(2, 3).unsqueeze(0), (1, 1, self.displacement_width, self.displacement_width)).view(1, 1, -1, 2).cuda()\n",
    "        grid_size = 32  # 25#30\n",
    "        self.grid_xy = F.affine_grid(torch.eye(2, 3).unsqueeze(0),\n",
    "                                (1, 1, grid_size, grid_size)).view(1, -1, 1,\n",
    "                                                                   2).cuda()\n",
    "\n",
    "    def forward(self, fixed_img, moving_img):\n",
    "        img_in_f = F.avg_pool2d(fixed_img, 3, padding=1, stride=2)\n",
    "        img_in_f = F.relu(self.batch0(self.layer0(img_in_f)))\n",
    "        sampled_f = F.grid_sample(img_in_f,self.ogrid_xy + self.offsets[0, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "        sampled_f -= F.grid_sample(img_in_f,self.ogrid_xy + self.offsets[1, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "\n",
    "        x_1 = F.relu(self.batch1(self.layer1(sampled_f)))\n",
    "        x_1 = F.relu(self.batch2(self.layer2(x_1)))\n",
    "        features_fixed = self.layer3(x_1)\n",
    "        \n",
    "        img_in_m = F.avg_pool2d(moving_img, 3, padding=1, stride=2)\n",
    "        img_in_m = F.relu(self.batch0(self.layer0(img_in_m)))\n",
    "        sampled_m = F.grid_sample(img_in_m,self.ogrid_xy + self.offsets[0, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "        sampled_m -= F.grid_sample(img_in_m,self.ogrid_xy + self.offsets[1, :, :].view(1, -1,1,2)).view(1, -1, self.o_m, self.o_n)\n",
    "\n",
    "        x_2 = F.relu(self.batch1(self.layer1(sampled_m)))\n",
    "        x_2 = F.relu(self.batch2(self.layer2(x_2)))\n",
    "        features_moving = self.layer3(x_2)\n",
    "\n",
    "        ssd_distance = self.correlation_layer(features_moving, features_fixed)\n",
    "        soft_cost,disp_xy = self.meanfield(ssd_distance, fixed_img, self.displace_range, self.o_m, self.o_n)\n",
    "        \n",
    "        return soft_cost, disp_xy\n",
    "\n",
    "\n",
    "    def min_convolution(self, ssd_distance, displace_range, H, W):\n",
    "        # Prepare operators for smooth dense displacement space\n",
    "        pad1 = nn.ReplicationPad2d(5)\n",
    "        avg1 = nn.AvgPool2d(5, stride=1)\n",
    "        max1 = nn.MaxPool2d(3, stride=1)\n",
    "        pad2 = nn.ReplicationPad2d(4)\n",
    "        # approximate min convolution / displacement compatibility\n",
    "\n",
    "        ssd_minconv = avg1(avg1(-max1(-pad1(\n",
    "            ssd_distance.permute(0, 2, 3, 1).reshape(1, -1, self.displace_range,\n",
    "                                                    self.displace_range)))))\n",
    "\n",
    "        ssd_minconv = ssd_minconv.permute(0, 2, 3, 1).view(1, -1, H, W)\n",
    "        min_conv_cost = avg1(avg1(pad2(ssd_minconv)))\n",
    "\n",
    "        return min_conv_cost\n",
    "\n",
    "\n",
    "    def meanfield(self, ssd_distance, img_fixed, displace_range, H, W):\n",
    "        crnt_dev = ssd_distance.device\n",
    "\n",
    "        cost = self.min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "        soft_cost = F.softmax(-10 * cost.view(displace_range ** 2, -1).t(), 1)\n",
    "\n",
    "        disp_hw = (displace_range - 1) // 2\n",
    "        disp_mesh_grid = disp_hw * F.affine_grid(torch.eye(2, 3).unsqueeze(0), (\n",
    "        1, 1, displace_range, displace_range), align_corners=True)\n",
    "        disp_mesh_grid /= torch.Tensor([(W - 1) * .5, (H - 1) * .5])\n",
    "\n",
    "        disp_xy = torch.sum(\n",
    "            soft_cost.view(1, H, W, -1, 1) * disp_mesh_grid.view(1, 1, 1, -1,\n",
    "                                                                2).to(crnt_dev),\n",
    "            3).permute(0, 3, 1, 2)\n",
    "\n",
    "        return soft_cost, disp_xy\n",
    "\n",
    "\n",
    "    def correlation_layer(self, feat_moving, feat_fixed):\n",
    "        disp_hw = (self.displacement_width - 1) // 2\n",
    "        feat_moving_unfold = F.unfold(feat_moving.transpose(1, 0),\n",
    "                                    (self.displace_range, self.displace_range),\n",
    "                                    padding=self.disp_hw)\n",
    "        B, C, H, W = feat_fixed.size()\n",
    "\n",
    "        ssd_distance = ((feat_moving_unfold - feat_fixed.view(C, 1, -1)) ** 2).sum(0).view(1, displace_range ** 2, H, W)\n",
    "\n",
    "        return ssd_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25314965",
   "metadata": {},
   "source": [
    "# Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flownet = load_flownet2().cuda()\n",
    "flownet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd158d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc = load_pwcnet().cuda()\n",
    "pwc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258e39f",
   "metadata": {},
   "source": [
    "# Experiment 2.1\n",
    "Using soft labels of flownet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.002\n",
    "# minibatch training\n",
    "grad_accum = 20\n",
    "\n",
    "student = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "unwarped = []\n",
    "scale=1\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i, rnd_idx in enumerate(rnd_train_idx):\n",
    "        tmp_loss = []\n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous() * 2\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous() * 2\n",
    "\n",
    "        if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "            fixed_seg = fixed_seg*2\n",
    "        if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "            moving_seg = moving_seg*2\n",
    "        \n",
    "        ########## FlowNet2\n",
    "        # Here we have to rescale the images for the flownet\n",
    "        # the flownet expects intputs that match x*64, when m and n < 200\n",
    "        teacher_fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "        teacher_moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "        # Generate the teacher flow estimation\n",
    "        flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(scale*64,scale*64,1),teacher_moving.detach().clone().reshape(scale*64,scale*64,1)).cuda() * 255\n",
    "        flownet_flow = flownet(flow_in)\n",
    "        flownet_flow = F.interpolate(flownet_flow.cpu(), size=(H,W), mode='bicubic')\n",
    "\n",
    "        # warp segmentation with flownet flow\n",
    "        warped_flownet_seg = warp(moving_seg.float().unsqueeze(0).cuda(), flownet_flow.cuda()).cpu()\n",
    "\n",
    "        # Label preparation for PDD\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving_onehot = F.one_hot(moving_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving_onehot,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        label_fixed = F.one_hot(fixed_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ########## PDD Forward pass\n",
    "            feat00 = student(fixed.cuda())\n",
    "            feat50 = student(moving.cuda())\n",
    "\n",
    "            # compute the cost tensor using the correlation layer\n",
    "            ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "            # compute the MIN-convolution & probabilistic output with the given function\n",
    "            soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "\n",
    "\n",
    "            # warp the label\n",
    "            label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "\n",
    "            flownet_onehot_seg = F.one_hot(F.interpolate(warped_flownet_seg, size=(H//4,W//4)).long(),3)\n",
    "\n",
    "            diff = torch.mean(torch.pow(label_warped-flownet_onehot_seg.view(label_warped.shape),2))\n",
    "            label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2),0)\n",
    "\n",
    "            diffloss = 1.5*((disp_xy[0,:,1:,:]-disp_xy[0,:,:-1,:])**2).mean()+\\\n",
    "                1.5*((disp_xy[0,1:,:,:]-disp_xy[0,:-1,:,:])**2).mean()+\\\n",
    "                1.5*((disp_xy[0,:,:,1:]-disp_xy[0,:,:,:-1])**2).mean()\n",
    "\n",
    "            loss = diff + diffloss #+ label_distance1.mean()\n",
    "        loss.backward()\n",
    "        tmp_loss.append(loss.item())\n",
    "        \n",
    "        if (i+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    losses.append(np.mean(tmp_loss))\n",
    "    eval_dice, unw = evaluate_model(student)\n",
    "    accs.append(eval_dice)\n",
    "    unwarped.append(unw)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627d2c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_mean_dice, unwarped_dice = evaluate_model(student)\n",
    "print(eval_mean_dice)\n",
    "print(unwarped_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c58bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.plot(np.arange(len(unwarped)), unwarped, label='Unwarped')\n",
    "plt.legend()\n",
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "plt.savefig(f'plots/flownet_teacher{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653eedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'SoftTarget flownet + diffloss',grad_accum,lr, mode, eval_mean_dice.item(), unwarped_dice, \"One Hot for MSE + Scale_Factr=1*64 + dataset3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf579902",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), f\"models/Experiment_2/obel16_flownet_soft_teacher_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c71369",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "\n",
    "# quick fix here...\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat = student(fixed.cuda())\n",
    "    moving_feat = student(moving.cuda())\n",
    "\n",
    "ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,3)\n",
    "\n",
    "print(f\"{d1} VS unwarped {d0}\")\n",
    "\n",
    "rgb = showFlow(dense_flow_fit.cpu().transpose(-2,-1))\n",
    "overlay = overlaySegment(fixed.squeeze(),warped_student_seg.data.squeeze(),False)\n",
    "\n",
    "overlay_fixed = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),False)\n",
    "overlay_moving = overlaySegment(moving.squeeze(),moving_seg.data.squeeze(),False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"Flow field\")\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Warped\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(overlay_fixed)\n",
    "plt.title('Fixed')\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_moving)\n",
    "plt.title(\"Moving\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb37986",
   "metadata": {},
   "source": [
    "# Using PWC-Net as Teacher\n",
    "\n",
    "again we need to figure out what loss works best. The KL_Div of the onehot encoding between the warped labels of the student and teacher did not work great previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a943e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.0002\n",
    "# minibatch training\n",
    "grad_accum = 20\n",
    "\n",
    "student = OBELISK2d(16)\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "unwarped = []\n",
    "scale=1\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i, rnd_idx in enumerate(rnd_train_idx):\n",
    "        tmp_loss = []\n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous() * 2\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous() * 2\n",
    "        \n",
    "        if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "            fixed_seg = fixed_seg*2\n",
    "        if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "            moving_seg = moving_seg*2\n",
    "        ########## PWC-Net\n",
    "        # Here we rescale the images for the Teacher \n",
    "        # the flownet expects intputs that match Nx64. \n",
    "        teacher_fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "        teacher_moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "\n",
    "        # Generate the teacher flow estimation\n",
    "        pwc_flow_in = preprocessing_pwc(teacher_fixed.detach().clone().reshape(scale*64,scale*64,1),teacher_moving.detach().clone().reshape(scale*64,scale*64,1)).cuda()\n",
    "        pwc_flow = pwc(pwc_flow_in)\n",
    "        pwc_flow = pwc_flow[0] * 20.0\n",
    "        pwc_flow = F.interpolate(pwc_flow.unsqueeze(0), size=(H,W))\n",
    "\n",
    "        # warp the segmentations with pwc flow\n",
    "        warped_pwc_seg = warp(moving_seg.float().unsqueeze(0).cuda(), pwc_flow.cuda()).cpu()\n",
    "\n",
    "\n",
    "        # Label preparation for PDD\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving_onehot = F.one_hot(moving_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving_onehot,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        label_fixed = F.one_hot(fixed_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bicubic')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ########## PDD Forward pass\n",
    "            feat00 = student(fixed.cuda())\n",
    "            feat50 = student(moving.cuda())\n",
    "\n",
    "            # compute the cost tensor using the correlation layer\n",
    "            ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "            # compute the MIN-convolution & probabilistic output with the given function\n",
    "            soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "\n",
    "            # warp the label PDD\n",
    "            label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "\n",
    "            pwc_onehot_seg = F.one_hot(F.interpolate(warped_pwc_seg, size=(H//4,W//4)).long(),3)\n",
    "\n",
    "            diff = torch.mean(torch.pow(label_warped-pwc_onehot_seg.view(label_warped.shape),2))\n",
    "            label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2),0)\n",
    "\n",
    "            diffloss = 1.5*((disp_xy[0,:,1:,:]-disp_xy[0,:,:-1,:])**2).mean()+\\\n",
    "                1.5*((disp_xy[0,1:,:,:]-disp_xy[0,:-1,:,:])**2).mean()+\\\n",
    "                1.5*((disp_xy[0,:,:,1:]-disp_xy[0,:,:,:-1])**2).mean()\n",
    "        \n",
    "            loss = diff + diffloss# + label_distance1.mean()\n",
    "        loss.backward()\n",
    "        tmp_loss.append(loss.item())\n",
    "        \n",
    "        if (i+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    losses.append(np.mean(tmp_loss))\n",
    "    eval_dice, unw = evaluate_model(student)\n",
    "    accs.append(eval_dice)\n",
    "    unwarped.append(unw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91811d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "eval_mean_dice, unwarped_dice = evaluate_model(student)\n",
    "print(eval_mean_dice)\n",
    "print(unwarped_dice)\n",
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.plot(np.arange(len(unwarped)), unwarped, label='Unwarped')\n",
    "plt.legend()\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "plt.savefig(f'plots/pwc_teacher_{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'SoftTarget PWC + diffloss',grad_accum,lr, mode, eval_mean_dice.item(), unwarped_dice, \"One Hot for MSE + Scale_Factr=1*64 + dataset3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), f\"models/Experiment_2/obel16_pwc_soft_teacher_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.eval()\n",
    "\n",
    "rnd_test_idx = torch.randperm(test_set.size(0))\n",
    "p_fix = test_set[rnd_test_idx[0]]\n",
    "\n",
    "fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "\n",
    "# quick fix here...\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_feat = student(fixed.cuda())\n",
    "    moving_feat = student(moving.cuda())\n",
    "\n",
    "ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "#apply and evaluate transformation\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "#warped_teacher_seg = warp(moving_seg.unsqueeze(0).float().cuda(),teacher_flow.squeeze().cuda()).cpu()\n",
    "\n",
    "d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "d0 = dice_coeff(fixed_seg,moving_seg,3)\n",
    "\n",
    "print(f\"{d1} VS unwarped {d0}\")\n",
    "\n",
    "rgb = showFlow(dense_flow_fit.cpu().transpose(-2,-1))\n",
    "overlay = overlaySegment(fixed.squeeze(),warped_student_seg.data.squeeze(),False)\n",
    "\n",
    "overlay_fixed = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),False)\n",
    "overlay_moving = overlaySegment(moving.squeeze(),moving_seg.data.squeeze(),False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"Flow field\")\n",
    "plt.subplot(222)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Warped\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(overlay_fixed)\n",
    "plt.title('Fixed')\n",
    "plt.subplot(224)\n",
    "plt.imshow(overlay_moving)\n",
    "plt.title(\"Moving\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b472bd",
   "metadata": {},
   "source": [
    "# Training with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "label_weights = torch.tensor([0.1,0.6, 0.3])# weights for background = 0.1, Vein = 0.6 and Artery = 0.3\n",
    "epochs = 100\n",
    "lr = 0.00025\n",
    "grad_accum = 5\n",
    "\n",
    "H=150;W=150\n",
    "\n",
    "student = OBELISK2d(16)\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr, weight_decay=0.00005)\n",
    "alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95561e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "unwarped = []\n",
    "scale=2\n",
    "for epoch in trange(epochs):\n",
    "\n",
    "    # Cross Validation\n",
    "    train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "    test_set = torch.arange(len(imgs))\n",
    "    for idx in train_set:\n",
    "        test_set = test_set[test_set != idx]\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i, rnd_idx in enumerate(rnd_train_idx):\n",
    "        tmp_loss = []\n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous() * 2\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous() * 2\n",
    "\n",
    "        if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "            fixed_seg = fixed_seg*2\n",
    "        if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "            moving_seg = moving_seg*2\n",
    "        ########## PWC-Net\n",
    "        # Here we rescale the images for the Teacher \n",
    "        # the flownet expects intputs that match Nx64. \n",
    "        teacher_fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "        teacher_moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "\n",
    "        # Generate the pwc flow estimation\n",
    "        pwc_flow_in = preprocessing_pwc(teacher_fixed.detach().clone().reshape(scale*64,scale*64,1),teacher_moving.detach().clone().reshape(scale*64,scale*64,1)).cuda()\n",
    "        pwc_flow = pwc(pwc_flow_in) \n",
    "        pwc_flow = F.interpolate(pwc_flow, size=(H,W),mode='bicubic')\n",
    "\n",
    "        # warp the segmentations with pwc flow\n",
    "        warped_pwc_seg = warp(moving_seg.float().unsqueeze(0).cuda(), pwc_flow.cuda()).cpu()\n",
    "        #print(dice_coeff(warped_pwc_seg, fixed_seg, 3))\n",
    "         ########## FlowNet2\n",
    "        # Generate the flownet flow estimation\n",
    "        flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(scale*64,scale*64,1),teacher_moving.detach().clone().reshape(scale*64,scale*64,1)).cuda() * 255\n",
    "        flownet_flow = flownet(flow_in)\n",
    "        flownet_flow = F.interpolate(flownet_flow.cpu(), size=(H,W), mode='bicubic')\n",
    "\n",
    "        # warp segmentation with flownet flow\n",
    "        warped_flownet_seg = warp(moving_seg.float().unsqueeze(0).cuda(), flownet_flow.cuda()).cpu()\n",
    "        \n",
    "\n",
    "        # Label preparation for PDD\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving_onehot = F.one_hot(moving_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving_onehot,size=(Hf//4 +1,Wf//4 +1),mode='bicubic')\n",
    "        label_fixed = F.one_hot(fixed_seg.long(),num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4 +1,Wf//4 +1),mode='bicubic')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        ########## PDD Forward pass\n",
    "        soft_cost,disp_xy = student(fixed.cuda(), moving.cuda())\n",
    "        #feat50 = student(moving.cuda())\n",
    "\n",
    "        # compute the cost tensor using the correlation layer\n",
    "        #ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        #soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "\n",
    "        # warp the label\n",
    "        label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "        \n",
    "        \n",
    "        #dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "        #apply and evaluate transformation\n",
    "        #identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "        #warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu() \n",
    "        \n",
    "        pwc_onehot = labelMatrixOneHot(F.interpolate(warped_pwc_seg, size=(H//4 +1, W//4 +1), mode='bicubic').view(1,H//4 +1, W//4 +1), 3)\n",
    "        flownet_onehot = labelMatrixOneHot(F.interpolate(warped_flownet_seg, size=(H//4 +1, W//4 +1), mode='bicubic').view(1,H//4 +1, W//4 +1), 3)\n",
    "\n",
    "        pwc_diff = torch.sum(torch.pow(label_warped.view(3,-1)-pwc_onehot.view(3,-1).detach(),2), 1)\n",
    "        flownet_diff = torch.sum(torch.pow(label_warped.view(3,-1)-flownet_onehot.view(label_warped.shape),2), 1)\n",
    "\n",
    "        label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2),1) * label_weights\n",
    "\n",
    "        diffloss = 2*((disp_xy[0,:,1:,:]-disp_xy[0,:,:-1,:])**2).mean()+\\\n",
    "            2*((disp_xy[0,1:,:,:]-disp_xy[0,:-1,:,:])**2).mean()+\\\n",
    "            2*((disp_xy[0,:,:,1:]-disp_xy[0,:,:,:-1])**2).mean()\n",
    "        \n",
    "        \n",
    "        # Caclculate the label weighted teacher loss\n",
    "        teacher_loss = (alpha * pwc_diff + (1-alpha)*flownet_diff) * label_weights\n",
    "        \n",
    "        # Combine the teacherloss with the label loss\n",
    "        loss = teacher_loss.mean() #+ label_distance1.mean() + diffloss\n",
    "        \n",
    "        # propagate backwards\n",
    "        loss.backward()\n",
    "        tmp_loss.append([teacher_loss.mean().item(),\n",
    "                        diffloss.item(),\n",
    "                        label_distance1.mean().item(), \n",
    "                        loss.item()])\n",
    "        \n",
    "        if i %grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    losses.append(np.mean(tmp_loss, axis=0))\n",
    "    print(np.mean(tmp_loss, axis=0))\n",
    "    #eval_dice, unw = evaluate_model(student)\n",
    "    #accs.append(eval_dice)\n",
    "    #print(np.mean(tmp_loss, axis=0))\n",
    "    #unwarped.append(unw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00021872",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "torch.save(student.state_dict(), f\"models/Experiment_2/obel_16_soft_WL_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,0], label='Combined Teacher Loss', alpha=alpha)\n",
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,2], label='Label Loss', alpha=alpha)\n",
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,3], label='Overall Loss', alpha=alpha)\n",
    "#plt.plot(np.arange(len(losses)), np.array(losses)[:,4], label='Overall Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"plots/KD_soft-100_{now}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,0], label='PWC Loss', alpha=alpha)\n",
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,1], label='Flow Loss', alpha=alpha)\n",
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,2], label='Label Loss', alpha=alpha)\n",
    "plt.plot(np.arange(len(losses)), np.array(losses)[:,3], label='Diff Loss', alpha=alpha)\n",
    "#plt.plot(np.arange(len(losses)), np.array(losses)[:,4], label='Overall Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f345a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "torch.save(student.state_dict(), f\"models/Experiment_2/obel_16_kd_{now}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(accs)), accs, label='Eval acc')\n",
    "plt.plot(np.arange(len(unwarped)), unwarped, label='Unwarped')\n",
    "plt.legend()\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d_%m_%y-%H-%M\")\n",
    "#plt.savefig(f'plots/ensemble_teacher_soft_{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optimizer, 'Ensemble + diffloss+ Label',grad_accum,lr, mode, eval_mean_dice.item(), unwarped_dice, \"One Hot for KLDiv + Scale_Factr=1*64 + alpha=0.5 + dataset3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ebad9",
   "metadata": {},
   "source": [
    "# !!ALL BELOW DID NOT WORK!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec28970",
   "metadata": {},
   "source": [
    "# Experiment 2.2\n",
    "Using MSE loss between the flow predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = OBELISK2d(24)\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfaa666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for rnd_idx in rnd_train_idx:\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].long().contiguous()\n",
    "\n",
    "        # Here we have to rescale the images for the flownet\n",
    "        # the flownet expects intputs that match x*64, when m and n < 200\n",
    "        teacher_fixed = F.interpolate(fixed, size=(128,128))\n",
    "        teacher_moving = F.interpolate(moving, size=(128,128))\n",
    "        # Generate the teacher flow estimation\n",
    "        flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(128,128,1),teacher_moving.detach().clone().reshape(128,128,1)).cuda()\n",
    "        teacher_flow = flownet(flow_in)\n",
    "        teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "        \n",
    "        # Label preparation\n",
    "        #C1,Hf,Wf = moving_seg.size()\n",
    "        #label_moving_onehot = F.one_hot(moving_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        #label_moving = F.interpolate(label_moving_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        #label_fixed_onehot = F.one_hot(fixed_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        #label_fixed = F.interpolate(label_fixed_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        #label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        feat00 = student(fixed.cuda())\n",
    "        feat50 = student(moving.cuda())\n",
    "\n",
    "        # compute the cost tensor using the correlation layer\n",
    "        ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "        #loss = cross_entropy_loss(disp_xy,teacher_flow)\n",
    "        loss = torch.sum(torch.pow(disp_xy - teacher_flow, 2))\n",
    "        loss.backward()\n",
    "        if (epoch+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0462ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), \"models/obel_MSE_teacher_flow.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d5ff8",
   "metadata": {},
   "source": [
    "# Experiment 2.3\n",
    "Using warped label loss as simple addition to the MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0435291",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = OBELISK2d(24)\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for rnd_idx in rnd_train_idx:\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].long().contiguous()\n",
    "\n",
    "        # Here we have to rescale the images for the flownet\n",
    "        # the flownet expects intputs that match x*64, when m and n < 200\n",
    "        teacher_fixed = F.interpolate(fixed, size=(128,128))\n",
    "        teacher_moving = F.interpolate(moving, size=(128,128))\n",
    "        # Generate the teacher flow estimation\n",
    "        flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(128,128,1),teacher_moving.detach().clone().reshape(128,128,1)).cuda()\n",
    "        teacher_flow = flownet(flow_in)\n",
    "        teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "        \n",
    "        # Label preparation\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving_onehot = F.one_hot(moving_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        label_fixed_onehot = F.one_hot(fixed_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        feat00 = student(fixed.cuda())\n",
    "        feat50 = student(moving.cuda())\n",
    "\n",
    "        # compute the cost tensor using the correlation layer\n",
    "        ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "        # warp the label\n",
    "        label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "        # warp segment with teacher flow\n",
    "        warped_teacher_seg = warp_seg(label_moving.squeeze().float().cuda(),teacher_flow.cuda()).cpu()\n",
    "        \n",
    "        teacher_loss = torch.sum(torch.pow(disp_xy - teacher_flow, 2))\n",
    "        label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2),0)\n",
    "        loss = teacher_loss + label_distance1.mean()\n",
    "        if (epoch+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b048572",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), \"models/obel_MSE_warped.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c94593",
   "metadata": {},
   "source": [
    "# Experiment 2.4\n",
    "Using the weighted label loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3579ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = OBELISK2d(24)\n",
    "init_weights(student)\n",
    "student.train().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595808ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for rnd_idx in rnd_train_idx:\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].long().contiguous()\n",
    "\n",
    "        # Here we have to rescale the images for the flownet\n",
    "        # the flownet expects intputs that match x*64, when m and n < 200\n",
    "        teacher_fixed = F.interpolate(fixed, size=(128,128))\n",
    "        teacher_moving = F.interpolate(moving, size=(128,128))\n",
    "        # Generate the teacher flow estimation\n",
    "        flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(128,128,1),teacher_moving.detach().clone().reshape(128,128,1)).cuda()\n",
    "        teacher_flow = flownet(flow_in)\n",
    "        teacher_flow = F.interpolate(teacher_flow, size=(H//4,W//4), mode='bilinear')\n",
    "        \n",
    "        # Label preparation\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving_onehot = F.one_hot(moving_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        label_fixed_onehot = F.one_hot(fixed_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed_onehot,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1)\n",
    "\n",
    "        feat00 = student(fixed.cuda())\n",
    "        feat50 = student(moving.cuda())\n",
    "\n",
    "        # compute the cost tensor using the correlation layer\n",
    "        ssd_distance = correlation_layer(displace_range, feat50, feat00)\n",
    "\n",
    "        # compute the MIN-convolution & probabilistic output with the given function\n",
    "        soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "        \n",
    "        # warp the label\n",
    "        label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "        # warp segment with teacher flow\n",
    "        warped_teacher_seg = warp_seg(moving_seg.float().cuda(),F.interpolate(teacher_flow, size=(H,W), mode='bilinear').cuda()).cpu()\n",
    "        \n",
    "        # calculate losses\n",
    "        teacher_loss = torch.sum(torch.pow(disp_xy - teacher_flow, 2))\n",
    "        label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-label_warped.reshape(3,-1),2),0)\n",
    "        \n",
    "        # calculate the dice score for the warped segmentation\n",
    "        d0 = dice_coeff(warped_teacher_seg.squeeze(), fixed_seg.unsqueeze(0).float(), 3)\n",
    "        \n",
    "        # The loss can then be weighted\n",
    "        loss = (d0.mean().item()) * teacher_loss + (1- d0.mean().item()) * label_distance1.mean()\n",
    "        if (epoch+1)%grad_accum == 0:\n",
    "            # every grad_accum iterations :Make an optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), \"models/obel_MSE_weighted_warped.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686a2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
