{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eab2d9",
   "metadata": {},
   "source": [
    "# Trying to finetune flownet2 on US data\n",
    "\n",
    "Did not yield good results. Should be further investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f833bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<torch.cuda.device object at 0x7fb7c1673c10>, 'GeForce RTX 2080 Ti')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp, warpImage\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "from models.flownet2_pytorch.flownet2_mph import *\n",
    "from models.flownet2_pytorch.flownet2_components import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39842d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135 train examples\n",
      "60 test examples\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames.pth')\n",
    "segs = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs.pth')\n",
    "\n",
    "# Go over the segs. Some values are [0,1], but we need [0,2]\n",
    "#for i, seg in enumerate(segs):\n",
    "#    f_seg = seg[0]\n",
    "#    m_seg = seg[1]\n",
    "#    if len(torch.where(torch.histc(f_seg) != 0)[0]) == 3 and f_seg.max() <= 2:\n",
    "#        segs[i][0] = segs[i][0]*2 \n",
    "#    if len(torch.where(torch.histc(m_seg) != 0)[0]) == 3 and m_seg.max() <= 2:\n",
    "#        segs[i][1] = segs[i][1]*2 \n",
    "\n",
    "#define a training split \n",
    "torch.manual_seed(42)\n",
    "# Now, we prepare our train & test dataset.\n",
    "train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "test_set = torch.arange(len(imgs))\n",
    "for idx in train_set:\n",
    "    test_set = test_set[test_set != idx]\n",
    "\n",
    "\n",
    "print(f\"{train_set.shape[0]} train examples\")\n",
    "print(f\"{test_set.shape[0]} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcefa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_seg(moving_seg, flow):\n",
    "    \"\"\"\n",
    "    function to warp the segemntation of the teacher and baseline\n",
    "    \n",
    "    moving_seg: CxHxW\n",
    "    flow: size: BxCxHxW\n",
    "    \"\"\"\n",
    "    B, C, H, W = flow.size()\n",
    "    # mesh grid\n",
    "    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1).float().to(flow.device)\n",
    "    \n",
    "    vgrid = grid + flow\n",
    "\n",
    "    # scale grid to [-1,1]\n",
    "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "    warped_seg_grid = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "    return warped_seg_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cbce34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flownet = FlowNet2()\n",
    "state_dict = torch.load(\"models/flownet2_pytorch/FlowNet2_checkpoint.pth.tar\")\n",
    "flownet.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3994d71b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# freeze all parameters from the other blocks, except the fusion block\n",
    "for param in flownet.flownetc.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flownet.flownets_1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flownet.flownets_2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flownet.flownets_d.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41760aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlowNet2(\n",
       "  (flownetc): FlowNetC(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv_redir): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (corr_activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (conv3_1): Sequential(\n",
       "      (0): Conv2d(473, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv5): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv4): Sequential(\n",
       "      (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv3): Sequential(\n",
       "      (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv2): Sequential(\n",
       "      (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow5): Conv2d(1026, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow4): Conv2d(770, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow3): Conv2d(386, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow2): Conv2d(194, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsample1): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       "  (upsample1): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  (flownets_1): FlowNetS(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(12, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3_1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv5): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv4): Sequential(\n",
       "      (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv3): Sequential(\n",
       "      (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv2): Sequential(\n",
       "      (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow5): Conv2d(1026, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow4): Conv2d(770, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow3): Conv2d(386, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow2): Conv2d(194, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsample1): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       "  (upsample2): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  (flownets_2): FlowNetS(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(12, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3_1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv5): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv4): Sequential(\n",
       "      (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv3): Sequential(\n",
       "      (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv2): Sequential(\n",
       "      (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow5): Conv2d(1026, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow4): Conv2d(770, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow3): Conv2d(386, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow2): Conv2d(194, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (upsample1): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       "  (flownets_d): FlowNetSD(\n",
       "    (conv0): Sequential(\n",
       "      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv1_1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2_1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv3_1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv4_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5_1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv6_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv5): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv4): Sequential(\n",
       "      (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv3): Sequential(\n",
       "      (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv2): Sequential(\n",
       "      (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (inter_conv5): Sequential(\n",
       "      (0): Conv2d(1026, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (inter_conv4): Sequential(\n",
       "      (0): Conv2d(770, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (inter_conv3): Sequential(\n",
       "      (0): Conv2d(386, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (inter_conv2): Sequential(\n",
       "      (0): Conv2d(194, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow5): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow4): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow3): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow2): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsample1): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       "  (upsample3): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (upsample4): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (flownetfusion): FlowNetFusion(\n",
       "    (conv0): Sequential(\n",
       "      (0): Conv2d(11, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv1_1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv2_1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv1): Sequential(\n",
       "      (0): ConvTranspose2d(128, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (deconv0): Sequential(\n",
       "      (0): ConvTranspose2d(162, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (inter_conv1): Sequential(\n",
       "      (0): Conv2d(162, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (inter_conv0): Sequential(\n",
       "      (0): Conv2d(82, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (predict_flow2): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow1): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (predict_flow0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsampled_flow2_to_1): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsampled_flow1_to_0): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flownet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2be89",
   "metadata": {},
   "source": [
    "# Before finetuning\n",
    "Before fine tuning, we need to see the performance of the flownet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412a542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval Flownet\n",
    "def eval_flownet(model):\n",
    "    overall_dice = []\n",
    "    unwarped_dice = [] \n",
    "    scale=4\n",
    "    for i,idx in enumerate(test_set):\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[idx:idx+1,0,:].contiguous()\n",
    "        moving_seg = segs[idx:idx+1,1,:].contiguous()\n",
    "        \n",
    "        fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "        moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "        \n",
    "        fixed_seg = F.interpolate(fixed_seg.unsqueeze(0), size=(scale*64,scale*64), mode='bicubic')\n",
    "        moving_seg = F.interpolate(moving_seg.unsqueeze(0), size=(scale*64,scale*64), mode='bicubic')\n",
    "        \n",
    "        flow_in = preprocessing_flownet(fixed.detach().clone().reshape(scale*64,scale*64,1),moving.clone().reshape(scale*64,scale*64,1)).cuda()\n",
    "        \n",
    "        flow_out = flownet(flow_in)\n",
    "        \n",
    "        warped_seg = warp_seg(moving_seg.view(1,scale*64,scale*64).cuda(), flow_out).cpu()\n",
    "        \n",
    "        d1 = dice_coeff(warped_seg,fixed_seg,3)\n",
    "        d2 = dice_coeff(moving_seg, fixed_seg, 3)\n",
    "            \n",
    "        overall_dice.append(d1.mean())\n",
    "        unwarped_dice.append(d2.mean())\n",
    "        \n",
    "    overall_dice = torch.from_numpy(np.array(overall_dice))\n",
    "    unwarped_dice = torch.from_numpy(np.array(unwarped_dice))\n",
    "    \n",
    "    return overall_dice.mean(), unwarped_dice.mean()\n",
    "    #print(f\"This model has an average Dice of {round(overall_dice.mean().item(), 5)} mit Variance: {round(overall_dice.var().item(), 5)}. The unwarped Mean dice is: {round(unwarped_dice.mean().item(), 5)} with Var {round(unwarped_dice.var().item(),5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad70528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.3655), tensor(0.3404))\n"
     ]
    }
   ],
   "source": [
    "print(eval_flownet(flownet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eda568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "lr = 0.00001\n",
    "# minibatch training\n",
    "grad_accum = 30\n",
    "\n",
    "optimizer = torch.optim.Adam(list(flownet.parameters()),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "acc = []\n",
    "scale=4\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for i, rnd_idx in enumerate(rnd_train_idx):\n",
    "        tmp_loss = []\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].contiguous()\n",
    "        \n",
    "        fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "        moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "        \n",
    "        fixed_seg = F.interpolate(fixed_seg.unsqueeze(0), size=(scale*64,scale*64), mode='bicubic')\n",
    "        moving_seg = F.interpolate(moving_seg.unsqueeze(0), size=(scale*64,scale*64), mode='bicubic')\n",
    "        \n",
    "        flow_in = preprocessing_flownet(fixed.detach().clone().reshape(scale*64,scale*64,1),moving.clone().reshape(scale*64,scale*64,1)).cuda()\n",
    "        \n",
    "        flow_out = flownet(flow_in)\n",
    "        \n",
    "        warped_seg = warp_seg(moving_seg.view(1,scale*64,scale*64).cuda(), flow_out).cpu()\n",
    "        warped_seg_onehot = F.one_hot(warped_seg.long(),num_classes=2).float()\n",
    "        fixed_seg_onehot = F.one_hot(fixed_seg.long(), num_classes=2).float()\n",
    "        \n",
    "        loss = torch.sum(torch.pow(warped_seg-fixed_seg,2)).mean()\n",
    "        loss.backward()\n",
    "        tmp_loss.append(loss.item())\n",
    "    \n",
    "        \n",
    "    if (epoch+1)%grad_accum == 0:\n",
    "        # every grad_accum iterations :Make an optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        d0 = []\n",
    "        for i,idx in enumerate(test_set):\n",
    "\n",
    "            # Get image and segmentation\n",
    "            fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "            moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "            fixed_seg = segs[idx:idx+1,0,:].contiguous()\n",
    "            moving_seg = segs[idx:idx+1,1,:].contiguous()\n",
    "\n",
    "            fixed = F.interpolate(fixed, size=(128,128), mode='bicubic')\n",
    "            moving = F.interpolate(moving, size=(128,128), mode='bicubic')\n",
    "\n",
    "            fixed_seg = F.interpolate(fixed_seg.unsqueeze(0), size=(128,128), mode='bicubic')\n",
    "            moving_seg = F.interpolate(moving_seg.unsqueeze(0), size=(128,128), mode='bicubic')\n",
    "\n",
    "            flow_in = preprocessing_flownet(fixed.detach().clone().reshape(128,128,1),moving.clone().reshape(128,128,1)).cuda()\n",
    "\n",
    "            flow_out = flownet(flow_in)\n",
    "\n",
    "            warped_seg = warp_seg(moving_seg.view(1,128,128).cuda(), flow_out).cpu()\n",
    "\n",
    "            d1 = dice_coeff(warped_seg,fixed_seg,3)\n",
    "            d0.append(d1.mean())\n",
    "    acc.append(np.mean(d0))\n",
    "    losses.append(np.mean(tmp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(acc)), acc)\n",
    "plt.savefig('Flownet_finetune_acc_500epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265445ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)\n",
    "plt.savefig('Flownet_finetune_loss_500epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_flownet(flownet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1659afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(flownet.state_dict(), \"flownet_finetuned_500.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5221e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0522c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
