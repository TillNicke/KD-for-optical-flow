{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a375e2ee",
   "metadata": {},
   "source": [
    "# Testing evaluation Video Making\n",
    "\n",
    "Generating an overlay frame for visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/share/data_ultraschall/compressions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the quality overview\n",
    "quality = pd.read_csv(\"~/quality.csv\")\n",
    "\n",
    "# More info about the sequences\n",
    "sequences = pd.read_csv(\"~/sequences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d932431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter based on the table above\n",
    "useable = quality[\n",
    "    (quality[\"Vessel in Frame\"] <3) &\n",
    "    (quality['LM configuration'] < 4) &\n",
    "    (quality['Vein contrast/cropping error'] < 3) &\n",
    "    (quality['Artery contrast'] < 3) &\n",
    "    (quality['Vein boundary'] < 3) &\n",
    "    (quality['Artery boundary'] < 3) &\n",
    "    (quality['Gain'] == 1) &\n",
    "    (quality['Artefacts'] < 3) &\n",
    "    (quality['Movement'] < 3) &\n",
    "    (quality['Total'] < 21)\n",
    "]\n",
    "useable\n",
    "\n",
    "# filter based on useable\n",
    "overview = sequences[sequences.Id.isin(useable.index.tolist())]\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_clean = overview[overview['Id'].astype(str).isin(os.listdir(path_to_data))]\n",
    "overview_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_seg = []\n",
    "for proband in os.listdir(path_to_data):\n",
    "    proband_path = os.path.join(path_to_data,str(proband))\n",
    "    if \"segmentations\" in os.listdir(proband_path):\n",
    "        available_seg.append(proband)\n",
    "len(available_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4732c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for proband in available_seg:\n",
    "    available_frames = len(os.listdir(os.path.join(path_to_data,proband,'frames')))\n",
    "    if available_frames > 200:\n",
    "        print(f\"There are {available_frames} frames available for id {proband}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_id='1164'\n",
    "frame_path = os.path.join(path_to_data,prob_id,'frames')\n",
    "seg_path = os.path.join(path_to_data,prob_id,'segmentations','1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31070713",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_list = []\n",
    "for frame in os.listdir(frame_path):\n",
    "    frame_list.append(os.path.join(frame_path,frame))\n",
    "frame_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = []\n",
    "for seg in os.listdir(seg_path):\n",
    "    seg_list.append(os.path.join(seg_path,seg))\n",
    "seg_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695d240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"{len(seg_list)} Segmentations and {len(frame_list)} Frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa32da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the overlay works\n",
    "def overlaySegment(gray1,seg1,flag=False):\n",
    "    H, W = seg1.squeeze().size()\n",
    "    colors=torch.FloatTensor([0,0,0,199,67,66,225,140,154,78,129,170,45,170,170,240,110,38,111,163,91,235,175,86,202,255,52,162,0,183]).view(-1,3)/255.0\n",
    "    segs1 = labelMatrixOneHot(seg1.unsqueeze(0),8)\n",
    "\n",
    "    seg_color = torch.mm(segs1.view(8,-1).t(),colors[:8,:]).view(H,W,3)\n",
    "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
    "\n",
    "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
    "    if(flag):\n",
    "        plt.imshow((overlay).numpy())\n",
    "        plt.show()\n",
    "    return overlay\n",
    "def labelMatrixOneHot(segmentation, label_num):\n",
    "    B, H, W = segmentation.size()\n",
    "    values = segmentation.view(B,1,H,W).expand(B,label_num,H,W).to(segmentation.device)\n",
    "    linspace = torch.linspace(0, label_num-1, label_num).long().view(1,label_num,1,1).expand(B,label_num,H,W).to(segmentation.device)\n",
    "    matrix = (values.float()==linspace.float()).float().to(segmentation.device)\n",
    "    for j in range(2,matrix.shape[1]):\n",
    "        matrix[0,j,:,:] = matrix[0,j,:,:]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "assert len(frame_list) == len(seg_list)\n",
    "\n",
    "frs = []\n",
    "fig = plt.figure()\n",
    "for i in range(len(frame_list)):\n",
    "    loaded_frame = torch.from_numpy(np.array(Image.open(frame_list[i]))) / 255\n",
    "    loaded_seg = torch.from_numpy(np.array(Image.open(seg_list[i]))) / 100\n",
    "\n",
    "    overlay = overlaySegment(loaded_frame, loaded_seg, False)\n",
    "    frs.append([plt.imshow(overlay, animated=True)])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frs, interval=50, blit=True, repeat_delay=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9011e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
