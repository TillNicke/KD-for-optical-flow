{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ad2a60",
   "metadata": {},
   "source": [
    "# Notebook used for evaluation\n",
    "\n",
    "This notebook was used to generate the results presented in the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from utils.plotting import showFlow, overlaySegment\n",
    "from utils.encoding import dice_coeff, hausdorff_dist\n",
    "from utils.load_models import load_flownet2, load_pwcnet\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.layers import warp\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33650882",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea06a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (150,150)\n",
    "o_m = H//4 +1\n",
    "o_n = W//4 +1\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2).cuda()\n",
    "disp_range = 0.25#0.25\n",
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2).cuda()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    function to set constant bias to all layers\n",
    "\n",
    "    m: torch.nn.Module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(4)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(pad2(ssd_minconv)))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed27c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading differnt trained models for evluation\n",
    "path_to_state_dict = \"models/Experiment_1/obel16_mix_WL_06_01_22-11-38.pth\"\n",
    "model_classic = OBELISK2d(16)\n",
    "model_classic.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_classic.eval().cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_2/obel16_ensemble_13_10_21-21-30.pth\"\n",
    "model_kc = OBELISK2d(16)\n",
    "model_kc.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_kc.eval().cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_3/27_10_21-10-59/student_16_1.pth\"\n",
    "model_dml = OBELISK2d(16)\n",
    "model_dml.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_dml.eval().cuda()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_2/fineTune/Tuned_Epoch195.pth\"\n",
    "model_classic = OBELISK2d(16)\n",
    "model_classic.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_classic.eval().cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_2/fineTuneSoft/tuned.pth\"\n",
    "model_kc = OBELISK2d(16)\n",
    "model_kc.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_kc.eval().cuda()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for comparison\n",
    "baseline = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "pwc = load_pwcnet().cuda()\n",
    "flownet = load_flownet2().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4695ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdd_warp(model, fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    function to warp segmentations with PDD-Net\n",
    "\n",
    "    model: instance of torch.nn.Module\n",
    "    fixed: torch.tensor of size (HxW)\n",
    "    moving: torch.tensor of size (HxW)\n",
    "    fixed_seg: torch.tensor of siez (HxW)\n",
    "    moving_seg: torch.tensor of siez (HxW)\n",
    "\n",
    "    return: troch.tensor warped segmentation and float runtime\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    # start measuring time\n",
    "    start = time.time()\n",
    "\n",
    "    # run inference\n",
    "    feat1 = model(moving.unsqueeze(0).unsqueeze(0).cuda())\n",
    "    feat2 = model(fixed.unsqueeze(0).unsqueeze(0).cuda())\n",
    "\n",
    "    # run correlation an meanfield inference\n",
    "    ssd_distance = correlation_layer(displace_range, feat2, feat1)\n",
    "    soft_cost,disp_xy = meanfield(ssd_distance, moving, displace_range, H//4 +1, W//4 +1)\n",
    "    \n",
    "    # scaling up the flow\n",
    "    flow=F.interpolate(disp_xy,size=(150,150), mode='bicubic')\n",
    "    end = time.time()\n",
    "    run_time = round(end-start, 4)\n",
    "    \n",
    "    # warp seg\n",
    "    identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "    warped_student_seg = F.grid_sample(fixed_seg.unsqueeze(0).unsqueeze(0).float(),identity+flow.detach().cpu().permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "    return warped_student_seg, run_time\n",
    "\n",
    "def warp_baseline(torch_flow, seg):\n",
    "    \"\"\"\n",
    "    function to warp segs with predicted flow\n",
    "\n",
    "    torch_flow: troch.tensor predicted flow [1 x 2 x 150 x 150]\n",
    "    seg: torch.tensor [150 x 150]\n",
    "\n",
    "    return warped_segmentation torch.tensor [150 x 150]\n",
    "    \"\"\"\n",
    "\n",
    "    B, C, H, W = torch_flow.size()\n",
    "    # mesh grid\n",
    "    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "    vgrid = grid + torch_flow\n",
    "\n",
    "    # scale grid to [-1,1]\n",
    "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "    warped_seg_grid = nn.functional.grid_sample(seg.float().unsqueeze(0).unsqueeze(0), vgrid)\n",
    "    return warped_seg_grid\n",
    "\n",
    "def eval_baseline(fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    evaluate dual-tvl1 on image pair\n",
    "\n",
    "    return  torch.tensor d0 dice of vein and artery\n",
    "            run_time: float\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare for baseline\n",
    "    in1 = moving.view(H,W,1).numpy().astype(np.float32)\n",
    "    in2 = fixed.view(H,W,1).numpy().astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    # run baseline\n",
    "    baseline_flow = baseline.calc(in1,in2,None)\n",
    "    end = time.time()\n",
    "    run_time = round(end-start, 4)\n",
    "    \n",
    "    # warp \n",
    "    warped_seg = warp_baseline(torch.from_numpy(baseline_flow.T).unsqueeze(0), fixed_seg)\n",
    "    d0 = dice_coeff(moving_seg, warped_seg, 3)\n",
    "    return d0, run_time\n",
    "\n",
    "\n",
    "def flownet_warp(fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    warping segmentation masks with Flownet2 Flow\n",
    "\n",
    "    fixed, moving: [1 x 1 x 150 x 150] fixed and moving images as torch.tensor\n",
    "    fixed_seg, moving_seg: [150 x 150] segmenation masks\n",
    "\n",
    "    return: warped fixed segmentation [150 x 150], runtime (float)\n",
    "    \"\"\"\n",
    "    scale=4\n",
    "    B,C,H,W = fixed.shape\n",
    "    teacher_fixed = F.interpolate(fixed, size=(4*64,4*64), mode='bicubic')\n",
    "    teacher_moving = F.interpolate(moving, size=(4*64,4*64), mode='bicubic')\n",
    "    # Generate the teacher flow estimation\n",
    "    flow_in = preprocessing_flownet(teacher_moving.reshape(scale*64,scale*64,1),teacher_fixed.reshape(scale*64,scale*64,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    flownet_flow = flownet(flow_in.cuda()).cpu()\n",
    "    \n",
    "    end = time.time()\n",
    "    run_time = end-start\n",
    "    \n",
    "    flownet_flow = F.interpolate(flownet_flow, size=(H,W), mode='bicubic')\n",
    "\n",
    "    # warp segmentation with flownet flow\n",
    "    warped_flownet_seg = warp(fixed_seg.float().unsqueeze(0).unsqueeze(0), flownet_flow)\n",
    "    \n",
    "    return warped_flownet_seg, run_time\n",
    "\n",
    "def pwc_warp(fixed, moving, fixed_seg, moving_seg):\n",
    "    \"\"\"\n",
    "    warping segmentation masks with PWC-Net Flow\n",
    "\n",
    "    fixed, moving: [1 x 1 x 150 x 150] fixed and moving images as torch.tensor\n",
    "    fixed_seg, moving_seg: [150 x 150] segmenation masks\n",
    "\n",
    "    return: warped fixed segmentation [150 x 150], runtime (float)\n",
    "    \"\"\"\n",
    "    scale=4\n",
    "    B,C,H,W = fixed.shape\n",
    "    \n",
    "    teacher_fixed = F.interpolate(fixed, size=(scale*64,scale*64), mode='bicubic')\n",
    "    teacher_moving = F.interpolate(moving, size=(scale*64,scale*64), mode='bicubic')\n",
    "\n",
    "    # Generate the teacher flow estimation\n",
    "    pwc_flow_in = preprocessing_pwc(teacher_moving.detach().clone().reshape(scale*64,scale*64,1),teacher_fixed.detach().clone().reshape(scale*64,scale*64,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    pwc_flow = pwc(pwc_flow_in.cuda()).cpu()\n",
    "    pwc_flow = pwc_flow[0] * 20.0\n",
    "    \n",
    "    end = time.time()\n",
    "    run_time = end-start\n",
    "    \n",
    "    pwc_flow = F.interpolate(pwc_flow.unsqueeze(0), size=(H,W)).cpu()\n",
    "\n",
    "    # warp the segmentations with pwc flow\n",
    "    warped_pwc_seg = warp(fixed_seg.float().unsqueeze(0).unsqueeze(0), pwc_flow).cpu()\n",
    "    \n",
    "    return warped_pwc_seg, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_id(frames, segs, last_segment_available):\n",
    "    \"\"\"\n",
    "    run evaluation for one specific ID with Method II\n",
    "\n",
    "    frames; segs: torch.tensor [X x 1 x 150 x 150]\n",
    "    last_segment_available: int that indicates the theoretically last available segmantation\n",
    "    \"\"\"\n",
    "    distance_between_frames = 6\n",
    "\n",
    "\n",
    "    classic_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    classic_hd = []\n",
    "    kc_dice =[]# torch.zeros(frames.shape[0],2)\n",
    "    kc_hd = []\n",
    "    \n",
    "    \n",
    "    dml_dice =[]# torch.zeros(frames.shape[0],2)\n",
    "    \n",
    "    #seq_classic_dice = torch.zeros(frames.shape[0],2)\n",
    "    #seq_kc_dice = torch.zeros(frames.shape[0],2)\n",
    "    #seq_dml_dice = torch.zeros(frames.shape[0],2)\n",
    "    \n",
    "    base_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    flow_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    pwc_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    unwarped = []#torch.zeros(frames.shape[0],2)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "\n",
    "        # skipp first X frames\n",
    "        if i < distance_between_frames:\n",
    "            continue\n",
    "            \n",
    "        # if seg is available, select it\n",
    "        if i-distance_between_frames <= last_segment_available:\n",
    "            fixed = torch.clone(frames[i- distance_between_frames])\n",
    "            fixed_seg = torch.clone(segs[i-distance_between_frames])\n",
    "    \n",
    "\n",
    "        # otherwise, choose the same seg every time\n",
    "        if i-distance_between_frames > last_segment_available:\n",
    "            fixed = torch.clone(frames[last_segment_available])\n",
    "            fixed_seg = torch.clone(segs[last_segment_available])\n",
    "            \n",
    "        # running moving frame and seg\n",
    "        moving = torch.clone(frames[i])\n",
    "        moving_seg = torch.clone(segs[i])\n",
    "        \n",
    "        # not segmentation available in this seg\n",
    "        if moving_seg.max().item() == 0.:\n",
    "            continue\n",
    "\n",
    "        ### Run inference and evaluation of different trained models\n",
    "        ### store dice scores and HD for each individual Frame!\n",
    "\n",
    "        # LABELLOSS\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_classic, fixed.float(), moving.float(), fixed_seg, moving_seg)\n",
    "        #classic_segs[i] = pdd_seg\n",
    "        classic_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        classic_hd.append(hausdorff_dist(moving_seg.unsqueeze(0), pdd_seg.unsqueeze(0), 3))\n",
    "\n",
    "        # KC\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_kc, fixed, moving, fixed_seg, moving_seg)\n",
    "        #kc_segs[i] = pdd_seg\n",
    "        kc_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        kc_hd.append(hausdorff_dist(moving_seg.unsqueeze(0), pdd_seg.unsqueeze(0), 3))\n",
    "\n",
    "        # DML\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_dml, fixed, moving, fixed_seg, moving_seg)\n",
    "        #dml_segs[i] = pdd_seg\n",
    "        dml_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "\n",
    "        \n",
    "        #base_d, run_time_base = eval_baseline(fixed.cpu(), moving.cpu(), fixed_seg, moving_seg)\n",
    "        #base_dice.append(base_d)\n",
    "        \n",
    "        #warped_seg, _ = flownet_warp(fixed.unsqueeze(0).unsqueeze(0), moving.unsqueeze(0).unsqueeze(0), fixed_seg, moving_seg)\n",
    "        #flow_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "\n",
    "        # PWC\n",
    "        #warped_seg, run_time = pwc_warp(fixed.unsqueeze(0).unsqueeze(0),moving.unsqueeze(0).unsqueeze(0),fixed_seg,moving_seg)\n",
    "        #pwc_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "        \n",
    "        unwarped.append(dice_coeff(fixed_seg,moving_seg,3))\n",
    "        \n",
    "\n",
    "    classic_tensor = torch.stack(classic_dice)\n",
    "    kc_tensor = torch.stack(kc_dice)\n",
    "    c_hd = torch.stack(classic_hd)\n",
    "    kc_hd = torch.stack(kc_hd)\n",
    "    \n",
    "    \n",
    "    dml_tensor = torch.stack(dml_dice)\n",
    "    #base_tensor = torch.stack(base_dice)\n",
    "    #flow_tensor = torch.stack(flow_dice)\n",
    "    #pwc_tensor = torch.stack(pwc_dice)\n",
    "    unwarped = torch.stack(unwarped)\n",
    "    \n",
    "    return classic_tensor,c_hd, kc_tensor, kc_hd, dml_tensor, unwarped #base_tensor, flow_tensor, pwc_tensor, unwarped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/share/data_ultraschall/compressions\"\n",
    "#ids = [157, 384, 717, 209, 106, 4808, 977, 1097, 1150, 1314, 58, 115, 2709, 2713,4814, 2199, 545, 216, 610, 341, 526, 12, 124, 1778, 195, 379, 327, 384, 2033, 797]\n",
    "ids = [3455, 2232, 3485, 5687, 3360, 3672, 3357, 3628, 3538, 3352, 3586, 3433, 3644, 3387, 3663,3424, 2219, 3649, 3335, 3397, 3683]\n",
    "\n",
    "overall_classic_dice = []\n",
    "overall_c_hd = []\n",
    "overall_kc_dice = []\n",
    "overall_kc_hd = []\n",
    "overall_dml_dice = []\n",
    "\n",
    "\n",
    "overall_base_dice = []\n",
    "overall_flow_dice = []\n",
    "overall_pwc_dice = []\n",
    "overall_unwarped = []\n",
    "\n",
    "# run inference for all IDs\n",
    "for prob_id in ids:\n",
    "    print(prob_id)\n",
    "    frame_path = os.path.join(path_to_data,str(prob_id),'frames')\n",
    "    seg_path = os.path.join(path_to_data,str(prob_id),'segmentations','1')\n",
    "\n",
    "    frame_list = []\n",
    "    for frame in os.listdir(frame_path):\n",
    "        frame_list.append(os.path.join(frame_path,frame))\n",
    "    frame_list.sort()\n",
    "\n",
    "    seg_list = []\n",
    "    for seg in os.listdir(seg_path):\n",
    "        seg_list.append(os.path.join(seg_path,seg))\n",
    "    seg_list.sort()\n",
    "\n",
    "    assert len(frame_list) == len(seg_list)\n",
    "    frames = torch.zeros([len(frame_list), 150,150])\n",
    "    segs = torch.zeros([len(frame_list), 150,150])\n",
    "    # read images\n",
    "    for i in range(len(frame_list)):\n",
    "        frames[i] = torch.from_numpy(np.array(Image.open(frame_list[i]))) / 255.\n",
    "        segs[i] = torch.from_numpy(np.array(Image.open(seg_list[i]))) / 100\n",
    "    \n",
    "    # normalize if needed\n",
    "    #if segs.max() > 3:\n",
    "    #    segs = segs / 100  \n",
    "    #if frames.max() > 2:\n",
    "    #    frames = frames / 255\n",
    "\n",
    "    landmarks = pd.read_csv('landmarks.csv')\n",
    "    landmarks = landmarks[landmarks['Id'].isin(ids)]\n",
    "    if prob_id in landmarks.Id.to_numpy():\n",
    "        start_frame = landmarks[landmarks['Id']== prob_id]['Start Frames'].iat[0]\n",
    "        #print(start_frame)\n",
    "        if isinstance(start_frame, str):\n",
    "            last_segment_available = np.fromstring(start_frame.strip(']['), sep=',', dtype=int)\n",
    "            \n",
    "        else:\n",
    "            last_segment_available = start_frame\n",
    "    else:\n",
    "        last_segment_available = 3\n",
    "    if segs.max().item() == 0.:\n",
    "        print('id not good: ', prob_id)\n",
    "        continue\n",
    "        \n",
    "    #classic_dice, kc_dice, dml_dice, seq_classic_dice, seq_kc_dice, seq_dml_dice , base_dice, flow_dice, pwc_dice, unwarped\n",
    "    #c_d, kc_d, dml_d, base, f_d, pwc_d, un = eval_id(frames,segs, last_segment_available)\n",
    "    c_d, c_hd, kc_d, kc_hd, dml_d, un = eval_id(frames,segs, last_segment_available)\n",
    "    \n",
    "    overall_classic_dice.append(c_d)\n",
    "    overall_c_hd.append(torch.from_numpy(np.nan_to_num(c_hd.numpy(), posinf=np.nan)))\n",
    "    \n",
    "    overall_kc_dice.append(kc_d)\n",
    "    overall_kc_hd.append(torch.from_numpy(np.nan_to_num(kc_hd.numpy(), posinf=np.nan)))\n",
    "    \n",
    "    \n",
    "    overall_dml_dice.append(dml_d)\n",
    "    \n",
    "    #overall_base_dice.append(base)\n",
    "    #overall_flow_dice.append(f_d)\n",
    "    #overall_pwc_dice.append(pwc_d)\n",
    "    overall_unwarped.append(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68716586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over the ID scores\n",
    "\n",
    "c_means = torch.zeros((len(ids), 2))\n",
    "c_hds = torch.zeros((len(ids), 2))\n",
    "kc_means = torch.zeros((len(ids), 2))\n",
    "kc_hds = torch.zeros((len(ids), 2))\n",
    "\n",
    "dml_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "\n",
    "base_means = torch.zeros((len(ids), 2))\n",
    "flow_means = torch.zeros((len(ids), 2))\n",
    "pwc_means = torch.zeros((len(ids), 2))\n",
    "un_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "\n",
    "for idx, dice in enumerate(overall_classic_dice):\n",
    "    \n",
    "    # MEANS DICE\n",
    "    c_means[idx] = torch.Tensor([overall_classic_dice[idx][:,0].mean(),overall_classic_dice[idx][:,1].mean()])\n",
    "    kc_means[idx] = torch.Tensor([overall_kc_dice[idx][:,0].mean(),overall_kc_dice[idx][:,1].mean()])\n",
    "    dml_means[idx] = torch.Tensor([overall_dml_dice[idx][:,0].mean(),overall_dml_dice[idx][:,1].mean()])\n",
    "    \n",
    "    #base_means[idx] = torch.Tensor([overall_base_dice[idx][:,0].mean(),overall_base_dice[idx][:,1].mean()])\n",
    "    #flow_means[idx] = torch.Tensor([overall_flow_dice[idx][:,0].mean(),overall_flow_dice[idx][:,1].mean()])\n",
    "    #pwc_means[idx] = torch.Tensor([overall_pwc_dice[idx][:,0].mean(),overall_pwc_dice[idx][:,1].mean()])\n",
    "    un_means[idx] = torch.Tensor([overall_unwarped[idx][:,0].mean(),overall_unwarped[idx][:,1].mean()])\n",
    "    \n",
    "    # MEANS HD\n",
    "    c_hds[idx] = torch.Tensor([np.nanmean(overall_c_hd[idx][:,0]),np.nanmean(overall_c_hd[idx][:,1])])\n",
    "    kc_hds[idx] = torch.Tensor([np.nanmean(overall_kc_hd[idx][:,0]),np.nanmean(overall_kc_hd[idx][:,1])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_c = []\n",
    "perc_kc = []\n",
    "for idx, dice in enumerate(overall_classic_dice):\n",
    "    perc_c.append(np.array([np.percentile(np.nanmean(overall_c_hd[idx][:,0]), 99),\n",
    "                            np.percentile(np.nanmean(overall_c_hd[idx][:,0]), 99)]))\n",
    "    perc_kc.append(np.array([np.percentile(np.nanmean(overall_kc_hd[idx][:,0]), 99),\n",
    "                            np.percentile(np.nanmean(overall_kc_hd[idx][:,0]), 99)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87299635",
   "metadata": {},
   "source": [
    "# Percentile for Paper\n",
    "\n",
    "Not needed for Thesis and eventuelly not used for Paper. \n",
    "But it is code that was written, and therefore is in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f49ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_c = np.array(perc_c)\n",
    "perc_kc = np.array(perc_kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classic:\")\n",
    "print(\"Vein: \", round(c_means[:,0].mean().item()*100,2),\"$\\pm$\", round(c_means[:,0].var().item()*100,2), ' ; ', round(perc_c[:,0].mean(),2),\"$\\pm$\", round(perc_c[:,0].var(),2))\n",
    "print(\"Artery: \", round(c_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,1].var().item()*100,2), ' ; ', round(perc_c[:,1].mean(),2),\"$\\pm$\", round(perc_c[:,1].var(),2))\n",
    "print(\"Overall: \", round(c_means.mean().item()*100,2),\"$\\pm$\", round(c_means.var().item()*100,2), ' ; ', round(perc_c.mean(),2),\"$\\pm$\", round(perc_c.var().item(),2))\n",
    "print()\n",
    "\n",
    "print(\"KC:\")\n",
    "print(\"Vein: \", round(kc_means[:,0].mean().item()*100,2),\"$\\pm$\", round(kc_means[:,0].var().item()*100,2), ' ; ', round(perc_kc[:,0].mean(),2),\"$\\pm$\", round(perc_kc[:,0].var(),2))\n",
    "print(\"Artery: \", round(kc_means[:,1].mean().item()*100,2),\"$\\pm$\", round(kc_means[:,1].var().item()*100,2), ' ; ', round(perc_kc[:,1].mean().item(),2),\"$\\pm$\", round(perc_kc[:,1].var(),2))\n",
    "print(\"Overall: \", round(kc_means.mean().item()*100,2),\"$\\pm$\", round(kc_means.var().item()*100,2), ' ; ', round(perc_kc.mean(),2),\"$\\pm$\", round(perc_kc.var(),2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a21f5",
   "metadata": {},
   "source": [
    "# Printing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unwapred:\")\n",
    "print(\"Vein: \", round(un_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(un_means[:,0].var().item()*100,3))\n",
    "print(\"Artery: \", round(un_means[:,1].mean().item()*100,3),\" $\\pm$ \", round(un_means[:,1].var().item()*100,3))\n",
    "print(\"Overall: \", round(un_means.mean().item()*100,3),\" $\\pm$ \", round(un_means.var().item()*100,3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classic:\")\n",
    "print(\"Vein: \", round(c_means[:,0].mean().item()*100,2),\"$\\pm$\", round(c_means[:,0].var().item()*100,2), ' ; ', round(c_hds[:,0].mean().item(),2),\"$\\pm$\", round(c_hds[:,0].var().item(),2))\n",
    "print(\"Artery: \", round(c_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,1].var().item()*100,2), ' ; ', round(c_hds[:,1].mean().item(),2),\"$\\pm$\", round(c_hds[:,1].var().item(),2))\n",
    "print(\"Overall: \", round(c_means.mean().item()*100,2),\"$\\pm$\", round(c_means.var().item()*100,2), ' ; ', round(c_hds.mean().item(),2),\"$\\pm$\", round(c_hds.var().item(),2))\n",
    "print()\n",
    "\n",
    "print(\"KC:\")\n",
    "print(\"Vein: \", round(kc_means[:,0].mean().item()*100,2),\"$\\pm$\", round(kc_means[:,0].var().item()*100,2), ' ; ', round(kc_hds[:,0].mean().item(),2),\"$\\pm$\", round(kc_hds[:,0].var().item(),2))\n",
    "print(\"Artery: \", round(kc_means[:,1].mean().item()*100,2),\"$\\pm$\", round(kc_means[:,1].var().item()*100,2), ' ; ', round(kc_hds[:,1].mean().item(),2),\"$\\pm$\", round(kc_hds[:,1].var().item(),2))\n",
    "print(\"Overall: \", round(kc_means.mean().item()*100,2),\"$\\pm$\", round(kc_means.var().item()*100,2), ' ; ', round(kc_hds.mean().item(),2),\"$\\pm$\", round(kc_hds.var().item(),2))\n",
    "print()\n",
    "\n",
    "print(\"DML:\")\n",
    "print(\"Vein: \", round(dml_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(dml_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(dml_means.mean().item()*100,2),\" $\\pm$ \", round(dml_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68165007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Flow2:\")\n",
    "print(\"Vein: \", round(flow_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(flow_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(flow_means[:,1].mean().item()*100,2), \" $\\pm$ \",round(flow_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(flow_means.mean().item()*100,2),\" $\\pm$ \", round(flow_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"PWC:\")\n",
    "print(\"Vein: \", round(pwc_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(pwc_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(pwc_means.mean().item()*100,2),\" $\\pm$ \", round(pwc_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"Dual:\")\n",
    "print(\"Vein: \", round(base_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(base_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(base_means.mean().item()*100,2),\" $\\pm$ \", round(base_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = {'fontsize': 22,'family': 'Latin Modern Roman'}\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "a = 0.8\n",
    "plt.scatter(np.arange(c_means.shape[0]),c_means.mean(axis=1), label='label Loss', alpha=a, marker='o')\n",
    "plt.scatter(np.arange(c_means.shape[0]),kc_means.mean(axis=1), label='KC', alpha=a, marker='s')\n",
    "plt.scatter(np.arange(c_means.shape[0]),dml_means.mean(axis=1), label='DML', alpha=a, marker='^')\n",
    "plt.scatter(np.arange(c_means.shape[0]),un_means.mean(axis=1), label='Unwarped', color='red', marker='x', alpha=a)\n",
    "\n",
    "plt.title(f'Average Dice Score Over {len(ids)} Videos', fontdict=fonts)\n",
    "plt.xlabel('Video ID', fontdict=fonts)\n",
    "plt.ylabel(\"Mean Dice Score over Video\",fontdict=fonts)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\",fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4655d1",
   "metadata": {},
   "source": [
    "# Instead of using one Fixed seg\n",
    "Let's use the predicted segmentation\n",
    "\n",
    "Structure works same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00265c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_id_moving(frames, segs, last_segment_available):\n",
    "    \"\"\"\n",
    "    run evaluation for one specific ID with Method I\n",
    "\n",
    "    frames; segs: torch.tensor [X x 1 x 150 x 150]\n",
    "    last_segment_available: int that indicates the theoretically last available segmantation\n",
    "    \"\"\"\n",
    "\n",
    "    distance_between_frames = 6\n",
    "\n",
    "\n",
    "    classic_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    kc_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    #dml_dice = []#torch.zeros(frames.shape[0],2)\n",
    "   \n",
    "    classic_segs = torch.zeros(segs.shape)\n",
    "    #base_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    #flow_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    #pwc_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    unwarped = []#torch.zeros(frames.shape[0],2)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "\n",
    "        # skipp first X frames\n",
    "        if i < distance_between_frames:\n",
    "            continue\n",
    "            \n",
    "        if i-distance_between_frames <= last_segment_available:\n",
    "            fixed = torch.clone(frames[i- distance_between_frames])\n",
    "            fixed_seg = torch.clone(segs[i-distance_between_frames])\n",
    "    \n",
    "        if i-distance_between_frames > last_segment_available:\n",
    "            fixed = torch.clone(frames[i-distance_between_frames])\n",
    "            fixed_seg = torch.clone(classic_segs[i-distance_between_frames])\n",
    "            \n",
    "        moving = torch.clone(frames[i])\n",
    "        moving_seg = torch.clone(segs[i])\n",
    "        \n",
    "        # not segmentation available in this seg\n",
    "        if moving_seg.max().item() == 0.:\n",
    "            continue\n",
    "\n",
    "        # LABELLOSS\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_classic, fixed, moving, fixed_seg, moving_seg)\n",
    "        classic_segs[i] = pdd_seg.detach()\n",
    "        classic_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "\n",
    "        # KC\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_kc, fixed, moving, fixed_seg, moving_seg)\n",
    "        #kc_segs[i] = pdd_seg\n",
    "        kc_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "\n",
    "        # DML\n",
    "        #pdd_seg, run_time_pdd = pdd_warp(model_dml, fixed, moving, fixed_seg, moving_seg)\n",
    "        #dml_segs[i] = pdd_seg\n",
    "        #dml_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        \n",
    "        #base_d, run_time_base = eval_baseline(fixed.cpu(), moving.cpu(), fixed_seg, moving_seg)\n",
    "        #base_dice.append(base_d)\n",
    "        \n",
    "        #warped_seg, _ = flownet_warp(fixed.unsqueeze(0).unsqueeze(0), moving.unsqueeze(0).unsqueeze(0), fixed_seg, moving_seg)\n",
    "        #flow_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "\n",
    "        # PWC\n",
    "        #warped_seg, run_time = pwc_warp(fixed.unsqueeze(0).unsqueeze(0),moving.unsqueeze(0).unsqueeze(0),fixed_seg,moving_seg)\n",
    "        #pwc_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "        \n",
    "        unwarped.append(dice_coeff(fixed_seg,moving_seg,3))\n",
    "        \n",
    "    \n",
    "    classic_tensor = torch.stack(classic_dice)\n",
    "    kc_tensor = torch.stack(kc_dice)\n",
    "    #dml_tensor = torch.stack(dml_dice)\n",
    "    #base_tensor = torch.stack(base_dice)\n",
    "    #flow_tensor = torch.stack(flow_dice)\n",
    "    #pwc_tensor = torch.stack(pwc_dice)\n",
    "    unwarped = torch.stack(unwarped)\n",
    "    \n",
    "    return classic_tensor, kc_tensor, unwarped#dml_tensor, base_tensor, flow_tensor, pwc_tensor, unwarped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/share/data_ultraschall/compressions\"\n",
    "#ids = [157, 384, 717, 209, 106, 4808, 977, 1097, 1150, 1314, 58, 115, 2709, 2713,4814, 2199, 545, 216, 610, 341, 526, 12, 124, 1778, 195, 379, 327, 384, 2033, 797]\n",
    "ids = [3485, 2232, 3455, 5687, 3360, 3672, 3357, 3628, 3538, 3352, 3586, 3433, 3644, 3387, 3663,3424, 2219, 3649, 3335, 3397, 3683]\n",
    "\n",
    "overall_classic_dice = []\n",
    "overall_kc_dice = []\n",
    "overall_dml_dice = []\n",
    "\n",
    "overall_seq_classic_dice = []\n",
    "overall_seq_kc_dice = []\n",
    "overall_seq_dml_dice = []\n",
    "\n",
    "overall_base_dice = []\n",
    "overall_flow_dice = []\n",
    "overall_pwc_dice = []\n",
    "overall_unwarped = []\n",
    "\n",
    "for prob_id in ids:\n",
    "    print(prob_id)\n",
    "    frame_path = os.path.join(path_to_data,str(prob_id),'frames')\n",
    "    seg_path = os.path.join(path_to_data,str(prob_id),'segmentations','1')\n",
    "\n",
    "    frame_list = []\n",
    "    for frame in os.listdir(frame_path):\n",
    "        frame_list.append(os.path.join(frame_path,frame))\n",
    "    frame_list.sort()\n",
    "\n",
    "    seg_list = []\n",
    "    for seg in os.listdir(seg_path):\n",
    "        seg_list.append(os.path.join(seg_path,seg))\n",
    "    seg_list.sort()\n",
    "\n",
    "    assert len(frame_list) == len(seg_list)\n",
    "    frames = torch.zeros([len(frame_list), 150,150])\n",
    "    segs = torch.zeros([len(frame_list), 150,150])\n",
    "    # read images\n",
    "    for i in range(len(frame_list)):\n",
    "        frames[i] = torch.from_numpy(np.array(Image.open(frame_list[i]))).float()\n",
    "        segs[i] = torch.from_numpy(np.array(Image.open(seg_list[i])))\n",
    "    \n",
    "    # normalize if needed\n",
    "    if segs.max() > 3:\n",
    "        segs = segs / 100    \n",
    "    if frames.max() > 2:\n",
    "         frames = frames / 255\n",
    "\n",
    "    landmarks = pd.read_csv('landmarks.csv')\n",
    "    landmarks = landmarks[landmarks['Id'].isin(ids)]\n",
    "    if prob_id in landmarks.Id.to_numpy():\n",
    "        start_frame = landmarks[landmarks['Id']== prob_id]['Start Frames'].iat[0]\n",
    "        #print(start_frame)\n",
    "        if isinstance(start_frame, str):\n",
    "            last_segment_available = np.fromstring(start_frame.strip(']['), sep=',', dtype=int)\n",
    "            \n",
    "        else:\n",
    "            last_segment_available = start_frame\n",
    "    else:\n",
    "        last_segment_available = 3\n",
    "    if segs.max().item() == 0.:\n",
    "        print('id not good: ', prob_id)\n",
    "        continue\n",
    "        \n",
    "    #classic_dice, kc_dice, dml_dice, seq_classic_dice, seq_kc_dice, seq_dml_dice , base_dice, flow_dice, pwc_dice, unwarped\n",
    "    c_d, kc_d,un = eval_id_moving(frames,segs, last_segment_available)\n",
    "    \n",
    "    overall_classic_dice.append(c_d)\n",
    "    overall_kc_dice.append(kc_d)\n",
    "    #overall_dml_dice.append(dml_d)\n",
    "    \n",
    "    #overall_base_dice.append(base)\n",
    "    #overall_flow_dice.append(f_d)\n",
    "    #overall_pwc_dice.append(pwc_d)\n",
    "    overall_unwarped.append(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb94171",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_means = torch.zeros((len(ids), 2))\n",
    "kc_means = torch.zeros((len(ids), 2))\n",
    "dml_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "base_means = torch.zeros((len(ids), 2))\n",
    "flow_means = torch.zeros((len(ids), 2))\n",
    "pwc_means = torch.zeros((len(ids), 2))\n",
    "un_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "\n",
    "\n",
    "for idx, dice in enumerate(overall_classic_dice):\n",
    "    \n",
    "    # MEANS\n",
    "    c_means[idx] = torch.Tensor([overall_classic_dice[idx][:,0].mean(),overall_classic_dice[idx][:,1].mean()])\n",
    "    kc_means[idx] = torch.Tensor([overall_kc_dice[idx][:,0].mean(),overall_kc_dice[idx][:,1].mean()])\n",
    "    #dml_means[idx] = torch.Tensor([overall_dml_dice[idx][:,0].mean(),overall_dml_dice[idx][:,1].mean()])\n",
    "    \n",
    "    #base_means[idx] = torch.Tensor([overall_base_dice[idx][:,0].mean(),overall_base_dice[idx][:,1].mean()])\n",
    "    #flow_means[idx] = torch.Tensor([overall_flow_dice[idx][:,0].mean(),overall_flow_dice[idx][:,1].mean()])\n",
    "    #pwc_means[idx] = torch.Tensor([overall_pwc_dice[idx][:,0].mean(),overall_pwc_dice[idx][:,1].mean()])\n",
    "    un_means[idx] = torch.Tensor([overall_unwarped[idx][:,0].mean(),overall_unwarped[idx][:,1].mean()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72badcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unwapred:\")\n",
    "print(\"Vein: \", round(un_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(un_means[:,0].var().item()*100,3))\n",
    "print(\"Artery: \", round(un_means[:,1].mean().item()*100,3),\" $\\pm$ \", round(un_means[:,1].var().item()*100,3))\n",
    "print(\"Overall: \", round(un_means.mean().item()*100,3),\" $\\pm$ \", round(un_means.var().item()*100,3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ecb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classic:\")\n",
    "print(\"Vein: \", round(c_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(c_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(c_means.mean().item()*100,2),\" $\\pm$ \", round(c_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"KC:\")\n",
    "print(\"Vein: \", round(kc_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(kc_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(kc_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(kc_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(kc_means.mean().item()*100,2),\" $\\pm$ \", round(kc_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"DML:\")\n",
    "print(\"Vein: \", round(dml_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(dml_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(dml_means.mean().item()*100,2),\" $\\pm$ \", round(dml_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flow2:\")\n",
    "print(\"Vein: \", round(flow_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(flow_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(flow_means[:,1].mean().item()*100,2), \" $\\pm$ \",round(flow_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(flow_means.mean().item()*100,2),\" $\\pm$ \", round(flow_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"PWC:\")\n",
    "print(\"Vein: \", round(pwc_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(pwc_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(pwc_means.mean().item()*100,2),\" $\\pm$ \", round(pwc_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"Dual:\")\n",
    "print(\"Vein: \", round(base_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(base_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(base_means.mean().item()*100,2),\" $\\pm$ \", round(base_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fc86a",
   "metadata": {},
   "source": [
    "# For completenes sake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6863bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_id_all(frames, segs, last_segment_available):\n",
    "    \"\"\"\n",
    "    run evaluation for one specific ID using all segmentations\n",
    "\n",
    "    frames; segs: torch.tensor [X x 1 x 150 x 150]\n",
    "    last_segment_available: int that indicates the theoretically last available segmantation\n",
    "    \"\"\"\n",
    "    distance_between_frames = 6\n",
    "\n",
    "\n",
    "    classic_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    classic_hd = []\n",
    "    kc_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    kc_hd = []\n",
    "    #dml_dice = []#torch.zeros(frames.shape[0],2)\n",
    "   \n",
    "    #base_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    #flow_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    #pwc_dice = []#torch.zeros(frames.shape[0],2)\n",
    "    unwarped = []#torch.zeros(frames.shape[0],2)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "\n",
    "        # skipp first X frames\n",
    "        if i < distance_between_frames:\n",
    "            continue\n",
    "            \n",
    "        fixed = torch.clone(frames[i- distance_between_frames])\n",
    "        fixed_seg = torch.clone(segs[i-distance_between_frames])\n",
    "            \n",
    "        moving = torch.clone(frames[i])\n",
    "        moving_seg = torch.clone(segs[i])\n",
    "\n",
    "                # not segmentation available in this seg\n",
    "        if moving_seg.max().item() == 0.:\n",
    "            continue\n",
    "\n",
    "        # LABELLOSS\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_classic, fixed, moving, fixed_seg, moving_seg)\n",
    "        #classic_segs[i] = pdd_seg\n",
    "        classic_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        classic_hd.append(hausdorff_dist(moving_seg.unsqueeze(0), pdd_seg.unsqueeze(0), 3))\n",
    "\n",
    "        # KC\n",
    "        pdd_seg, run_time_pdd = pdd_warp(model_kc, fixed, moving, fixed_seg, moving_seg)\n",
    "        #kc_segs[i] = pdd_seg\n",
    "        kc_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        kc_hd.append(hausdorff_dist(moving_seg.unsqueeze(0), pdd_seg.unsqueeze(0), 3))\n",
    "\n",
    "        # DML\n",
    "        #pdd_seg, run_time_pdd = pdd_warp(model_dml, fixed, moving, fixed_seg, moving_seg)\n",
    "        #dml_segs[i] = pdd_seg\n",
    "        #dml_dice.append(dice_coeff(moving_seg, pdd_seg, 3))\n",
    "        \n",
    "        #base_d, run_time_base = eval_baseline(fixed.cpu(), moving.cpu(), fixed_seg, moving_seg)\n",
    "        #base_dice.append(base_d)\n",
    "        \n",
    "        #warped_seg, _ = flownet_warp(fixed.unsqueeze(0).unsqueeze(0), moving.unsqueeze(0).unsqueeze(0), fixed_seg, moving_seg)\n",
    "        #flow_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "\n",
    "        # PWC\n",
    "        #warped_seg, run_time = pwc_warp(fixed.unsqueeze(0).unsqueeze(0),moving.unsqueeze(0).unsqueeze(0),fixed_seg,moving_seg)\n",
    "        #pwc_dice.append(dice_coeff(moving_seg.cpu(), warped_seg, 3).cpu())\n",
    "        \n",
    "        unwarped.append(dice_coeff(fixed_seg,moving_seg,3))\n",
    "        \n",
    "    \n",
    "    classic_tensor = torch.stack(classic_dice)\n",
    "    kc_tensor = torch.stack(kc_dice)\n",
    "    c_hd = torch.stack(classic_hd)\n",
    "    kc_hd = torch.stack(kc_hd)\n",
    "    \n",
    "    \n",
    "    #dml_tensor = torch.stack(dml_dice)\n",
    "    #base_tensor = torch.stack(base_dice)\n",
    "    #flow_tensor = torch.stack(flow_dice)\n",
    "    #pwc_tensor = torch.stack(pwc_dice)\n",
    "    #unwarped = torch.stack(unwarped)\n",
    "    \n",
    "    return classic_tensor,c_hd, kc_tensor, kc_hd#, dml_tensor, base_tensor, flow_tensor, pwc_tensor, unwarped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/share/data_ultraschall/compressions\"\n",
    "#ids = [157, 384, 717, 209, 106, 4808, 977, 1097, 1150, 1314, 58, 115, 2709, 2713,4814, 2199, 545, 216, 610, 341, 526, 12, 124, 1778, 195, 379, 327, 384, 2033, 797]\n",
    "ids = [3485, 2232, 3455, 5687, 3360, 3672, 3357, 3628, 3538, 3352, 3586, 3433, 3644, 3387, 3663,3424, 2219, 3649, 3335, 3397, 3683]\n",
    "\n",
    "\n",
    "overall_classic_dice = []\n",
    "overall_c_hd = []\n",
    "overall_kc_dice = []\n",
    "overall_kc_hd = []\n",
    "overall_dml_dice = []\n",
    "\n",
    "overall_seq_classic_dice = []\n",
    "overall_seq_kc_dice = []\n",
    "overall_seq_dml_dice = []\n",
    "\n",
    "overall_base_dice = []\n",
    "overall_flow_dice = []\n",
    "overall_pwc_dice = []\n",
    "overall_unwarped = []\n",
    "\n",
    "for prob_id in ids:\n",
    "    \n",
    "    print(prob_id)\n",
    "    frame_path = os.path.join(path_to_data,str(prob_id),'frames')\n",
    "    seg_path = os.path.join(path_to_data,str(prob_id),'segmentations','1')\n",
    "\n",
    "    frame_list = []\n",
    "    for frame in os.listdir(frame_path):\n",
    "        frame_list.append(os.path.join(frame_path,frame))\n",
    "    frame_list.sort()\n",
    "\n",
    "    seg_list = []\n",
    "    for seg in os.listdir(seg_path):\n",
    "        seg_list.append(os.path.join(seg_path,seg))\n",
    "    seg_list.sort()\n",
    "\n",
    "    assert len(frame_list) == len(seg_list)\n",
    "    frames = torch.zeros([len(frame_list), 150,150])\n",
    "    segs = torch.zeros([len(frame_list), 150,150])\n",
    "    # read images\n",
    "    for i in range(len(frame_list)):\n",
    "        frames[i] = torch.from_numpy(np.array(Image.open(frame_list[i]))).float()\n",
    "        segs[i] = torch.from_numpy(np.array(Image.open(seg_list[i])))\n",
    "    \n",
    "    # normalize if needed\n",
    "    if segs.max() > 3:\n",
    "        segs = segs / 100    \n",
    "    if frames.max() > 2:\n",
    "         frames = frames / 255\n",
    "\n",
    "    landmarks = pd.read_csv('landmarks.csv')\n",
    "    landmarks = landmarks[landmarks['Id'].isin(ids)]\n",
    "    if prob_id in landmarks.Id.to_numpy():\n",
    "        start_frame = landmarks[landmarks['Id']== prob_id]['Start Frames'].iat[0]\n",
    "        #print(start_frame)\n",
    "        if isinstance(start_frame, str):\n",
    "            last_segment_available = np.fromstring(start_frame.strip(']['), sep=',', dtype=int)\n",
    "            \n",
    "        else:\n",
    "            last_segment_available = start_frame\n",
    "    else:\n",
    "        last_segment_available = 3\n",
    "    if segs.max().item() == 0.:\n",
    "        print('id not good: ', prob_id)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    #classic_dice, kc_dice, dml_dice, seq_classic_dice, seq_kc_dice, seq_dml_dice , base_dice, flow_dice, pwc_dice, unwarped\n",
    "    #c_d, kc_d,un = eval_id_all(frames,segs, last_segment_available)\n",
    "    c_d, c_hd, kc_d, kc_hd = eval_id_all(frames,segs, last_segment_available)\n",
    "    \n",
    "    overall_classic_dice.append(c_d)\n",
    "    overall_c_hd.append(torch.from_numpy(np.nan_to_num(c_hd.numpy(), posinf=np.nan)))\n",
    "    \n",
    "    overall_kc_dice.append(kc_d)\n",
    "    overall_kc_hd.append(torch.from_numpy(np.nan_to_num(kc_hd.numpy(), posinf=np.nan)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4eef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_means = torch.zeros((len(ids), 2))\n",
    "c_hds = torch.zeros((len(ids), 2))\n",
    "kc_means = torch.zeros((len(ids), 2))\n",
    "kc_hds = torch.zeros((len(ids), 2))\n",
    "\n",
    "dml_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "\n",
    "base_means = torch.zeros((len(ids), 2))\n",
    "flow_means = torch.zeros((len(ids), 2))\n",
    "pwc_means = torch.zeros((len(ids), 2))\n",
    "un_means = torch.zeros((len(ids), 2))\n",
    "\n",
    "\n",
    "for idx, dice in enumerate(overall_classic_dice):\n",
    "    \n",
    "    # MEANS DICE\n",
    "    c_means[idx] = torch.Tensor([overall_classic_dice[idx][:,0].mean(),overall_classic_dice[idx][:,1].mean()])\n",
    "    kc_means[idx] = torch.Tensor([overall_kc_dice[idx][:,0].mean(),overall_kc_dice[idx][:,1].mean()])\n",
    "    #dml_means[idx] = torch.Tensor([overall_dml_dice[idx][:,0].mean(),overall_dml_dice[idx][:,1].mean()])\n",
    "    \n",
    "    #base_means[idx] = torch.Tensor([overall_base_dice[idx][:,0].mean(),overall_base_dice[idx][:,1].mean()])\n",
    "    #flow_means[idx] = torch.Tensor([overall_flow_dice[idx][:,0].mean(),overall_flow_dice[idx][:,1].mean()])\n",
    "    #pwc_means[idx] = torch.Tensor([overall_pwc_dice[idx][:,0].mean(),overall_pwc_dice[idx][:,1].mean()])\n",
    "    #un_means[idx] = torch.Tensor([overall_unwarped[idx][:,0].mean(),overall_unwarped[idx][:,1].mean()])\n",
    "    \n",
    "    # MEANS HD\n",
    "    c_hds[idx] = torch.Tensor([np.nanmean(overall_c_hd[idx][:,0]),np.nanmean(overall_c_hd[idx][:,1])])\n",
    "    kc_hds[idx] = torch.Tensor([np.nanmean(overall_kc_hd[idx][:,0]),np.nanmean(overall_kc_hd[idx][:,1])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unwapred:\")\n",
    "print(\"Vein: \", round(un_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(un_means[:,0].var().item()*100,3))\n",
    "print(\"Artery: \", round(un_means[:,1].mean().item()*100,3),\" $\\pm$ \", round(un_means[:,1].var().item()*100,3))\n",
    "print(\"Overall: \", round(un_means.mean().item()*100,3),\" $\\pm$ \", round(un_means.var().item()*100,3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8ea90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Classic:\")\n",
    "print(\"Vein: \", round(c_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,0].var().item()*100,2), ' ; ', round(c_hds[:,0].mean().item(),2),\" $\\pm$ \", round(c_hds[:,0].var().item(),2))\n",
    "print(\"Artery: \", round(c_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(c_means[:,1].var().item()*100,2), ' ; ', round(c_hds[:,1].mean().item(),2),\" $\\pm$ \", round(c_hds[:,1].var().item(),2))\n",
    "print(\"Overall: \", round(c_means.mean().item()*100,2),\" $\\pm$ \", round(c_means.var().item()*100,2), ' ; ', round(c_hds.mean().item(),2),\" $\\pm$ \", round(c_hds.var().item(),2))\n",
    "print()\n",
    "\n",
    "print(\"KC:\")\n",
    "print(\"Vein: \", round(kc_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(kc_means[:,0].var().item()*100,2), ' ; ', round(kc_hds[:,0].mean().item(),2),\" $\\pm$ \", round(kc_hds[:,0].var().item(),2))\n",
    "print(\"Artery: \", round(kc_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(kc_means[:,1].var().item()*100,2), ' ; ', round(kc_hds[:,1].mean().item(),2),\" $\\pm$ \", round(kc_hds[:,1].var().item(),2))\n",
    "print(\"Overall: \", round(kc_means.mean().item()*100,2),\" $\\pm$ \", round(kc_means.var().item()*100,2), ' ; ', round(kc_hds.mean().item(),2),\" $\\pm$ \", round(kc_hds.var().item(),2))\n",
    "print()\n",
    "\n",
    "print(\"DML:\")\n",
    "print(\"Vein: \", round(dml_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(dml_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(dml_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(dml_means.mean().item()*100,2),\" $\\pm$ \", round(dml_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flow2:\")\n",
    "print(\"Vein: \", round(flow_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(flow_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(flow_means[:,1].mean().item()*100,2), \" $\\pm$ \",round(flow_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(flow_means.mean().item()*100,2),\" $\\pm$ \", round(flow_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"PWC:\")\n",
    "print(\"Vein: \", round(pwc_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(pwc_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(pwc_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(pwc_means.mean().item()*100,2),\" $\\pm$ \", round(pwc_means.var().item()*100,2))\n",
    "print()\n",
    "\n",
    "print(\"Dual:\")\n",
    "print(\"Vein: \", round(base_means[:,0].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,0].var().item()*100,2))\n",
    "print(\"Artery: \", round(base_means[:,1].mean().item()*100,2),\" $\\pm$ \", round(base_means[:,1].var().item()*100,2))\n",
    "print(\"Overall: \", round(base_means.mean().item()*100,2),\" $\\pm$ \", round(base_means.var().item()*100,2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82bea3",
   "metadata": {},
   "source": [
    "# Test this against the nnU-Net predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = [3485, 2232, 4333, 3753, 3455, 5687, 3360, 3672, 3357, 2098, 4242, 3933, 3628, 3538, 5209, 4160, 3352, 3586, 3433, 3644, 3387, 3663,3424, 2219, 3649, 3335, 4186, 3397, 3683]\n",
    "\n",
    "ids = {2098: 0,\n",
    "3424: 1,\n",
    "5687: 2,\n",
    "3649: 3,\n",
    "2219: 4,\n",
    "3360: 5,\n",
    "3352: 6,\n",
    "3357: 7,\n",
    "3397: 8,\n",
    "3455: 9,\n",
    "1703: 10,\n",
    "3387: 11,\n",
    "3485: 12,\n",
    "3433: 13,\n",
    "3335: 14,\n",
    "3663: 15,\n",
    "3538: 16,\n",
    "3644: 17,\n",
    "3628: 18,\n",
    "3586: 19,\n",
    "3683: 20,\n",
    "3672: 21,\n",
    "2232: 22}\n",
    "pred_path = '/share/data_ultraschall/result_segmentations_nnunet2d/results'\n",
    "path_to_data = \"/share/data_ultraschall/compressions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.load(open(os.path.join(pred_path,'summary.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = results_dict['results']['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = []\n",
    "\n",
    "for r in result_list:\n",
    "    dice.append([r['1']['Dice'],r['2']['Dice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = np.array(dice)\n",
    "print(f\"{round(dice.mean(axis=0)[0] * 100,2)} $\\pm$ {round(dice.var(axis=0)[0] * 100,2)} & {round(dice.mean(axis=0)[1] * 100,2)} $\\pm$ {round(dice.var(axis=0)[1] * 100,2)} & {round(dice.mean() * 100,2)} $\\pm$ {round(dice.var() * 100,2)} \") \n",
    "     # &  {round(dice.mean(axis=0)[1] * 100,2)} $\\pm$ {round(dice.var(axis=0)[1] * 100,2)} \n",
    "     # & {round(dice.mean(axis=0) * 100,2)} $\\pm$ {round(dice.var(axis=0) * 100,2)}\n",
    "     # \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = '/share/data_ultraschall/result_segmentations_nnunet2d/results'\n",
    "path_to_data = \"/share/data_ultraschall/compressions\"\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "prediction_list = os.listdir(pred_path)\n",
    "prediction_list.sort()\n",
    "#print(len(prediction_list))\n",
    "\n",
    "nii_files = []\n",
    "for idx,f in enumerate(prediction_list):\n",
    "    if '.nii.gz' in f:\n",
    "        nii_files.append(f)\n",
    "nii_files.sort()\n",
    "print(nii_files)\n",
    "#for f in os.listdir(pred_path):\n",
    "#    if '.nii.gz' in f:\n",
    "#        example = nib.load(os.path.join(pred_path,f)).get_fdata()\n",
    "#        for prob_id in ids:\n",
    "#            examp_path = os.path.join(path_to_data, str(prob_id), 'segmentations', '1')\n",
    "#            if example.shape[0] == len(os.listdir(examp_path)):\n",
    "#                mapping[f] = str(prob_id)\n",
    "                #print(f, prob_id, example.shape, len(os.listdir(examp_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDs = []\n",
    "Dice = []\n",
    "\n",
    "for idx, key in enumerate(ids.keys()):\n",
    "    pred = torch.from_numpy(nib.load(os.path.join(pred_path,nii_files[idx])).get_fdata())\n",
    "    gt = torch.zeros(pred.shape)\n",
    "    gt_path = os.path.join(path_to_data, str(key), 'segmentations', '1')\n",
    "    gt_files = os.listdir(gt_path)\n",
    "    gt_files.sort()\n",
    "    for idx, f in enumerate(gt_files):\n",
    "        gt[idx] = torch.from_numpy(np.array(Image.open(os.path.join(gt_path,f)))) / 100\n",
    "    \n",
    "    hd_tmp = []\n",
    "    dice_tmp = []\n",
    "    for i in range(gt.shape[0]):\n",
    "        hd_tmp.append(hausdorff_dist(pred[i].unsqueeze(0), gt[i].unsqueeze(0), 3).numpy())\n",
    "        dice_tmp.append(dice_coeff(pred[i], gt[i], 3).numpy())\n",
    "    \n",
    "    HDs.append(np.array(hd_tmp))    \n",
    "    Dice.append(np.array(dice_tmp))\n",
    "\n",
    "HDs = np.array(HDs)\n",
    "Dice = np.array(Dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_d = []\n",
    "mean_hd = []\n",
    "for j in range(HDs.shape[0]):\n",
    "    mean_d.append(np.array([Dice[j][:,0].mean(), Dice[j][:,1].mean()]))\n",
    "    mean_hd.append(np.array([np.nanmean(np.nan_to_num(HDs[j][:,0], posinf=np.nan)), np.nanmean(np.nan_to_num(HDs[j][:,1], posinf=np.nan))]))\n",
    "    \n",
    "mean_d = np.array(mean_d) \n",
    "mean_hd = np.array(mean_hd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13843f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_d[:,0].mean(),mean_d[:,0].var() ,mean_d[:,1].mean(), mean_d[:,1].var(), mean_d.mean(), mean_d.var())\n",
    "print(np.nanmean(mean_hd[:,0]),np.nanvar(mean_hd[:,0]) ,mean_hd[:,1].mean(), mean_hd[:,1].var(), np.nanmean(mean_hd), np.nanvar(mean_hd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bf7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
