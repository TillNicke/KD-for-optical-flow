{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd52033",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Here we will examine the TV-L1 baseline and also see how the FlowNet2 performs on US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp, warpImage\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.optflow.DualTVL1OpticalFlow_create(outerIterations=5, )#flowNet = load_flownet2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81580d1",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b95434",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames.pth')\n",
    "segs = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random set for testing\n",
    "torch.manual_seed(42)\n",
    "random_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.1), replace=False))\n",
    "random_set\n",
    "\n",
    "rndidx = random_set[0]\n",
    "fixed = imgs[rndidx:rndidx+1, 0,:,:]\n",
    "moving = imgs[rndidx:rndidx+1, 1,:,:]\n",
    "\n",
    "fixed_seg = segs[rndidx:rndidx+1, 0,:,:]\n",
    "moving_seg = segs[rndidx:rndidx+1, 1,:,:]\n",
    "\n",
    "# we need to check the max and min, so we can be sure to get the two labels, if there are some\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 2:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 2:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "overlay_fixed = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),True)\n",
    "overlay_moving = overlaySegment(moving.squeeze(),moving_seg.data.squeeze(),False)\n",
    "plt.imshow(overlay_moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coeff(fixed_seg, moving_seg, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4cfaf",
   "metadata": {},
   "source": [
    "# TV-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "H,W = fixed_seg.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82346fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and visualize the flow\n",
    "\n",
    "in1 = fixed.view(H,W,1).numpy().astype(np.float32)\n",
    "in2 = moving.view(H,W,1).numpy().astype(np.float32)\n",
    "flow = baseline.calc(in1,in2,None)\n",
    "\n",
    "torch_flow = torch.from_numpy(flow).unsqueeze(0)\n",
    "torch_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = showFlow(torch_flow.cpu().permute(0,3,1,2).flip(1))\n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99750ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warping method found in PWC-Net code\n",
    "# \\models\\pwc-net\\all_in_one_pwc.py\n",
    "\n",
    "B, C, H, W = torch_flow.permute(0,3,1,2).size()\n",
    "# mesh grid\n",
    "xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "vgrid = grid + torch_flow.permute(0,3,1,2)\n",
    "\n",
    "# scale grid to [-1,1]\n",
    "vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "warped_seg_grid = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "plt.imshow(warped_seg_grid.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(fixed_seg, warped_seg_grid, 3)\n",
    "d1 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "print(d0, d0.mean())\n",
    "print(d1, d1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_seg = overlaySegment(fixed.squeeze(),warped_seg_grid.data.squeeze(),True)\n",
    "overlayed_seg = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e3b23",
   "metadata": {},
   "source": [
    "# Flownet 2\n",
    "warping and flow viz testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowNet.eval()\n",
    "# flownet2 and PWC-Net need specific input sizes\n",
    "teacher_fixed = F.interpolate(fixed.unsqueeze(0), size=(128,128))\n",
    "teacher_moving = F.interpolate(moving.unsqueeze(0), size=(128,128))\n",
    "# Generate the teacher flow estimation\n",
    "flow_in = preprocessing_flownet(teacher_fixed.detach().clone().reshape(128,128,1),teacher_moving.detach().clone().reshape(128,128,1))\n",
    "teacher_flow = flowNet(flow_in).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_flow = F.interpolate(teacher_flow, size=(H,W), mode='bilinear')\n",
    "teacher_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = showFlow(teacher_flow.cpu().flip(1))\n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e90371",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = teacher_flow.size()\n",
    "# mesh grid\n",
    "xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "vgrid = grid + teacher_flow\n",
    "\n",
    "# scale grid to [-1,1]\n",
    "vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "flwonet_warped = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "plt.imshow(flwonet_warped.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2965b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(fixed_seg, flwonet_warped, 3)\n",
    "d1 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "print(d0, d0.mean())\n",
    "print(d1, d1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_seg = overlaySegment(fixed.squeeze(),flwonet_warped.data.squeeze(),True)\n",
    "overlayed_seg = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c9a2f",
   "metadata": {},
   "source": [
    "# PDD\n",
    "\n",
    "somes tests on how to warp pdd-Net segs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd755e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (150,150)\n",
    "o_m = H//2\n",
    "o_n = W//2\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2)\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 15#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2)\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "\n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "\n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29040b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = OBELISK2d(24)\n",
    "pdd.load_state_dict(torch.load(\"models/obel_solo_24.pth\"))\n",
    "pdd.eval()\n",
    "displace_range= 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fixed_feat = pdd(fixed.unsqueeze(0).float())\n",
    "    moving_feat = pdd(moving.unsqueeze(0).float())\n",
    "\n",
    "ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "#regularise using meanfield inference with approx. min-convolutions\n",
    "soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//2, W//2)\n",
    "#upsample field to original resolution\n",
    "dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5886c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply and evaluate transformation\n",
    "\n",
    "# gridsampling with flow and identity\n",
    "identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "warped_student_seg_1 = F.grid_sample(moving_seg.float().unsqueeze(1),identity+dense_flow_fit.flip(1).permute(0,2,3,1),mode='nearest',align_corners=False)\n",
    "\n",
    "\n",
    "# vs warping method from PWC-Net\n",
    "B, C, H, W = dense_flow_fit.size()\n",
    "# mesh grid\n",
    "xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "grid = torch.cat((xx, yy), 1).float()\n",
    "\n",
    "vgrid = grid + dense_flow_fit\n",
    "\n",
    "# scale grid to [-1,1]\n",
    "vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "warped_student_seg_2 = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "plt.imshow(warped_student_seg_1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ad52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(warped_student_seg_2.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b47b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = showFlow(dense_flow_fit.cpu())\n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f621c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(fixed_seg, warped_student_seg_1, 3)\n",
    "d1 = dice_coeff(fixed_seg, warped_student_seg_2, 3)\n",
    "d2 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "\n",
    "print(d0, d0.mean())\n",
    "print(d1, d1.mean())\n",
    "print(d2, d2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a36566",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_seg = overlaySegment(fixed.squeeze(),warped_student_seg_1.data.squeeze(),True)\n",
    "overlayed_seg = overlaySegment(fixed.squeeze(),fixed_seg.data.squeeze(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b47758",
   "metadata": {},
   "source": [
    "# Experiment with FlowNet2 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_flownet_concat(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Preprocessing function for FlowNet2.\n",
    "\n",
    "    img1: numpy.ndarray in shape of (H, W, C)\n",
    "    img2: numpy.ndarray in shape of (H, W, C)\n",
    "\n",
    "    return: torch.tensor in the\tshape of (1, B, C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    img1 = img_1.numpy().copy()\n",
    "    img2 = img_2.numpy().copy()\n",
    "\n",
    "    # normalization\n",
    "    if img1.max() <= 1.0:\n",
    "        img1 = img1 * 255\n",
    "    if img2.max() <= 1.0:\n",
    "        img2 = img2 * 255\n",
    "\n",
    "    \n",
    "    # concatenation to generate RGB-Like input\n",
    "    if img1.shape[2] == 1:\n",
    "        img1 = np.concatenate([img1,img1,img1], 2)\n",
    "\n",
    "    if img2.shape[2] == 1:\n",
    "        img2 = np.concatenate([img2,img2,img2], 2)\n",
    "\n",
    "    images = [img1, img2]\n",
    "    images = np.array(images).transpose(3, 0, 1, 2)\n",
    "    return torch.from_numpy(images.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "def preprocessing_flownet_colorMap(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Preprocessing function for FlowNet2.\n",
    "\n",
    "    img1: torch.Tensor in shape of (H, W, C)\n",
    "    img2: torch.Tensor in shape of (H, W, C)\n",
    "\n",
    "    return: torch.tensor in the\tshape of (1, B, C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    img1 = img_1.numpy().copy()\n",
    "    img2 = img_2.numpy().copy()\n",
    "\n",
    "    if img1.max() <= 1.0:\n",
    "        img1 = img1 * 255\n",
    "    if img2.max() <= 1.0:\n",
    "        img2 = img2 * 255\n",
    "\n",
    "        \n",
    "    # generate RGB-like input through colormapping\n",
    "    img1 = cv2.applyColorMap(img1, cv2.COLORMAP_HSV)\n",
    "    img2 = cv2.applyColorMap(img2, cv2.COLORMAP_HSV)\n",
    "\n",
    "    images = [img1, img2]\n",
    "    images = np.array(images).transpose(3, 0, 1, 2)\n",
    "    return torch.from_numpy(images.astype(np.float32)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rndidx = random_set[0]\n",
    "fixed_orig = imgs[rndidx:rndidx+1, 0,:,:]\n",
    "moving_orig = imgs[rndidx:rndidx+1, 1,:,:]\n",
    "\n",
    "fixed_seg = segs[rndidx:rndidx+1, 0,:,:]\n",
    "moving_seg = segs[rndidx:rndidx+1, 1,:,:]\n",
    "\n",
    "# we need to check the max and min, so we can be sure to get the two labels, if there are some\n",
    "if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 2:\n",
    "    fixed_seg = fixed_seg*2\n",
    "if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 2:\n",
    "    moving_seg = moving_seg*2\n",
    "\n",
    "fixed = F.interpolate(fixed_orig.unsqueeze(0), size=(128,128))\n",
    "moving = F.interpolate(moving_orig.unsqueeze(0), size=(128,128))\n",
    "\n",
    "fixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f54ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compate the two RGB-Like images for optical flow estimation\n",
    "imgs_in_concat = preprocessing_flownet_concat(fixed.view(128,128,1),moving.view(128,128,1))\n",
    "print(imgs_in_concat.shape)\n",
    "imgs_in_maped = preprocessing_flownet_concat(fixed.view(128,128,1),moving.view(128,128,1))\n",
    "print(imgs_in_maped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_concat = flowNet(imgs_in_concat.cuda()).cpu().detach()\n",
    "flow_map = flowNet(imgs_in_maped.cuda()).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_concat = F.interpolate(flow_concat, size=(150,150))\n",
    "rgb = showFlow(flow_concat.flip(1))\n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_map = F.interpolate(flow_map, size=(150,150))\n",
    "rgb = showFlow(flow_map.flip(1))\n",
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_seg(moving_seg, flow):\n",
    "    \"\"\"\n",
    "    function to warp the segemntation of the teacher and baseline\n",
    "    \n",
    "    moving_seg: CxHxW\n",
    "    flow: size: BxCxHxW\n",
    "    \"\"\"\n",
    "    B, C, H, W = flow.size()\n",
    "    # mesh grid\n",
    "    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1).float().to(flow.device)\n",
    "    \n",
    "    vgrid = grid + flow\n",
    "\n",
    "    # scale grid to [-1,1]\n",
    "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "    warped_seg_grid = nn.functional.grid_sample(moving_seg.float().unsqueeze(0), vgrid)\n",
    "    return warped_seg_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44147a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_seg_warped = warp_seg(moving_seg,flow_concat)\n",
    "map_seg_warped = warp_seg(moving_seg,flow_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e151734",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_seg_warped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(map_seg_warped.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cccb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dice_coeff(fixed_seg.squeeze(), concat_seg_warped.squeeze(),3)\n",
    "d1 = dice_coeff(fixed_seg.squeeze(), map_seg_warped.squeeze(),3)\n",
    "d2 = dice_coeff(fixed_seg.squeeze(), moving_seg.squeeze(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43785f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2, d2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_seg = overlaySegment(fixed_orig.squeeze(),concat_seg_warped.data.squeeze(),True)\n",
    "overlayed_seg = overlaySegment(fixed_orig.squeeze(),map_seg_warped.data.squeeze(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072b57e",
   "metadata": {},
   "source": [
    "# Verdict\n",
    "color maps do not influence the FlowNet2 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34159929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
