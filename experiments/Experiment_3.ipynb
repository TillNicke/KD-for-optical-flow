{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4176db",
   "metadata": {},
   "source": [
    "# Third Experiment\n",
    "In this run, we investigate deep mutual learning \n",
    "\n",
    "We use 5 students and train them, like described [here](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf). So we use the direct label loss to evaluate the best performing network and then calculate the loss of the other students accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c635453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from math import ceil\n",
    "\n",
    "from utils.preprocessing import preprocessing_flownet, preprocessing_pwc\n",
    "from utils.load_models import load_flownet2, load_pwcnet, init_weights\n",
    "from utils.plotting import flow2img, overlaySegment, showFlow\n",
    "from utils.layers import warp, warpImage\n",
    "from utils.encoding import labelMatrixOneHot, dice_coeff\n",
    "\n",
    "from models.pdd_net.pdd_student import OBELISK2d\n",
    "\n",
    "mode = \"DML\"\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75012537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sheet(epochs, optim, loss,grad_accum,lr, mode, eval_mean_dice, unwarped_dice,var, notes=''):\n",
    "    \"\"\"\n",
    "    function to update a csv table to keep track of results\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df = df.append({\n",
    "        'epochs':epochs,\n",
    "        'optim':optim,\n",
    "        'batch size': grad_accum,\n",
    "        'learning rate': lr,\n",
    "        'loss':loss,\n",
    "        'mode': mode,\n",
    "        'Eval mean dice': eval_mean_dice,\n",
    "        'Var eval dice': var,\n",
    "        'unwarped dice': unwarped_dice,\n",
    "        'time': datetime.now().strftime(\"%d.%m.%y %H:%M\"),\n",
    "        'notes': notes\n",
    "    }, ignore_index=True)\n",
    "    df.to_csv('experiments.csv')\n",
    "    print('updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959924d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_frames_disp_6.pth')\n",
    "segs2 = torch.load('/share/data_ultraschall/nicke_ma/data/train_segs_disp_6.pth')\n",
    "\n",
    "imgs3 = torch.load('/share/data_ultraschall/nicke_ma/data/frames_oneFixed_multipleMoving.pth')\n",
    "segs3 = torch.load('/share/data_ultraschall/nicke_ma/data/segs_oneFixed_multipleMoving.pth')\n",
    "\n",
    "imgs = torch.cat((imgs2,imgs3))\n",
    "segs = torch.cat((segs2,segs3))\n",
    "#define a training split \n",
    "torch.manual_seed(42)\n",
    "# Now, we prepare our train & test dataset.\n",
    "train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "test_set = torch.arange(len(imgs))\n",
    "for idx in train_set:\n",
    "    test_set = test_set[test_set != idx]\n",
    "\n",
    "\n",
    "print(f\"{train_set.shape[0]} train examples\")\n",
    "print(f\"{test_set.shape[0]} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed26856",
   "metadata": {},
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356270c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (150,150)\n",
    "o_m = H//4 +1\n",
    "o_n = W//4 +1\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2).cuda()\n",
    "disp_range = 0.25#0.25\n",
    "displacement_width = 15#15#11#17\n",
    "shift_xy = F.affine_grid(disp_range*torch.eye(2,3).unsqueeze(0),(1,1,displacement_width,displacement_width)).view(1,1,-1,2).cuda()\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2).cuda()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "\n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e85c4e",
   "metadata": {},
   "source": [
    "# Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c44277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    overall_dice = []\n",
    "    unwarped_dice = []\n",
    "    \n",
    "    for i,idx in enumerate(test_set):\n",
    "        \n",
    "        fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[idx:idx+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[idx:idx+1,1,:].long().contiguous()\n",
    "        \n",
    "        # Some images have no segmentation to them, \n",
    "        # even if it was present in the directory\n",
    "        # We leave these ones out, as they cannot be avaluated\n",
    "        if len(torch.where(torch.histc(fixed_seg) != 0)[0]) == 3 and fixed_seg.max() <= 1:\n",
    "            fixed_seg = fixed_seg*2\n",
    "        if len(torch.where(torch.histc(moving_seg) != 0)[0]) == 3 and moving_seg.max() <= 1:\n",
    "            moving_seg = moving_seg*2\n",
    "        else:\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                fixed_feat = model(fixed.cuda())\n",
    "                moving_feat = model(moving.cuda())\n",
    "\n",
    "            ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "            #regularise using meanfield inference with approx. min-convolutions\n",
    "            soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "            #upsample field to original resolution\n",
    "            dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "\n",
    "            #apply and evaluate transformation\n",
    "            identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "            warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "            d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "            d2 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "            \n",
    "            overall_dice.append(d1.mean())\n",
    "            unwarped_dice.append(d2.mean())\n",
    "    \n",
    "    overall_dice = torch.from_numpy(np.array(overall_dice))\n",
    "    unwarped_dice = torch.from_numpy(np.array(unwarped_dice))\n",
    "    return overall_dice.mean(), unwarped_dice.mean()\n",
    "\n",
    "    #print(f\"This model has an average Dice of {round(overall_dice.mean().item(), 5)} mit Variance: {round(overall_dice.var().item(), 5)}. The unwarped Mean dice is: {round(unwarped_dice.mean().item(), 5)} with Var {round(unwarped_dice.var().item(),5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3b236",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.002\n",
    "# minibatch training\n",
    "grad_accum = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9c279",
   "metadata": {},
   "source": [
    "# Experiment 3.1\n",
    "Deep mutual learning with label loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5170d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 5\n",
    "students = []#torch.zeros(num_students, dtype=torch.nn.Module)#torch.zeros(num_students)\n",
    "optims = []#torch.zeros(num_students)\n",
    "\n",
    "for i in range(num_students):\n",
    "    student = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(32),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.PReLU(),\n",
    "                          torch.nn.Conv2d(64,16,kernel_size=1,stride=1,padding=0,dilation=1),\n",
    "                          torch.nn.Sigmoid())\n",
    "    init_weights(student)\n",
    "    student.train().cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)\n",
    "    \n",
    "    students.append(student)\n",
    "    optims.append(optimizer)\n",
    "#loss_array = torch.from_numpy(np.array([[0 for x in range(epochs)] for i in range(num_students)])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f61446",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.zeros(num_students,epochs)\n",
    "accs = torch.zeros(num_students, epochs)\n",
    "unwarped_dice = torch.zeros(num_students, epochs)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Cross Validation\n",
    "    train_set = torch.from_numpy(np.random.choice(np.arange(len(imgs)),size=int(len(imgs)*0.95), replace=False))\n",
    "\n",
    "    test_set = torch.arange(len(imgs))\n",
    "    for idx in train_set:\n",
    "        test_set = test_set[test_set != idx]\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # store the loss for all train image pairs\n",
    "    tmp_loss = torch.zeros(num_students, train_set.size(0))\n",
    "    \n",
    "    # show all examples to model\n",
    "    for i, rnd_idx in enumerate(rnd_train_idx):\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].long().contiguous()\n",
    "        \n",
    "        # Label preparation\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving = F.one_hot(moving_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving,size=(Hf//4 +1,Wf//4 +1),mode='bilinear')\n",
    "        label_fixed = F.one_hot(fixed_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4 +1,Wf//4 +1),mode='bilinear')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1).cuda()\n",
    "    \n",
    "        outputs = []\n",
    "        warped_labels = []\n",
    "        \n",
    "        fixed = Variable(fixed.cuda())\n",
    "        moving = Variable(moving.cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        for stud in students:\n",
    "            \n",
    "            feat_fixed = stud(fixed)\n",
    "            feat_moving = stud(moving)\n",
    "\n",
    "            ssd_distance = correlation_layer(displace_range, feat_moving, feat_fixed)\n",
    "            soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "            \n",
    "            # append the output\n",
    "            outputs.append([soft_cost, disp_xy])\n",
    "            \n",
    "            label_warped = torch.sum(soft_cost.t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "            \n",
    "            # store the warped prediction of the student\n",
    "            warped_labels.append(label_warped.cpu())\n",
    "        \n",
    "        # Backward pass\n",
    "        for idx, stud in enumerate(students):\n",
    "            \n",
    "            # calculate the individual label loss\n",
    "            label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-warped_labels[idx].reshape(3,-1),2),0)\n",
    "            \n",
    "            loss_between_students = 0\n",
    "            for run_idx in range(num_students):\n",
    "                if idx != run_idx:\n",
    "                    pred = Variable(warped_labels[run_idx])\n",
    "                    # cauculate the loss between students\n",
    "                    loss_between_students += torch.sum(torch.pow(label_distance1 - pred, 2),0)\n",
    "                    del(pred)\n",
    "            \n",
    "            diffloss = 1.5*((outputs[idx][1][0,:,1:,:]-outputs[idx][1][0,:,:-1,:])**2).mean()+\\\n",
    "                       1.5*((outputs[idx][1][0,1:,:,:]-outputs[idx][1][0,:-1,:,:])**2).mean()+\\\n",
    "                       1.5*((outputs[idx][1][0,:,:,1:]-outputs[idx][1][0,:,:,:-1])**2).mean()\n",
    "            \n",
    "            loss = diffloss + label_distance1.mean() + (loss_between_students.mean() / (num_students))\n",
    "            loss.backward()\n",
    "            tmp_loss[idx][i] += loss.item()\n",
    "\n",
    "        if (i+1)%grad_accum == 0:\n",
    "            for optim in optims:\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "          \n",
    "    for student_idx,individual_loss in enumerate(tmp_loss):  \n",
    "        losses[student_idx][epoch] = individual_loss.mean()\n",
    "    \n",
    "    #overall_dice = torch.zeros(num_students, epochs)\n",
    "    #unwarped_dice = torch.zeros(num_students, epochs)\n",
    "    for student_idx, s in enumerate(students):\n",
    "        all_dice = []\n",
    "        unwarped = []\n",
    "        for i,idx in enumerate(test_set):\n",
    "\n",
    "            fixed = imgs[idx:idx+1,0,:].unsqueeze(0).float()\n",
    "            moving = imgs[idx:idx+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "            fixed_seg = segs[idx:idx+1,0,:].contiguous() * 2\n",
    "            moving_seg = segs[idx:idx+1,1,:].contiguous() * 2\n",
    "\n",
    "            if moving_seg.max() <= 0.1 and fixed_seg.max() <= 0.1:\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fixed_feat = s(fixed.cuda())\n",
    "                moving_feat = s(moving.cuda())\n",
    "\n",
    "            ssd_distance = correlation_layer(displace_range, moving_feat, fixed_feat).contiguous()\n",
    "            #regularise using meanfield inference with approx. min-convolutions\n",
    "            soft_cost_one,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4 +1, W//4 +1)\n",
    "            #upsample field to original resolution\n",
    "            dense_flow_fit = F.interpolate(disp_xy,size=(H,W),mode='bicubic')\n",
    "\n",
    "\n",
    "            #apply and evaluate transformation\n",
    "            identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False).cuda()\n",
    "            warped_student_seg = F.grid_sample(moving_seg.cuda().float().unsqueeze(1),identity+dense_flow_fit.permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "            d1 = dice_coeff(fixed_seg,warped_student_seg.squeeze(),3)\n",
    "            d2 = dice_coeff(fixed_seg, moving_seg, 3)\n",
    "\n",
    "            #print(f\"{d1} VS unwarped {d2}; Means: {d1.mean()} VS unwarped {d2.mean()}\")\n",
    "            all_dice.append(d1.mean())\n",
    "            unwarped.append(d2.mean())\n",
    "\n",
    "        accs[student_idx][epoch]=torch.from_numpy(np.array(all_dice)).mean()\n",
    "        unwarped_dice[student_idx][epoch]=torch.from_numpy(np.array(unwarped)).mean()\n",
    "    \n",
    "    #print(accs)\n",
    "    #print(unwarped_dice)\n",
    "    #print(losses)\n",
    "    #break\n",
    "    #if (epoch+1)%10 ==0:\n",
    "    #    print(f\"Epoche: {epoch+1}\")\n",
    "    #    for i in range(num_studs):\n",
    "    #        print(f\"Loss of Student {i} : {loss_array[i][epoch-4:epoch].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%d_%m_%y-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "fig = plt.figure()\n",
    "for i in range(num_students):\n",
    "    plt.plot(losses[i], label=f'Student {i}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig(f\"plots/dml_loss_{now}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "fig = plt.figure()\n",
    "for i in range(num_students):\n",
    "    plt.plot(accs[i], label=f'Student {i}')\n",
    "plt.plot(unwarped_dice[i], label=f'unwarped Dice')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig(f'plots/dml_acc_{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27914af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mean_dice = [accs[i].mean().item() for i in range(len(accs))]\n",
    "print(eval_mean_dice)\n",
    "unwarped_mean_dice = [unwarped_dice[i].mean().item() for i in range(len(unwarped_dice))]\n",
    "eval_var = [accs[i].var().item() for i in range(len(accs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sheet(epochs, optims,\"Label + diff+ labeldiff between studs\",grad_accum,lr, mode, eval_mean_dice, unwarped_dice,eval_var, 'Dataset 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(f\"models/Experiment_3/{now}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f872364",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_students):\n",
    "    torch.save(students[i].state_dict(), f\"models/Experiment_3/{now}/student_16_{i}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 5\n",
    "students = []#torch.zeros(num_students, dtype=torch.nn.Module)#torch.zeros(num_students)\n",
    "optims = []#torch.zeros(num_students)\n",
    "\n",
    "for i in range(num_students):\n",
    "    student = OBELISK2d(24)\n",
    "    init_weights(student)\n",
    "    student.train().cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(student.parameters()),lr=lr)\n",
    "    \n",
    "    students.append(student)\n",
    "    optims.append(optimizer)\n",
    "    \n",
    "loss_array = torch.from_numpy(np.array([[0 for x in range(epochs)] for i in range(num_students)])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    # Shuffle training examples\n",
    "    rnd_train_idx = torch.randperm(train_set.size(0))\n",
    "\n",
    "    # show all examples to model\n",
    "    for rnd_idx in rnd_train_idx:\n",
    "        \n",
    "        p_fix = train_set[rnd_idx]\n",
    "\n",
    "        # Get image and segmentation\n",
    "        fixed = imgs[p_fix:p_fix+1,0,:].unsqueeze(0).float()\n",
    "        moving = imgs[p_fix:p_fix+1,1,:].unsqueeze(0).float()\n",
    "\n",
    "        fixed_seg = segs[p_fix:p_fix+1,0,:].long().contiguous()\n",
    "        moving_seg = segs[p_fix:p_fix+1,1,:].long().contiguous()\n",
    "        \n",
    "        # Label preparation\n",
    "        C1,Hf,Wf = moving_seg.size()\n",
    "        label_moving = F.one_hot(moving_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_moving = F.interpolate(label_moving,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        label_fixed = F.one_hot(fixed_seg,num_classes=3).permute(0,3,1,2).float()\n",
    "        label_fixed = F.interpolate(label_fixed,size=(Hf//4,Wf//4),mode='bilinear')\n",
    "        # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n",
    "        # according to the corresponding discrete displacement pair\n",
    "        label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,3,displace_range**2,-1).cuda()\n",
    "    \n",
    "        outputs = []\n",
    "        warped_labels = []\n",
    "        fixed = Variable(fixed.cuda())\n",
    "        moving = Variable(moving.cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        for stud in students:\n",
    "            \n",
    "            feat_fixed = stud(fixed)\n",
    "            feat_moving = stud(moving)\n",
    "\n",
    "            ssd_distance = correlation_layer(displace_range, feat_moving, feat_fixed)\n",
    "            soft_cost,disp_xy = meanfield(ssd_distance, fixed, displace_range, H//4, W//4)\n",
    "            \n",
    "            # append the output\n",
    "            outputs.append([soft_cost, disp_xy])\n",
    "            \n",
    "            label_warped = torch.sum(soft_cost.t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n",
    "            \n",
    "            # store the warped prediction of the student\n",
    "            warped_labels.append(label_warped.cpu())\n",
    "        \n",
    "        # Backward pass\n",
    "        for idx, stud in enumerate(students):\n",
    "            \n",
    "            # calculate the individual label loss\n",
    "            label_distance1 = torch.sum(torch.pow(label_fixed.reshape(3,-1)-warped_labels[idx].reshape(3,-1),2),0)\n",
    "            \n",
    "            loss_between_students = 0\n",
    "            for run_idx in range(num_students):\n",
    "                if idx != run_idx:\n",
    "                    pred = Variable(outputs[run_idx][1])\n",
    "                    # cauculate the loss between students\n",
    "                    loss_between_students += torch.sum(torch.pow(outputs[idx][1] - pred, 2))\n",
    "                    del(pred)\n",
    "            \n",
    "            loss = label_distance1.mean() + loss_between_students.mean() / (num_students)\n",
    "            loss.backward()\n",
    "            loss_array[idx][epoch] += loss.item()\n",
    "\n",
    "        if (epoch+1)%grad_accum == 0:\n",
    "            for optim in optims:\n",
    "                optim.step()\n",
    "                optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fada119",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(num_students):\n",
    "    plt.plot(loss_array[i])\n",
    "plt.savefig(\"dml_convergence_3_2_100Epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eda49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_students):\n",
    "    evaluate_model(students[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_students):\n",
    "    torch.save(students[i].state_dict(), f\"student_{i}_dml_3_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
