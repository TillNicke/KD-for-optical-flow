{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b218c69a",
   "metadata": {},
   "source": [
    "# Video generation for visualization\n",
    "used for inference of single videos and create .mp4 files for visual examination of the warped segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "from utils.plotting import showFlow, overlaySegment\n",
    "from utils.encoding import dice_coeff\n",
    "import time\n",
    "\n",
    "import os\n",
    "# Select a GPU for the work\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "available_gpus = [(torch.cuda.device(i),torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the overlay works\n",
    "def overlaySegment(gray1,seg1,flag=False):\n",
    "    H, W = seg1.squeeze().size()\n",
    "    colors=torch.FloatTensor([0,0,0,199,67,66,225,140,154,78,129,170,45,170,170,240,110,38,111,163,91,235,175,86,202,255,52,162,0,183]).view(-1,3)/255.0\n",
    "    segs1 = labelMatrixOneHot(seg1.unsqueeze(0),3)\n",
    "\n",
    "    seg_color = torch.mm(segs1.view(3,-1).t(),colors[:3,:]).view(H,W,3)\n",
    "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
    "\n",
    "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
    "    if(flag):\n",
    "        plt.imshow((overlay).numpy())\n",
    "        plt.show()\n",
    "    return overlay\n",
    "def labelMatrixOneHot(segmentation, label_num):\n",
    "    B, H, W = segmentation.size()\n",
    "    values = segmentation.view(B,1,H,W).expand(B,label_num,H,W).to(segmentation.device)\n",
    "    linspace = torch.linspace(0, label_num-1, label_num).long().view(1,label_num,1,1).expand(B,label_num,H,W).to(segmentation.device)\n",
    "    matrix = (values.float()==linspace.float()).float().to(segmentation.device)\n",
    "    for j in range(2,matrix.shape[1]):\n",
    "        matrix[0,j,:,:] = matrix[0,j,:,:]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10111ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (150,150)\n",
    "o_m = H//4 +1\n",
    "o_n = W//4 +1\n",
    "ogrid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,o_m,o_n)).view(1,1,-1,2)#.cuda()\n",
    "disp_range = 0.25#0.25\n",
    "disp_hw = 5\n",
    "displace_range = 11\n",
    "\n",
    "grid_size = 32#25#30\n",
    "grid_xy = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,grid_size,grid_size)).view(1,-1,1,2)#.cuda()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant(m.bias, 0.0)\n",
    "\n",
    "class OBELISK2d(nn.Module):\n",
    "    def __init__(self, chan = 16):\n",
    "\n",
    "        super(OBELISK2d, self).__init__()\n",
    "        channels = chan\n",
    "        self.offsets = nn.Parameter(torch.randn(2,channels *2,2) *0.05)\n",
    "        self.layer0 = nn.Conv2d(1, 4, 5, stride=2, bias=False, padding=2)\n",
    "        self.batch0 = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.layer1 = nn.Conv2d(channels *8, channels *4, 1, bias=False, groups=1)\n",
    "        self.batch1 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer2 = nn.Conv2d(channels *4, channels *4, 3, bias=False, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(channels *4)\n",
    "        self.layer3 = nn.Conv2d(channels *4, channels *1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img):\n",
    "        img_in = F.avg_pool2d(input_img ,3 ,padding=1 ,stride=2)\n",
    "        img_in = F.relu(self.batch0(self.layer0(img_in)))\n",
    "        sampled = F.grid_sample(img_in ,ogrid_xy + self.offsets[0 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "        sampled -= F.grid_sample(img_in ,ogrid_xy + self.offsets[1 ,:,:].view(1 ,-1 ,1 ,2)).view(1 ,-1 ,o_m ,o_n)\n",
    "\n",
    "        x = F.relu(self.batch1(self.layer1(sampled)))\n",
    "        x = F.relu(self.batch2(self.layer2(x)))\n",
    "        features = self.layer3(x)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def min_convolution(ssd_distance, displace_range, H, W):\n",
    "    # Prepare operators for smooth dense displacement space\n",
    "    pad1 = nn.ReplicationPad2d(5)\n",
    "    avg1 = nn.AvgPool2d(5,stride=1)\n",
    "    max1 = nn.MaxPool2d(3,stride=1)\n",
    "    pad2 = nn.ReplicationPad2d(6)\n",
    "    # approximate min convolution / displacement compatibility\n",
    "\n",
    "    ssd_minconv = avg1(avg1(-max1(-pad1(ssd_distance.permute(0,2,3,1).reshape(1,-1,displace_range,displace_range)))))\n",
    "\n",
    "    ssd_minconv = ssd_minconv.permute(0,2,3,1).view(1,-1,H,W)\n",
    "    min_conv_cost = avg1(avg1(avg1(pad2(ssd_minconv))))\n",
    "    \n",
    "    return min_conv_cost\n",
    "\n",
    "def meanfield(ssd_distance,img_fixed,displace_range,H,W):\n",
    "\n",
    "    crnt_dev = ssd_distance.device\n",
    "\n",
    "    cost = min_convolution(ssd_distance, displace_range, H, W)\n",
    "\n",
    "    soft_cost = F.softmax(-10*cost.view(displace_range**2,-1).t(),1)\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    disp_mesh_grid = disp_hw*F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,displace_range,displace_range),align_corners=True)\n",
    "    disp_mesh_grid /= torch.Tensor([(W-1)*.5,(H-1)*.5])\n",
    "\n",
    "    disp_xy = torch.sum(soft_cost.view(1,H,W,-1,1)*disp_mesh_grid.view(1,1,1,-1,2).to(crnt_dev),3).permute(0,3,1,2) \n",
    "    \n",
    "\n",
    "    return soft_cost,disp_xy\n",
    "\n",
    "def correlation_layer(displace_range, feat_moving, feat_fixed):\n",
    "    \n",
    "    disp_hw = (displace_range-1)//2\n",
    "    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n",
    "    B,C,H,W = feat_fixed.size()\n",
    "    \n",
    "    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n",
    "\n",
    "    return ssd_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2120ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_state_dict = \"models/Experiment_1/obel16_mix_100E_28_12_21-10-41.pth\"\n",
    "model_classic = OBELISK2d(16)\n",
    "model_classic.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_classic.eval()#.cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_2/obel16_ensemble_13_10_21-21-30.pth\"\n",
    "model_kc = OBELISK2d(16)\n",
    "model_kc.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_kc.eval()#.cuda()\n",
    "\n",
    "path_to_state_dict = \"models/Experiment_3/27_10_21-10-59/student_16_1.pth\"\n",
    "model_dml = OBELISK2d(16)\n",
    "model_dml.load_state_dict(torch.load(path_to_state_dict))\n",
    "model_dml.eval()#.cuda()\n",
    "\n",
    "model_soft = OBELISK2d(16)\n",
    "model_soft.load_state_dict(torch.load(\"models/Experiment_2/obel16_ensemble_soft_27_10_21-13-26.pth\"))\n",
    "model_soft.eval()#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdd_warp(model, fixed, moving, fixed_seg, moving_seg):\n",
    "    model.eval()\n",
    "    \n",
    "    #start = time.time()\n",
    "    feat1 = model(moving.unsqueeze(0).unsqueeze(0))#.cuda())\n",
    "    feat2 = model(fixed.unsqueeze(0).unsqueeze(0))#.cuda())\n",
    "    ssd_distance = correlation_layer(displace_range, feat2, feat1)\n",
    "    soft_cost,disp_xy = meanfield(ssd_distance, moving, displace_range, H//4 +1, W//4 +1)\n",
    "    # scaling\n",
    "    flow=F.interpolate(disp_xy,size=(150,150), mode='bicubic')\n",
    "    #end = time.time()\n",
    "    #run_time = round(end-start, 4)\n",
    "    \n",
    "    identity = F.affine_grid(torch.eye(2,3).unsqueeze(0),(1,1,H,W),align_corners=False)\n",
    "    warped_student_seg = F.grid_sample(fixed_seg.unsqueeze(0).unsqueeze(0).float(),identity+flow.cpu().permute(0,2,3,1),mode='nearest',align_corners=False).cpu()\n",
    "\n",
    "    return warped_student_seg, 200, flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/share/data_ultraschall/compressions\"\n",
    "ids = [157, 384, 717, 1097, 1150, 1314, 58, 115, 2709, 2713,4814, 2199,545, 216, 610,341,526,12,124,1778,195,379,327,384,2033,797]\n",
    "prob_id = 384\n",
    "#overall_classic_dice = []\n",
    "#overall_kc_dice = []\n",
    "#overall_dml_dice = []\n",
    "#overall_soft_dice = []\n",
    "\n",
    "print(prob_id)\n",
    "frame_path = os.path.join(path_to_data,str(prob_id),'frames')\n",
    "seg_path = os.path.join(path_to_data,str(prob_id),'segmentations','1')\n",
    "\n",
    "frame_list = []\n",
    "for frame in os.listdir(frame_path):\n",
    "    frame_list.append(os.path.join(frame_path,frame))\n",
    "frame_list.sort()\n",
    "\n",
    "seg_list = []\n",
    "for seg in os.listdir(seg_path):\n",
    "    seg_list.append(os.path.join(seg_path,seg))\n",
    "seg_list.sort()\n",
    "\n",
    "assert len(frame_list) == len(seg_list)\n",
    "frames = torch.zeros([len(frame_list), 150,150])\n",
    "segs = torch.zeros([len(frame_list), 150,150])\n",
    "for i in range(len(frame_list)):\n",
    "    frames[i] = torch.from_numpy(np.array(Image.open(frame_list[i]))) / 255\n",
    "    segs[i] = (torch.from_numpy(np.array(Image.open(seg_list[i]))) / 100)\n",
    "\n",
    "landmarks = pd.read_csv('landmarks.csv')\n",
    "landmarks[landmarks['Id'].isin(ids)]\n",
    "\n",
    "last_segment_available = np.fromstring(landmarks[landmarks['Id']== prob_id]['Start Frames'].iat[0].strip(']['), sep=',', dtype=int)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_between_frames = 6\n",
    "\n",
    "classic_segs = torch.zeros(segs.shape)\n",
    "kc_segs = torch.zeros(segs.shape)\n",
    "dml_segs = torch.zeros(segs.shape)\n",
    "\n",
    "classic_dice = torch.zeros(frames.shape[0],2)\n",
    "kc_dice = torch.zeros(frames.shape[0],2)\n",
    "dml_dice = torch.zeros(frames.shape[0],2)\n",
    "\n",
    "classic_field = torch.zeros(frames.shape[0], 2, frames.shape[1], frames.shape[2])\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "\n",
    "    # skipp first X frames\n",
    "    if i < distance_between_frames:\n",
    "        continue\n",
    "    \n",
    "    if i-distance_between_frames <= last_segment_available:\n",
    "        fixed = torch.clone(frames[i- distance_between_frames])\n",
    "        fixed_seg = torch.clone(segs[i-distance_between_frames])\n",
    "    \n",
    "    if i-distance_between_frames > last_segment_available:\n",
    "        fixed = torch.clone(frames[last_segment_available])\n",
    "        fixed_seg = torch.clone(segs[last_segment_available])\n",
    "    \n",
    "    \n",
    "    moving = torch.clone(frames[i])\n",
    "    moving_seg = torch.clone(segs[i])\n",
    "\n",
    "\n",
    "    # LABELLOSS\n",
    "    pdd_seg, run_time_pdd, flowfield = pdd_warp(model_classic, fixed, moving, fixed_seg, moving_seg)\n",
    "    classic_segs[i] = pdd_seg.detach()\n",
    "    classic_dice[i] = dice_coeff(moving_seg, pdd_seg, 3)\n",
    "    classic_field[i] = flowfield.detach()\n",
    "\n",
    "    # KC\n",
    "    #pdd_seg, run_time_pdd, _ = pdd_warp(model_kc, fixed, moving, fixed_seg, moving_seg)\n",
    "    #kc_segs[i] = pdd_seg.detach()\n",
    "    #kc_dice[i] = dice_coeff(moving_seg, pdd_seg, 3)\n",
    "\n",
    "    # DML\n",
    "    #pdd_seg, run_time_pdd, _ = pdd_warp(model_dml, fixed, moving, fixed_seg, moving_seg)\n",
    "    #dml_segs[i] = pdd_seg.detach()\n",
    "    #dml_dice[i] = dice_coeff(moving_seg, pdd_seg, 3)\n",
    "\n",
    "     # SOFT\n",
    "    #pdd_seg, run_time_pdd, _ = pdd_warp(model_soft, fixed, moving, fixed_seg, moving_seg)\n",
    "    #soft_segs[i] = pdd_seg.detach()\n",
    "    #soft_dice[i] = dice_coeff(moving_seg, pdd_seg, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464e622",
   "metadata": {},
   "source": [
    "# Show Comparison in Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ec8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vein: \", classic_dice[:,0].mean(), classic_dice[:,0].var())\n",
    "print(\"Artery: \", classic_dice[:,1].mean(), classic_dice[:,1].var())\n",
    "print(\"Overall: \", classic_dice.mean(), classic_dice.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00833f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "\n",
    "\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax2.set_title('Labelloss trained')\n",
    "ax3.set_title('Flow Field')\n",
    "#ax4.set_title('DML trained')\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(len(frames)):\n",
    "    overlay_ground = overlaySegment(frames[i], segs[i])\n",
    "    overlay_classic = overlaySegment(frames[i], classic_segs[i].squeeze())\n",
    "    flow_vis = showFlow(classic_field[i])\n",
    "    #overlay_kc = overlaySegment(frames[i], kc_segs[i].squeeze())\n",
    "    #overlay_dml = overlaySegment(frames[i], dml_segs[i].squeeze())\n",
    "    #overlay_soft = overlaySegment(frames[i], soft_segs[i].squeeze())\n",
    "    \n",
    "    ground_truth_image = ax1.imshow(overlay_ground, animated=True)\n",
    "    classic_image = ax2.imshow(overlay_classic, animated=True)\n",
    "    flow_img = ax3.imshow(flow_vis, animated=True)\n",
    "    #kc_image = ax3.imshow(overlay_kc, animated=True)\n",
    "    #dml_image = ax4.imshow(overlay_dml, animated=True)\n",
    "    #soft_image = ax4.imshow(overlay_soft, animated=True)\n",
    "    \n",
    "    lines.append([ground_truth_image, classic_image, flow_img])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig,lines,interval=100,blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(f'flowfield_{prob_id}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax2.set_title('Labelloss trained')\n",
    "ax3.set_title('KD trained')\n",
    "ax4.set_title('DML trained')\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(len(frames)):\n",
    "    overlay_ground = overlaySegment(frames[i], segs[i])\n",
    "    overlay_classic = overlaySegment(frames[i], classic_segs[i].squeeze())\n",
    "    overlay_kc = overlaySegment(frames[i], kc_segs[i].squeeze())\n",
    "    overlay_dml = overlaySegment(frames[i], dml_segs[i].squeeze())\n",
    "    #overlay_soft = overlaySegment(frames[i], soft_segs[i].squeeze())\n",
    "    \n",
    "    ground_truth_image = ax1.imshow(overlay_ground, animated=True)\n",
    "    classic_image = ax2.imshow(overlay_classic, animated=True)\n",
    "    kc_image = ax3.imshow(overlay_kc, animated=True)\n",
    "    dml_image = ax4.imshow(overlay_dml, animated=True)\n",
    "    #soft_image = ax4.imshow(overlay_soft, animated=True)\n",
    "    \n",
    "    lines.append([ground_truth_image, classic_image, kc_image,dml_image])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig,lines,interval=100,blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(f'compare_models_on_{prob_id}.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a91b3fa",
   "metadata": {},
   "source": [
    "# Generate Video of ground truth and No seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16044b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "ax1.set_title('No Segmentation')\n",
    "ax2.set_title('Ground Truth')\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(len(frames)):\n",
    "    overlay_ground = overlaySegment(frames[i], segs[i])\n",
    "    #overlay_classic = overlaySegment(frames[i], classic_segs[i].squeeze())\n",
    "    #overlay_kc = overlaySegment(frames[i], kc_segs[i].squeeze())\n",
    "    #overlay_dml = overlaySegment(frames[i], dml_segs[i].squeeze())\n",
    "    #overlay_soft = overlaySegment(frames[i], soft_segs[i].squeeze())\n",
    "    \n",
    "    ground_truth_image = ax2.imshow(overlay_ground, animated=True)\n",
    "    no_seg = ax1.imshow(frames[i], cmap='gray', animated=True)\n",
    "    #classic_image = ax1.imshow(overlay_classic, animated=True)\n",
    "    #kc_image = ax2.imshow(overlay_kc, animated=True)\n",
    "    #dml_image = ax3.imshow(overlay_dml, animated=True)\n",
    "    #soft_image = ax4.imshow(overlay_soft, animated=True)\n",
    "    \n",
    "    lines.append([no_seg, ground_truth_image])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig,lines,interval=100,blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b622119",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(f'gt_and_no_seg{prob_id}.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715fa76",
   "metadata": {},
   "source": [
    "# All in one Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f158c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(9,13))\n",
    "ax1 = fig.add_subplot(4, 2, 1)\n",
    "ax2 = fig.add_subplot(4, 2, 2)\n",
    "ax3 = fig.add_subplot(4, 2, 3)\n",
    "ax4 = fig.add_subplot(4, 2, 4)\n",
    "ax5 = fig.add_subplot(4, 2, 5)\n",
    "ax6 = fig.add_subplot(4, 2, 6)\n",
    "\n",
    "ax1.set_title('No Segmentation')\n",
    "ax2.set_title('Ground Truth')\n",
    "ax3.set_title('Labelloss trained')\n",
    "ax4.set_title('KD trained')\n",
    "ax5.set_title('DML trained')\n",
    "ax6.set_title('Soft trained')\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(len(frames)):\n",
    "    overlay_ground = overlaySegment(frames[i], segs[i])\n",
    "    overlay_classic = overlaySegment(frames[i], classic_segs[i].squeeze())\n",
    "    overlay_kc = overlaySegment(frames[i], kc_segs[i].squeeze())\n",
    "    overlay_dml = overlaySegment(frames[i], dml_segs[i].squeeze())\n",
    "    overlay_soft = overlaySegment(frames[i], soft_segs[i].squeeze())\n",
    "    \n",
    "    no_seg_image = ax1.imshow(frames[i], cmap='gray', animated=True)\n",
    "    no_seg_image = ax2.imshow(overlay_ground, animated=True)\n",
    "    classic_image = ax3.imshow(overlay_classic, animated=True)\n",
    "    kc_image = ax4.imshow(overlay_kc, animated=True)\n",
    "    dml_image = ax5.imshow(overlay_dml, animated=True)\n",
    "    soft_image = ax6.imshow(overlay_soft, animated=True)\n",
    "    \n",
    "    lines.append([no_seg_image,no_seg_image,classic_image, kc_image,dml_image, soft_image])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig,lines,interval=100,blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(f'all_compare{prob_id}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b795fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
